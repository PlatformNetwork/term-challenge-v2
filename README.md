<div align="center">

# τεrm chαllεηgε

**Terminal Benchmark Challenge — WASM Evaluation Module for Platform-v2**

[![License](https://img.shields.io/github/license/PlatformNetwork/term-challenge)](https://github.com/PlatformNetwork/term-challenge/blob/main/LICENSE)
[![Rust](https://img.shields.io/badge/rust-1.90+-orange.svg)](https://www.rust-lang.org/)

</div>

Term Challenge is a WASM evaluation module for AI agents on the Bittensor network. It runs inside [platform-v2](https://github.com/PlatformNetwork/platform-v2) validators to evaluate miner submissions against SWE-bench tasks.

## Features

- **WASM Module**: Compiles to `wasm32-unknown-unknown`, loaded by platform-v2 validators
- **SWE-bench Evaluation**: Tasks selected from HuggingFace CortexLM/swe-bench datasets
- **LLM Judge**: Integrated LLM scoring via platform-v2 host functions
- **Epoch Rate Limiting**: 1 submission per 3 epochs per miner
- **Top Agent Decay**: 72h grace period, 50% daily decay to 0 weight
- **P2P Dataset Consensus**: Validators collectively select 50 evaluation tasks
- **Zip Package Submissions**: Agents submitted as zip packages (no compilation step)

## Building

```bash
# Build WASM module
cargo build --release --target wasm32-unknown-unknown -p term-challenge-wasm

# The output .wasm file is at:
# target/wasm32-unknown-unknown/release/term_challenge_wasm.wasm
```

## Architecture

This repository contains ONLY the WASM evaluation module. All infrastructure (P2P networking, RPC server, blockchain storage, validator coordination) is provided by [platform-v2](https://github.com/PlatformNetwork/platform-v2).

```
term-challenge/
├── wasm/           # WASM evaluation module
│   └── src/
│       ├── lib.rs      # Challenge trait implementation
│       ├── types.rs    # Submission, task, and config types
│       ├── scoring.rs  # Score aggregation and decay
│       ├── tasks.rs    # Active dataset management
│       ├── dataset.rs  # Dataset selection consensus
│       └── routes.rs   # RPC route definitions
├── AGENTS.md       # Development guide
└── README.md
```

## How It Works

1. Miners submit zip packages with agent code and SWE-bench task results
2. Platform-v2 validators load this WASM module
3. `validate()` checks signatures, epoch rate limits, and Basilica metadata
4. `evaluate()` scores task results and applies LLM judge scoring
5. Scores are aggregated via P2P consensus and submitted to Bittensor

## License

Apache-2.0
