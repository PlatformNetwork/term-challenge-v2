"""Tests for term_sdk LLM client."""

import pytest
from unittest.mock import patch, MagicMock
from term_sdk import LLM, LLMResponse, LLMError, Tool, FunctionCall


class TestLLMError:
    def test_basic_error(self):
        err = LLMError("rate_limit", "Too many requests")
        assert err.code == "rate_limit"
        assert err.message == "Too many requests"
        assert err.details == {}
    
    def test_error_with_details(self):
        err = LLMError("invalid_model", "Model not found", {"model": "gpt-5"})
        assert err.details["model"] == "gpt-5"
    
    def test_to_dict(self):
        err = LLMError("test", "Test error")
        d = err.to_dict()
        assert d["error"]["code"] == "test"
        assert d["error"]["message"] == "Test error"
    
    def test_to_json(self):
        err = LLMError("test", "Test")
        import json
        data = json.loads(err.to_json())
        assert "error" in data
    
    def test_str(self):
        err = LLMError("code", "message")
        assert "code" in str(err)
        assert "message" in str(err)


class TestLLMResponse:
    def test_basic_response(self):
        resp = LLMResponse(
            text="Hello",
            model="gpt-4",
            tokens=10,
            cost=0.001,
            latency_ms=100
        )
        assert resp.text == "Hello"
        assert resp.model == "gpt-4"
        assert resp.tokens == 10
    
    def test_json_parsing(self):
        resp = LLMResponse(
            text='{"command": "ls", "task_complete": false}',
            model="gpt-4"
        )
        data = resp.json()
        assert data["command"] == "ls"
    
    def test_json_from_markdown(self):
        resp = LLMResponse(
            text='```json\n{"key": "value"}\n```',
            model="gpt-4"
        )
        data = resp.json()
        assert data["key"] == "value"
    
    def test_has_function_calls(self):
        resp = LLMResponse(text="", model="gpt-4")
        assert resp.has_function_calls() is False
        
        resp.function_calls = [FunctionCall(name="test", arguments={})]
        assert resp.has_function_calls() is True


class TestLLM:
    def test_invalid_provider(self):
        with pytest.raises(LLMError) as exc:
            LLM(provider="invalid_provider")
        assert exc.value.code == "invalid_provider"
    
    def test_no_model_error(self):
        llm = LLM()
        with pytest.raises(LLMError) as exc:
            llm._get_model(None)
        assert exc.value.code == "no_model"
    
    def test_default_model(self):
        llm = LLM(default_model="gpt-4")
        model = llm._get_model(None)
        assert model == "gpt-4"
    
    def test_override_model(self):
        llm = LLM(default_model="gpt-4")
        model = llm._get_model("claude-3-haiku")
        assert model == "claude-3-haiku"
    
    def test_register_function(self):
        llm = LLM()
        
        def my_func(x: int) -> int:
            return x * 2
        
        llm.register_function("double", my_func)
        assert "double" in llm._function_handlers
    
    def test_execute_function(self):
        llm = LLM()
        llm.register_function("add", lambda a, b: a + b)
        
        call = FunctionCall(name="add", arguments={"a": 1, "b": 2})
        result = llm.execute_function(call)
        assert result == 3
    
    def test_execute_unknown_function(self):
        llm = LLM()
        call = FunctionCall(name="unknown", arguments={})
        with pytest.raises(LLMError) as exc:
            llm.execute_function(call)
        assert exc.value.code == "unknown_function"
    
    def test_get_stats_empty(self):
        llm = LLM()
        stats = llm.get_stats()
        assert stats["total_tokens"] == 0
        assert stats["total_cost"] == 0.0
    
    def test_get_stats_per_model(self):
        llm = LLM()
        llm._update_model_stats("gpt-4", 100, 0.01)
        llm._update_model_stats("gpt-4", 50, 0.005)
        
        stats = llm.get_stats("gpt-4")
        assert stats["tokens"] == 150
        assert stats["cost"] == 0.015
        assert stats["requests"] == 2
    
    def test_calculate_cost(self):
        llm = LLM()
        # gpt-4o: $5/1M input, $15/1M output
        cost = llm._calculate_cost("gpt-4o", 1000, 1000)
        expected = (1000 * 5 + 1000 * 15) / 1_000_000
        assert abs(cost - expected) < 0.0001
    
    def test_context_manager(self):
        with LLM() as llm:
            assert llm is not None
        # Should not raise after exit


class TestPromptCaching:
    """Tests for Anthropic prompt caching via OpenRouter."""
    
    def test_caching_default_enabled(self):
        llm = LLM()
        assert llm._enable_cache is True
        assert llm._cache_ttl == "5min"  # Default is now 5min (Anthropic default)
        assert llm._cache_min_chars == 4000  # ~1024 tokens minimum for auto-detect
    
    def test_caching_custom_config(self):
        llm = LLM(enable_cache=False, cache_ttl="5m", cache_min_chars=1000)
        assert llm._enable_cache is False
        assert llm._cache_ttl == "5m"
        assert llm._cache_min_chars == 1000
    
    def test_is_anthropic_model(self):
        llm = LLM()
        assert llm._is_anthropic_model("anthropic/claude-3.5-sonnet") is True
        assert llm._is_anthropic_model("anthropic/claude-3-opus") is True
        assert llm._is_anthropic_model("claude-3-haiku") is True
        assert llm._is_anthropic_model("Claude-3.5-Sonnet") is True  # Case insensitive
        assert llm._is_anthropic_model("gpt-4o") is False
        assert llm._is_anthropic_model("deepseek-v3") is False
        assert llm._is_anthropic_model("openai/gpt-4") is False
    
    def test_get_message_content_length_string(self):
        llm = LLM()
        msg = {"role": "user", "content": "Hello world"}
        assert llm._get_message_content_length(msg) == 11
    
    def test_get_message_content_length_multipart(self):
        llm = LLM()
        msg = {
            "role": "user", 
            "content": [
                {"type": "text", "text": "Hello"},
                {"type": "text", "text": " world"}
            ]
        }
        assert llm._get_message_content_length(msg) == 11
    
    def test_get_message_content_length_none(self):
        llm = LLM()
        msg = {"role": "assistant", "content": None}
        assert llm._get_message_content_length(msg) == 0
    
    def test_get_message_content_length_empty(self):
        llm = LLM()
        msg = {"role": "user"}
        assert llm._get_message_content_length(msg) == 0
    
    def test_select_messages_to_cache_max_4(self):
        # Use cache_min_chars=0 to test max 4 logic without auto-detect filtering
        llm = LLM(cache_min_chars=0)
        messages = [
            {"role": "system", "content": "System prompt"},
            {"role": "user", "content": "A" * 1000},
            {"role": "assistant", "content": "B" * 500},
            {"role": "user", "content": "C" * 2000},
            {"role": "assistant", "content": "D" * 1500},
            {"role": "user", "content": "E" * 100},  # Last user - should NOT be cached
        ]
        selected = llm._select_messages_to_cache(messages)
        assert len(selected) <= 4
        assert 5 not in selected  # Last user message not cached
    
    def test_select_messages_prioritizes_system(self):
        # Large system prompt triggers auto-detection
        llm = LLM()
        messages = [
            {"role": "system", "content": "A" * 5000},  # Large system - should be cached
            {"role": "user", "content": "B" * 10000},  # Much longer but user (not last)
            {"role": "user", "content": "Last"},
        ]
        selected = llm._select_messages_to_cache(messages)
        assert 0 in selected  # Large system always cached
    
    def test_select_messages_skips_last_user(self):
        llm = LLM()
        messages = [
            {"role": "system", "content": "A" * 5000},  # Large system
            {"role": "user", "content": "B" * 10000},  # Last user - very long but skipped
        ]
        selected = llm._select_messages_to_cache(messages)
        assert 1 not in selected  # Last user not cached
        assert 0 in selected  # System cached
    
    def test_select_messages_respects_min_chars(self):
        llm = LLM(cache_min_chars=500)
        messages = [
            {"role": "system", "content": "Short"},  # 5 chars < 500
            {"role": "user", "content": "A" * 1000},
            {"role": "assistant", "content": "Last"},
        ]
        selected = llm._select_messages_to_cache(messages)
        assert 0 not in selected  # System too short
        # Note: user message at index 1 won't be cached either (not repeated, not system)
    
    def test_select_messages_prioritizes_longer(self):
        # With auto-detection, only system prompts >= min_chars or repeated messages are cached
        # Use a large system prompt and test priority ordering
        llm = LLM()
        large_system = "A" * 5000  # > 4000 threshold
        messages = [
            {"role": "system", "content": large_system},  # Should be cached
            {"role": "user", "content": "B" * 100},
            {"role": "assistant", "content": "C" * 3000},
            {"role": "user", "content": "Last"},
        ]
        selected = llm._select_messages_to_cache(messages)
        assert 0 in selected  # Large system is cached
        # Assistant message not cached (not repeated, not system)
        assert 2 not in selected
    
    def test_select_messages_repeated_message(self):
        # Test that repeated messages are detected and cached
        llm = LLM()
        repeated_content = "This is a repeated message"
        
        # First call: message is seen but not repeated yet (assistant, not last user)
        messages1 = [
            {"role": "assistant", "content": repeated_content},
            {"role": "user", "content": "First question"},
        ]
        selected1 = llm._select_messages_to_cache(messages1)
        assert 0 not in selected1  # First time seeing it, not cached yet
        
        # Second call: same content is now repeated - should be cached
        messages2 = [
            {"role": "assistant", "content": repeated_content},  # Now seen before!
            {"role": "user", "content": "New question"},
        ]
        selected2 = llm._select_messages_to_cache(messages2)
        assert 0 in selected2  # Repeated message is now cached
    
    def test_add_cache_control_string_content(self):
        llm = LLM(cache_ttl="5m")
        msg = {"role": "system", "content": "Hello"}
        result = llm._add_cache_control_to_message(msg)
        
        assert isinstance(result["content"], list)
        assert len(result["content"]) == 1
        assert result["content"][0]["type"] == "text"
        assert result["content"][0]["text"] == "Hello"
        assert result["content"][0]["cache_control"]["type"] == "ephemeral"
        assert "ttl" not in result["content"][0]["cache_control"]  # 5m has no ttl
    
    def test_add_cache_control_with_1h_ttl(self):
        llm = LLM(cache_ttl="1h")
        msg = {"role": "system", "content": "Hello"}
        result = llm._add_cache_control_to_message(msg)
        
        assert result["content"][0]["cache_control"]["type"] == "ephemeral"
        assert result["content"][0]["cache_control"]["ttl"] == "1h"
    
    def test_add_cache_control_multipart(self):
        llm = LLM()
        msg = {
            "role": "user",
            "content": [
                {"type": "text", "text": "Part 1"},
                {"type": "text", "text": "Part 2"}
            ]
        }
        result = llm._add_cache_control_to_message(msg)
        
        # cache_control should be on last text part only
        assert "cache_control" not in result["content"][0]
        assert "cache_control" in result["content"][1]
        assert result["content"][1]["cache_control"]["type"] == "ephemeral"
    
    def test_add_cache_control_preserves_other_fields(self):
        llm = LLM()
        msg = {"role": "system", "content": "Hello", "name": "assistant"}
        result = llm._add_cache_control_to_message(msg)
        
        assert result["role"] == "system"
        assert result["name"] == "assistant"
    
    def test_prepare_messages_non_anthropic_unchanged(self):
        llm = LLM()
        messages = [
            {"role": "system", "content": "System"},
            {"role": "user", "content": "Hello"}
        ]
        result = llm._prepare_messages_with_cache(messages, "gpt-4o")
        
        # Should be unchanged for non-Anthropic
        assert result[0]["content"] == "System"
        assert result[1]["content"] == "Hello"
    
    def test_prepare_messages_caching_disabled(self):
        llm = LLM(enable_cache=False)
        messages = [
            {"role": "system", "content": "System"},
            {"role": "user", "content": "Hello"}
        ]
        result = llm._prepare_messages_with_cache(messages, "anthropic/claude-3.5-sonnet")
        
        # Should be unchanged when disabled
        assert result[0]["content"] == "System"
        assert result[1]["content"] == "Hello"
    
    def test_prepare_messages_non_openrouter_unchanged(self):
        llm = LLM(provider="openai")
        messages = [
            {"role": "system", "content": "System"},
            {"role": "user", "content": "Hello"}
        ]
        result = llm._prepare_messages_with_cache(messages, "claude-3")
        
        # Should be unchanged for non-OpenRouter provider
        assert result[0]["content"] == "System"
    
    def test_prepare_messages_full_flow(self):
        # Use large system prompt to trigger auto-detection
        llm = LLM(cache_ttl="1h")
        large_system = "You are a helpful assistant. " * 200  # ~6000 chars > 4000 threshold
        messages = [
            {"role": "system", "content": large_system},
            {"role": "user", "content": "What is 2+2?"},
        ]
        result = llm._prepare_messages_with_cache(messages, "anthropic/claude-3.5-sonnet")
        
        # System should be cached (multipart format)
        assert isinstance(result[0]["content"], list)
        assert result[0]["content"][0]["type"] == "text"
        assert result[0]["content"][0]["text"] == large_system
        assert result[0]["content"][0]["cache_control"]["type"] == "ephemeral"
        assert result[0]["content"][0]["cache_control"]["ttl"] == "1h"
        
        # Last user message should NOT be cached (still string)
        assert isinstance(result[1]["content"], str)
        assert result[1]["content"] == "What is 2+2?"
    
    def test_prepare_messages_conversation_history(self):
        # Use large system to trigger caching
        llm = LLM()
        large_system = "System prompt " * 400  # ~5600 chars > 4000 threshold
        messages = [
            {"role": "system", "content": large_system},
            {"role": "user", "content": "First question"},
            {"role": "assistant", "content": "First answer with lots of detail " * 100},
            {"role": "user", "content": "Second question"},
            {"role": "assistant", "content": "Second answer"},
            {"role": "user", "content": "Current question"},  # Last - not cached
        ]
        result = llm._prepare_messages_with_cache(messages, "anthropic/claude-3.5-sonnet")
        
        # System (index 0) should be cached (large enough)
        assert isinstance(result[0]["content"], list)
        
        # Last user message (index 5) should NOT be cached
        assert isinstance(result[5]["content"], str)
    
    def test_prepare_messages_empty_list(self):
        llm = LLM()
        result = llm._prepare_messages_with_cache([], "anthropic/claude-3.5-sonnet")
        assert result == []


class TestTool:
    def test_tool_to_dict(self):
        tool = Tool(
            name="search",
            description="Search files",
            parameters={
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                },
                "required": ["query"]
            }
        )
        d = tool.to_dict()
        assert d["type"] == "function"
        assert d["function"]["name"] == "search"
        assert d["function"]["parameters"]["required"] == ["query"]
