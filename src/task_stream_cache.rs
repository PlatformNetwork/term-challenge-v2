//! Real-time task progress cache for live evaluation updates
//!
//! Stores streaming stdout/stderr from validators during task execution.
//! Clients can poll for live progress before task results are persisted to DB.
//!
//! Features:
//! - Max 1MB per task entry (configurable)
//! - 1 hour TTL with automatic cleanup
//! - Thread-safe concurrent access via DashMap
//! - Automatic eviction when task is persisted to DB

use dashmap::DashMap;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use tracing::{debug, info};

/// Default maximum size per task entry (1 MB)
pub const DEFAULT_MAX_ENTRY_SIZE: usize = 1_048_576;

/// Default TTL in seconds (1 hour)
pub const DEFAULT_TTL_SECS: u64 = 3600;

/// Default cleanup interval in seconds (5 minutes)
pub const DEFAULT_CLEANUP_INTERVAL_SECS: u64 = 300;

/// Default streaming interval in milliseconds (2 seconds)
pub const DEFAULT_STREAM_INTERVAL_MS: u64 = 2000;

/// Configuration for the task stream cache
#[derive(Debug, Clone)]
pub struct TaskStreamConfig {
    pub max_entry_size_bytes: usize,
    pub ttl_secs: u64,
    pub cleanup_interval_secs: u64,
    pub stream_interval_ms: u64,
    pub enabled: bool,
}

impl Default for TaskStreamConfig {
    fn default() -> Self {
        Self {
            max_entry_size_bytes: DEFAULT_MAX_ENTRY_SIZE,
            ttl_secs: DEFAULT_TTL_SECS,
            cleanup_interval_secs: DEFAULT_CLEANUP_INTERVAL_SECS,
            stream_interval_ms: DEFAULT_STREAM_INTERVAL_MS,
            enabled: true,
        }
    }
}

impl TaskStreamConfig {
    pub fn from_env() -> Self {
        Self {
            max_entry_size_bytes: std::env::var("TASK_STREAM_MAX_SIZE")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(DEFAULT_MAX_ENTRY_SIZE),
            ttl_secs: std::env::var("TASK_STREAM_TTL_SECS")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(DEFAULT_TTL_SECS),
            cleanup_interval_secs: std::env::var("TASK_STREAM_CLEANUP_INTERVAL")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(DEFAULT_CLEANUP_INTERVAL_SECS),
            stream_interval_ms: std::env::var("TASK_STREAM_INTERVAL_MS")
                .ok()
                .and_then(|v| v.parse().ok())
                .unwrap_or(DEFAULT_STREAM_INTERVAL_MS),
            enabled: std::env::var("TASK_STREAM_ENABLED")
                .map(|v| v != "false" && v != "0")
                .unwrap_or(true),
        }
    }
}

/// A single task's streaming progress entry
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskStreamEntry {
    pub agent_hash: String,
    pub validator_hotkey: String,
    pub task_id: String,
    pub task_name: String,
    /// Status: "running", "completed", "failed"
    pub status: String,
    /// Accumulated stdout (truncated to max size, keeps recent data)
    pub stdout_buffer: String,
    /// Accumulated stderr (truncated to max size, keeps recent data)
    pub stderr_buffer: String,
    /// Current step number from agent
    pub current_step: i32,
    /// Unix timestamp when task started
    pub started_at: i64,
    /// Unix timestamp of last update
    pub updated_at: i64,
    /// Current total size in bytes
    pub size_bytes: usize,
}

impl TaskStreamEntry {
    pub fn new(
        agent_hash: String,
        validator_hotkey: String,
        task_id: String,
        task_name: String,
    ) -> Self {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs() as i64;

        Self {
            agent_hash,
            validator_hotkey,
            task_id,
            task_name,
            status: "running".to_string(),
            stdout_buffer: String::new(),
            stderr_buffer: String::new(),
            current_step: 0,
            started_at: now,
            updated_at: now,
            size_bytes: 0,
        }
    }

    fn calculate_size(&self) -> usize {
        self.stdout_buffer.len() + self.stderr_buffer.len()
    }

    /// Append to stdout, keeping recent data if exceeds max size
    pub fn append_stdout(&mut self, chunk: &str, max_size: usize) {
        if chunk.is_empty() {
            return;
        }
        self.stdout_buffer.push_str(chunk);
        self.truncate_if_needed(max_size);
        self.update_timestamp();
    }

    /// Append to stderr, keeping recent data if exceeds max size
    pub fn append_stderr(&mut self, chunk: &str, max_size: usize) {
        if chunk.is_empty() {
            return;
        }
        self.stderr_buffer.push_str(chunk);
        self.truncate_if_needed(max_size);
        self.update_timestamp();
    }

    /// Truncate from the beginning to keep recent data
    fn truncate_if_needed(&mut self, max_size: usize) {
        let current_size = self.calculate_size();
        if current_size > max_size {
            let excess = current_size - max_size;
            // Remove from stdout first (usually larger), keeping recent data
            if self.stdout_buffer.len() > excess {
                // Find a good boundary (newline) near the truncation point
                let truncate_at = self.stdout_buffer[..excess]
                    .rfind('\n')
                    .map(|i| i + 1)
                    .unwrap_or(excess);
                self.stdout_buffer = self.stdout_buffer[truncate_at..].to_string();
            } else {
                let remaining = excess - self.stdout_buffer.len();
                self.stdout_buffer.clear();
                if self.stderr_buffer.len() > remaining {
                    let truncate_at = self.stderr_buffer[..remaining]
                        .rfind('\n')
                        .map(|i| i + 1)
                        .unwrap_or(remaining);
                    self.stderr_buffer = self.stderr_buffer[truncate_at..].to_string();
                }
            }
        }
        self.size_bytes = self.calculate_size();
    }

    fn update_timestamp(&mut self) {
        self.updated_at = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs() as i64;
    }

    pub fn is_expired(&self, ttl_secs: u64) -> bool {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs() as i64;
        (now - self.updated_at) > ttl_secs as i64
    }

    pub fn duration_secs(&self) -> i64 {
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap()
            .as_secs() as i64;
        now - self.started_at
    }
}

/// Thread-safe cache for task streaming progress
#[derive(Clone)]
pub struct TaskStreamCache {
    entries: Arc<DashMap<String, TaskStreamEntry>>,
    config: TaskStreamConfig,
}

impl TaskStreamCache {
    pub fn new(config: TaskStreamConfig) -> Self {
        Self {
            entries: Arc::new(DashMap::new()),
            config,
        }
    }

    pub fn from_env() -> Self {
        Self::new(TaskStreamConfig::from_env())
    }

    pub fn is_enabled(&self) -> bool {
        self.config.enabled
    }

    pub fn stream_interval_ms(&self) -> u64 {
        self.config.stream_interval_ms
    }

    /// Generate cache key
    pub fn make_key(agent_hash: &str, validator_hotkey: &str, task_id: &str) -> String {
        format!("{}:{}:{}", agent_hash, validator_hotkey, task_id)
    }

    /// Push a streaming update
    pub fn push_update(&self, update: TaskStreamUpdate) {
        if !self.config.enabled {
            return;
        }

        let key = Self::make_key(
            &update.agent_hash,
            &update.validator_hotkey,
            &update.task_id,
        );
        let max_size = self.config.max_entry_size_bytes;

        self.entries
            .entry(key)
            .and_modify(|entry| {
                if let Some(ref status) = update.status {
                    entry.status = status.clone();
                }
                if let Some(ref chunk) = update.stdout_chunk {
                    entry.append_stdout(chunk, max_size);
                }
                if let Some(ref chunk) = update.stderr_chunk {
                    entry.append_stderr(chunk, max_size);
                }
                if let Some(step) = update.current_step {
                    entry.current_step = step;
                }
                entry.update_timestamp();
            })
            .or_insert_with(|| {
                let mut entry = TaskStreamEntry::new(
                    update.agent_hash.clone(),
                    update.validator_hotkey.clone(),
                    update.task_id.clone(),
                    update.task_name.clone().unwrap_or_default(),
                );
                if let Some(ref status) = update.status {
                    entry.status = status.clone();
                }
                if let Some(ref chunk) = update.stdout_chunk {
                    entry.append_stdout(chunk, max_size);
                }
                if let Some(ref chunk) = update.stderr_chunk {
                    entry.append_stderr(chunk, max_size);
                }
                if let Some(step) = update.current_step {
                    entry.current_step = step;
                }
                entry
            });
    }

    /// Get entry by key
    pub fn get_entry(&self, key: &str) -> Option<TaskStreamEntry> {
        self.entries.get(key).map(|e| e.clone())
    }

    /// Get entry by components
    pub fn get_task(
        &self,
        agent_hash: &str,
        validator_hotkey: &str,
        task_id: &str,
    ) -> Option<TaskStreamEntry> {
        let key = Self::make_key(agent_hash, validator_hotkey, task_id);
        self.get_entry(&key)
    }

    /// Get all live tasks for an agent
    pub fn get_agent_tasks(&self, agent_hash: &str) -> Vec<TaskStreamEntry> {
        self.entries
            .iter()
            .filter(|e| e.agent_hash == agent_hash)
            .map(|e| e.clone())
            .collect()
    }

    /// Get all entries for a specific task across validators
    pub fn get_task_by_id(&self, agent_hash: &str, task_id: &str) -> Vec<TaskStreamEntry> {
        self.entries
            .iter()
            .filter(|e| e.agent_hash == agent_hash && e.task_id == task_id)
            .map(|e| e.clone())
            .collect()
    }

    /// Remove entry (called when task is persisted to DB)
    pub fn remove(&self, agent_hash: &str, validator_hotkey: &str, task_id: &str) {
        let key = Self::make_key(agent_hash, validator_hotkey, task_id);
        if self.entries.remove(&key).is_some() {
            debug!(
                "Removed task stream entry: {}:{}",
                &agent_hash[..16.min(agent_hash.len())],
                task_id
            );
        }
    }

    /// Remove all entries for an agent
    pub fn remove_agent(&self, agent_hash: &str) {
        let keys_to_remove: Vec<String> = self
            .entries
            .iter()
            .filter(|e| e.agent_hash == agent_hash)
            .map(|e| e.key().clone())
            .collect();

        for key in keys_to_remove {
            self.entries.remove(&key);
        }
    }

    /// Cleanup expired entries
    pub fn cleanup_expired(&self) -> usize {
        let ttl = self.config.ttl_secs;
        let keys_to_remove: Vec<String> = self
            .entries
            .iter()
            .filter(|e| e.is_expired(ttl))
            .map(|e| e.key().clone())
            .collect();

        let count = keys_to_remove.len();
        for key in keys_to_remove {
            self.entries.remove(&key);
        }

        if count > 0 {
            info!("Cleaned up {} expired task stream entries", count);
        }
        count
    }

    /// Get cache stats
    pub fn stats(&self) -> TaskStreamStats {
        let entries: Vec<_> = self.entries.iter().collect();
        let total_size: usize = entries.iter().map(|e| e.size_bytes).sum();

        TaskStreamStats {
            entry_count: entries.len(),
            total_size_bytes: total_size,
            max_entry_size: self.config.max_entry_size_bytes,
            ttl_secs: self.config.ttl_secs,
            enabled: self.config.enabled,
        }
    }

    /// Spawn background cleanup task
    pub fn spawn_cleanup_task(self: Arc<Self>) {
        let cleanup_interval = self.config.cleanup_interval_secs;
        let interval = Duration::from_secs(cleanup_interval);

        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(interval);
            loop {
                ticker.tick().await;
                self.cleanup_expired();
            }
        });

        info!(
            "Task stream cache cleanup task started (interval: {}s)",
            cleanup_interval
        );
    }
}

/// Update to push to the cache
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskStreamUpdate {
    pub agent_hash: String,
    pub validator_hotkey: String,
    pub task_id: String,
    pub task_name: Option<String>,
    pub status: Option<String>,
    pub stdout_chunk: Option<String>,
    pub stderr_chunk: Option<String>,
    pub current_step: Option<i32>,
}

/// Cache statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskStreamStats {
    pub entry_count: usize,
    pub total_size_bytes: usize,
    pub max_entry_size: usize,
    pub ttl_secs: u64,
    pub enabled: bool,
}

/// Response for live task progress
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LiveTaskProgress {
    pub task_id: String,
    pub task_name: String,
    pub validator_hotkey: String,
    pub status: String,
    pub stdout: String,
    pub stderr: String,
    pub current_step: i32,
    pub duration_secs: i64,
    pub size_bytes: usize,
    pub is_live: bool,
}

impl From<TaskStreamEntry> for LiveTaskProgress {
    fn from(entry: TaskStreamEntry) -> Self {
        let is_live = entry.status == "running";
        let duration_secs = entry.duration_secs();
        let size_bytes = entry.size_bytes;
        Self {
            task_id: entry.task_id,
            task_name: entry.task_name,
            validator_hotkey: entry.validator_hotkey,
            status: entry.status,
            stdout: entry.stdout_buffer,
            stderr: entry.stderr_buffer,
            current_step: entry.current_step,
            duration_secs,
            size_bytes,
            is_live,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cache_basic_operations() {
        let cache = TaskStreamCache::new(TaskStreamConfig::default());

        let update = TaskStreamUpdate {
            agent_hash: "agent123".to_string(),
            validator_hotkey: "val456".to_string(),
            task_id: "task789".to_string(),
            task_name: Some("test_task".to_string()),
            status: Some("running".to_string()),
            stdout_chunk: Some("Hello ".to_string()),
            stderr_chunk: None,
            current_step: Some(1),
        };

        cache.push_update(update);

        let entry = cache.get_task("agent123", "val456", "task789");
        assert!(entry.is_some());
        let entry = entry.unwrap();
        assert_eq!(entry.status, "running");
        assert_eq!(entry.stdout_buffer, "Hello ");

        // Append more
        let update2 = TaskStreamUpdate {
            agent_hash: "agent123".to_string(),
            validator_hotkey: "val456".to_string(),
            task_id: "task789".to_string(),
            task_name: None,
            status: None,
            stdout_chunk: Some("World!".to_string()),
            stderr_chunk: None,
            current_step: Some(2),
        };
        cache.push_update(update2);

        let entry = cache.get_task("agent123", "val456", "task789").unwrap();
        assert_eq!(entry.stdout_buffer, "Hello World!");
        assert_eq!(entry.current_step, 2);

        // Remove
        cache.remove("agent123", "val456", "task789");
        assert!(cache.get_task("agent123", "val456", "task789").is_none());
    }

    #[test]
    fn test_size_limit() {
        let config = TaskStreamConfig {
            max_entry_size_bytes: 100,
            ..Default::default()
        };
        let cache = TaskStreamCache::new(config);

        let large_chunk = "X".repeat(80);
        let update = TaskStreamUpdate {
            agent_hash: "agent".to_string(),
            validator_hotkey: "val".to_string(),
            task_id: "task".to_string(),
            task_name: Some("test".to_string()),
            status: Some("running".to_string()),
            stdout_chunk: Some(large_chunk.clone()),
            stderr_chunk: None,
            current_step: None,
        };
        cache.push_update(update);

        // Push more to exceed limit
        let update2 = TaskStreamUpdate {
            agent_hash: "agent".to_string(),
            validator_hotkey: "val".to_string(),
            task_id: "task".to_string(),
            task_name: None,
            status: None,
            stdout_chunk: Some(large_chunk),
            stderr_chunk: None,
            current_step: None,
        };
        cache.push_update(update2);

        let entry = cache.get_task("agent", "val", "task").unwrap();
        assert!(entry.size_bytes <= 100);
    }

    #[test]
    fn test_get_agent_tasks() {
        let cache = TaskStreamCache::new(TaskStreamConfig::default());

        for i in 0..3 {
            let update = TaskStreamUpdate {
                agent_hash: "agent123".to_string(),
                validator_hotkey: format!("val{}", i),
                task_id: format!("task{}", i),
                task_name: Some(format!("test_{}", i)),
                status: Some("running".to_string()),
                stdout_chunk: None,
                stderr_chunk: None,
                current_step: None,
            };
            cache.push_update(update);
        }

        let tasks = cache.get_agent_tasks("agent123");
        assert_eq!(tasks.len(), 3);
    }

    #[test]
    fn test_task_stream_entry_creation() {
        let entry = TaskStreamEntry::new(
            "agent1".to_string(),
            "validator1".to_string(),
            "task1".to_string(),
            "Test Task".to_string(),
        );

        assert_eq!(entry.agent_hash, "agent1");
        assert_eq!(entry.validator_hotkey, "validator1");
        assert_eq!(entry.task_id, "task1");
        assert_eq!(entry.task_name, "Test Task");
        assert_eq!(entry.status, "running");
        assert!(entry.stdout_buffer.is_empty());
        assert!(entry.stderr_buffer.is_empty());
        assert_eq!(entry.current_step, 0);
        assert!(entry.started_at > 0);
    }

    #[test]
    fn test_task_stream_entry_append_stdout() {
        let mut entry = TaskStreamEntry::new(
            "agent".to_string(),
            "val".to_string(),
            "task".to_string(),
            "Test".to_string(),
        );

        entry.append_stdout("Hello ", 1000);
        assert_eq!(entry.stdout_buffer, "Hello ");

        entry.append_stdout("World!", 1000);
        assert_eq!(entry.stdout_buffer, "Hello World!");

        // Empty chunk should not change anything
        entry.append_stdout("", 1000);
        assert_eq!(entry.stdout_buffer, "Hello World!");
    }

    #[test]
    fn test_task_stream_entry_append_stderr() {
        let mut entry = TaskStreamEntry::new(
            "agent".to_string(),
            "val".to_string(),
            "task".to_string(),
            "Test".to_string(),
        );

        entry.append_stderr("Error: ", 1000);
        assert_eq!(entry.stderr_buffer, "Error: ");

        entry.append_stderr("Something failed", 1000);
        assert_eq!(entry.stderr_buffer, "Error: Something failed");
    }

    #[test]
    fn test_task_stream_update_struct() {
        let update = TaskStreamUpdate {
            agent_hash: "agent".to_string(),
            validator_hotkey: "val".to_string(),
            task_id: "task".to_string(),
            task_name: Some("My Task".to_string()),
            status: Some("completed".to_string()),
            stdout_chunk: Some("output".to_string()),
            stderr_chunk: Some("error".to_string()),
            current_step: Some(5),
        };

        assert_eq!(update.agent_hash, "agent");
        assert_eq!(update.task_name.as_ref().unwrap(), "My Task");
        assert_eq!(update.status.as_ref().unwrap(), "completed");
        assert_eq!(update.current_step.unwrap(), 5);
    }

    #[test]
    fn test_task_stream_config_default() {
        let config = TaskStreamConfig::default();

        assert!(config.max_entry_size_bytes > 0);
        assert!(config.ttl_secs > 0);
        assert!(config.cleanup_interval_secs > 0);
        assert!(config.enabled);
    }

    #[test]
    fn test_update_status() {
        let cache = TaskStreamCache::new(TaskStreamConfig::default());

        // Create task
        let update = TaskStreamUpdate {
            agent_hash: "agent".to_string(),
            validator_hotkey: "val".to_string(),
            task_id: "task".to_string(),
            task_name: Some("Test".to_string()),
            status: Some("running".to_string()),
            stdout_chunk: None,
            stderr_chunk: None,
            current_step: None,
        };
        cache.push_update(update);

        // Update status
        let update2 = TaskStreamUpdate {
            agent_hash: "agent".to_string(),
            validator_hotkey: "val".to_string(),
            task_id: "task".to_string(),
            task_name: None,
            status: Some("completed".to_string()),
            stdout_chunk: None,
            stderr_chunk: None,
            current_step: Some(10),
        };
        cache.push_update(update2);

        let entry = cache.get_task("agent", "val", "task").unwrap();
        assert_eq!(entry.status, "completed");
        assert_eq!(entry.current_step, 10);
    }

    #[test]
    fn test_nonexistent_task() {
        let cache = TaskStreamCache::new(TaskStreamConfig::default());

        let entry = cache.get_task("nonexistent", "val", "task");
        assert!(entry.is_none());
    }

    #[test]
    fn test_empty_agent_tasks() {
        let cache = TaskStreamCache::new(TaskStreamConfig::default());

        let tasks = cache.get_agent_tasks("nonexistent");
        assert!(tasks.is_empty());
    }

    #[test]
    fn test_stderr_update() {
        let cache = TaskStreamCache::new(TaskStreamConfig::default());

        let update = TaskStreamUpdate {
            agent_hash: "agent".to_string(),
            validator_hotkey: "val".to_string(),
            task_id: "task".to_string(),
            task_name: Some("Test".to_string()),
            status: Some("running".to_string()),
            stdout_chunk: None,
            stderr_chunk: Some("Warning message".to_string()),
            current_step: None,
        };
        cache.push_update(update);

        let entry = cache.get_task("agent", "val", "task").unwrap();
        assert_eq!(entry.stderr_buffer, "Warning message");
    }
}
