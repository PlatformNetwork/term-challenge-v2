//! LLM Review Worker
//!
//! Background service that reviews pending agent submissions using an LLM
//! (Kimi-K2.5-TEE via Chutes API) inside an isolated Docker container.
//!
//! Flow:
//! 1. Polls DB for agents with llm_review_status='pending'
//! 2. Loads validation rules from the validation_rules table
//! 3. Launches a Docker container (term-llm-reviewer:latest) with the agent code + rules
//! 4. Container calls Chutes API and returns JSON verdict
//! 5. Updates DB: approved -> llm_review_status='approved', rejected -> flagged=true

use crate::storage::pg::PgStorage;
use anyhow::{Context, Result};
use bollard::container::{
    Config, CreateContainerOptions, LogsOptions, RemoveContainerOptions, WaitContainerOptions,
};
use bollard::Docker;
use futures::StreamExt;
use std::sync::Arc;
use std::time::Duration;
use tokio::time::interval;
use tracing::{debug, error, info, warn};

const LLM_REVIEWER_IMAGE: &str = "term-llm-reviewer:latest";
const REVIEW_TIMEOUT_SECS: u64 = 180;
const POLL_INTERVAL_SECS: u64 = 10;
const BATCH_SIZE: i64 = 5;
const LLM_MODEL: &str = "moonshotai/Kimi-K2.5-TEE";

pub struct LlmReviewWorkerConfig {
    pub poll_interval_secs: u64,
    pub batch_size: i64,
    pub chutes_api_token: String,
}

impl Default for LlmReviewWorkerConfig {
    fn default() -> Self {
        Self {
            poll_interval_secs: POLL_INTERVAL_SECS,
            batch_size: BATCH_SIZE,
            chutes_api_token: String::new(),
        }
    }
}

pub struct LlmReviewWorker {
    storage: Arc<PgStorage>,
    config: LlmReviewWorkerConfig,
}

impl LlmReviewWorker {
    pub fn new(storage: Arc<PgStorage>, config: LlmReviewWorkerConfig) -> Self {
        Self { storage, config }
    }

    /// Create from environment (reads CHUTES_API_TOKEN)
    pub fn from_env(storage: Arc<PgStorage>) -> Option<Self> {
        let token = std::env::var("CHUTES_API_TOKEN")
            .or_else(|_| std::env::var("CHUTES_API_KEY"))
            .ok()?;

        if token.is_empty() {
            return None;
        }

        Some(Self::new(
            storage,
            LlmReviewWorkerConfig {
                chutes_api_token: token,
                ..Default::default()
            },
        ))
    }

    /// Start the worker (runs forever)
    pub async fn run(&self) {
        let api_key_preview = if self.config.chutes_api_token.len() > 8 {
            format!(
                "{}...{}",
                &self.config.chutes_api_token[..4],
                &self.config.chutes_api_token[self.config.chutes_api_token.len() - 4..]
            )
        } else {
            "****".to_string()
        };

        info!(
            "LLM Review worker started (poll={}s, batch={}, model={}, api_key={})",
            self.config.poll_interval_secs, self.config.batch_size, LLM_MODEL, api_key_preview
        );

        let mut ticker = interval(Duration::from_secs(self.config.poll_interval_secs));

        loop {
            ticker.tick().await;

            if let Err(e) = self.process_pending().await {
                error!("Error processing pending LLM reviews: {}", e);
            }
        }
    }

    async fn process_pending(&self) -> Result<()> {
        let pending = self
            .storage
            .get_pending_llm_reviews(self.config.batch_size)
            .await?;

        if pending.is_empty() {
            debug!("No pending LLM reviews");
            return Ok(());
        }

        info!("Found {} agents pending LLM review", pending.len());

        // Load validation rules from DB
        let rules = self.storage.get_active_validation_rules().await?;
        if rules.is_empty() {
            warn!("No active validation rules found in validation_rules table. LLM review cannot proceed.");
            warn!("Make sure migration 023_validation_rules.sql has been applied and validation_rules table has active rules.");
            return Ok(());
        }

        let formatted_rules = rules
            .iter()
            .enumerate()
            .map(|(i, r)| format!("{}. {}", i + 1, r))
            .collect::<Vec<_>>()
            .join("\n");

        for submission in pending {
            let agent_hash = &submission.agent_hash;
            let short_hash = &agent_hash[..16.min(agent_hash.len())];

            // Mark as reviewing
            if let Err(e) = self
                .storage
                .set_llm_review_status_reviewing(agent_hash)
                .await
            {
                error!("Failed to mark {} as reviewing: {}", short_hash, e);
                continue;
            }

            info!("Reviewing agent {} with {}", short_hash, LLM_MODEL);

            match self
                .review_in_container(&submission.source_code, &formatted_rules)
                .await
            {
                Ok(result) => {
                    let approved = result["approved"].as_bool().unwrap_or(false);
                    let reason = result["reason"]
                        .as_str()
                        .unwrap_or("No reason provided")
                        .to_string();
                    let violations: Vec<String> = result["violations"]
                        .as_array()
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(String::from))
                                .collect()
                        })
                        .unwrap_or_default();

                    if approved {
                        info!("Agent {} APPROVED by LLM review", short_hash);
                        if let Err(e) = self
                            .storage
                            .update_llm_review_result(agent_hash, "approved", LLM_MODEL, &result)
                            .await
                        {
                            error!("Failed to update approved status for {}: {}", short_hash, e);
                        }
                    } else {
                        warn!(
                            "Agent {} REJECTED by LLM review: {} (violations: {:?})",
                            short_hash, reason, violations
                        );
                        if let Err(e) = self
                            .storage
                            .update_llm_review_rejected(agent_hash, LLM_MODEL, &result, &reason)
                            .await
                        {
                            error!("Failed to update rejected status for {}: {}", short_hash, e);
                        }
                    }
                }
                Err(e) => {
                    error!("LLM review failed for agent {}: {}", short_hash, e);
                    let error_result = serde_json::json!({
                        "approved": false,
                        "reason": format!("Review container error: {}", e),
                        "violations": ["container_error"]
                    });
                    // On error, reset to pending so it can be retried
                    if let Err(e2) = self
                        .storage
                        .update_llm_review_result(agent_hash, "pending", LLM_MODEL, &error_result)
                        .await
                    {
                        error!("Failed to reset review status for {}: {}", short_hash, e2);
                    }
                }
            }
        }

        Ok(())
    }

    /// Launch a Docker container to review the code and return the JSON result
    async fn review_in_container(
        &self,
        source_code: &str,
        rules: &str,
    ) -> Result<serde_json::Value> {
        let docker =
            Docker::connect_with_local_defaults().context("Failed to connect to Docker daemon")?;

        // Write source code to a temp file that will be mounted
        let tmp_dir = tempfile::tempdir().context("Failed to create temp dir")?;
        let code_path = tmp_dir.path().join("code.py");
        std::fs::write(&code_path, source_code).context("Failed to write temp code file")?;

        let container_name = format!("llm-review-{}", uuid::Uuid::new_v4());

        let host_path = code_path.to_str().context("Invalid temp path")?.to_string();

        let env_vars = vec![
            format!("CHUTES_API_TOKEN={}", self.config.chutes_api_token),
            format!("RULES={}", rules),
        ];

        let container_config = Config {
            image: Some(LLM_REVIEWER_IMAGE.to_string()),
            env: Some(env_vars.clone()),
            host_config: Some(bollard::models::HostConfig {
                binds: Some(vec![format!("{}:/review/code.py:ro", host_path)]),
                memory: Some(256 * 1024 * 1024),          // 256MB
                nano_cpus: Some(1_000_000_000),           // 1 CPU
                network_mode: Some("bridge".to_string()), // Needs network for API call
                ..Default::default()
            }),
            ..Default::default()
        };

        let create_opts = CreateContainerOptions {
            name: container_name.as_str(),
            platform: None,
        };

        // Create and start container
        docker
            .create_container(Some(create_opts), container_config)
            .await
            .context("Failed to create review container")?;

        docker
            .start_container(
                &container_name,
                None::<bollard::container::StartContainerOptions<String>>,
            )
            .await
            .context("Failed to start review container")?;

        // Wait for container to finish with timeout
        let wait_result = tokio::time::timeout(
            Duration::from_secs(REVIEW_TIMEOUT_SECS),
            Self::wait_for_container(&docker, &container_name),
        )
        .await;

        // Collect stdout logs regardless of exit status
        let stdout = Self::collect_logs(&docker, &container_name).await;

        // Cleanup container
        let _ = docker
            .remove_container(
                &container_name,
                Some(RemoveContainerOptions {
                    force: true,
                    ..Default::default()
                }),
            )
            .await;

        // Handle timeout
        let exit_code = match wait_result {
            Ok(Ok(code)) => code,
            Ok(Err(e)) => {
                return Err(anyhow::anyhow!("Container wait error: {}", e));
            }
            Err(_) => {
                return Err(anyhow::anyhow!(
                    "Review container timed out after {}s",
                    REVIEW_TIMEOUT_SECS
                ));
            }
        };

        debug!(
            "Review container exited with code {}, stdout len={}",
            exit_code,
            stdout.len()
        );

        // Parse JSON from stdout
        if stdout.trim().is_empty() {
            return Err(anyhow::anyhow!(
                "Review container produced no output (exit code {})",
                exit_code
            ));
        }

        let result: serde_json::Value = serde_json::from_str(stdout.trim()).context(format!(
            "Failed to parse review output as JSON: {}",
            &stdout[..200.min(stdout.len())]
        ))?;

        Ok(result)
    }

    async fn wait_for_container(docker: &Docker, container_name: &str) -> Result<i64> {
        let options = WaitContainerOptions {
            condition: "not-running",
        };

        let mut stream = docker.wait_container(container_name, Some(options));

        if let Some(result) = stream.next().await {
            match result {
                Ok(response) => Ok(response.status_code),
                Err(e) => Err(anyhow::anyhow!("Wait error: {}", e)),
            }
        } else {
            Err(anyhow::anyhow!("Container wait stream ended unexpectedly"))
        }
    }

    async fn collect_logs(docker: &Docker, container_name: &str) -> String {
        let options = LogsOptions::<String> {
            stdout: true,
            stderr: false,
            follow: false,
            ..Default::default()
        };

        let mut output = String::new();
        let mut stream = docker.logs(container_name, Some(options));

        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(log) => output.push_str(&log.to_string()),
                Err(e) => {
                    warn!("Error reading container logs: {}", e);
                    break;
                }
            }
        }

        output
    }
}

/// Build the LLM reviewer Docker image from docker/Dockerfile.llm-reviewer
pub async fn build_reviewer_image() -> Result<()> {
    let docker =
        Docker::connect_with_local_defaults().context("Failed to connect to Docker daemon")?;

    // Check if image already exists
    if docker.inspect_image(LLM_REVIEWER_IMAGE).await.is_ok() {
        info!("LLM reviewer image {} already exists", LLM_REVIEWER_IMAGE);
        return Ok(());
    }

    info!("Building LLM reviewer image {}...", LLM_REVIEWER_IMAGE);

    // Build image from project root context
    use bollard::image::BuildImageOptions;
    use std::io::Read;

    let project_root = std::env::current_dir().context("Failed to get current dir")?;

    // Create a tar archive with the Dockerfile and script
    let mut tar_builder = tar::Builder::new(Vec::new());

    // Add Dockerfile
    let dockerfile_path = project_root.join("docker/Dockerfile.llm-reviewer");
    if dockerfile_path.exists() {
        let mut file = std::fs::File::open(&dockerfile_path)?;
        let mut content = Vec::new();
        file.read_to_end(&mut content)?;

        let mut header = tar::Header::new_gnu();
        header.set_size(content.len() as u64);
        header.set_mode(0o644);
        header.set_cksum();
        tar_builder.append_data(&mut header, "Dockerfile", &content[..])?;
    } else {
        return Err(anyhow::anyhow!(
            "Dockerfile.llm-reviewer not found at {:?}",
            dockerfile_path
        ));
    }

    // Add the review script
    let script_path = project_root.join("scripts/llm_review.sh");
    if script_path.exists() {
        let mut file = std::fs::File::open(&script_path)?;
        let mut content = Vec::new();
        file.read_to_end(&mut content)?;

        let mut header = tar::Header::new_gnu();
        header.set_size(content.len() as u64);
        header.set_mode(0o755);
        header.set_cksum();
        tar_builder.append_data(&mut header, "scripts/llm_review.sh", &content[..])?;
    } else {
        return Err(anyhow::anyhow!(
            "llm_review.sh not found at {:?}",
            script_path
        ));
    }

    let tar_data = tar_builder.into_inner()?;

    let build_options = BuildImageOptions {
        dockerfile: "Dockerfile",
        t: LLM_REVIEWER_IMAGE,
        rm: true,
        ..Default::default()
    };

    let mut stream = docker.build_image(build_options, None, Some(tar_data.into()));

    while let Some(result) = stream.next().await {
        match result {
            Ok(output) => {
                if let Some(stream_str) = output.stream {
                    debug!("{}", stream_str.trim());
                }
                if let Some(err) = output.error {
                    return Err(anyhow::anyhow!("Docker build error: {}", err));
                }
            }
            Err(e) => {
                return Err(anyhow::anyhow!("Docker build stream error: {}", e));
            }
        }
    }

    info!(
        "LLM reviewer image {} built successfully",
        LLM_REVIEWER_IMAGE
    );
    Ok(())
}
