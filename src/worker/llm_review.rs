//! LLM Review Worker
//!
//! Background service that reviews pending agent submissions using a conversational
//! LLM agent that can explore the code via function calls before submitting a verdict.
//!
//! Flow:
//! 1. Polls DB for agents with llm_review_status='pending'
//! 2. Loads validation rules from the validation_rules table
//! 3. Creates a temporary workspace with the agent code
//! 4. Runs a conversation loop where the LLM can:
//!    - read_file(path) - Read a file from the workspace
//!    - list_files(path) - List files in a directory
//!    - grep(pattern, path) - Search for a pattern in files
//!    - dump_instruction(json) - Store an extracted instruction in the database
//!    - submit_verdict(approved, reason, violations) - Submit final verdict
//! 5. Updates DB based on the verdict
//!
//! Note: Instructions are stored in the database (llm_review_instructions table)
//! instead of a file for better analysis and querying capabilities.

use crate::storage::pg::PgStorage;
use crate::validation::package::PackageValidator;
use anyhow::{Context, Result};
use chrono::Utc;
use reqwest::Client;
use serde_json::json;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::time::interval;
use tracing::{debug, error, info, warn};

const REVIEW_TIMEOUT_SECS: u64 = 180;
const POLL_INTERVAL_SECS: u64 = 10;
const BATCH_SIZE: i64 = 10;
const CONCURRENT_REVIEWS: usize = 5;
const LLM_MODEL: &str = "moonshotai/Kimi-K2.5-TEE";
const CHUTES_API_URL: &str = "https://llm.chutes.ai/v1/chat/completions";
const MAX_CONVERSATION_TURNS: u32 = 150;
const MAX_LLM_REVIEW_RETRIES: i32 = 3;
const MAX_CHUTES_RETRIES: u32 = 10;
const CHUTES_INITIAL_RETRY_DELAY_MS: u64 = 1000;
const CHUTES_MAX_RETRY_DELAY_MS: u64 = 60_000;

/// Default system prompt for rules validation review (used if database has no custom prompt)
const DEFAULT_SYSTEM_PROMPT_RULES: &str = r#"You are a strict security code reviewer for a terminal-based AI agent challenge.

Your task is to analyze Python agent code and determine if it complies with ALL of the validation rules.

VALIDATION RULES:
{rules}

You have access to a workspace containing the agent's source code. Use the provided tools to explore and analyze the code:

- list_files(path): List files in a directory (use "." for root)
- read_file(path): Read the contents of a file
- grep(pattern, path): Search for a regex pattern in files (path can be "." for all files)
- dump_instruction(json): Store an extracted instruction/prompt variable as JSON in the database for analysis
- submit_verdict(approved, reason, violations): Submit your final verdict

REQUIRED ACTIONS:
1. As you analyze the code, track ALL prompt variables you detect (system prompts, user prompts, template variables, etc.)
2. For EACH detected variable, call dump_instruction with JSON format: {"variable": "name", "prompt": "content", "context": "where found"}
3. Your analysis MUST include:
   - Summary of what the code does
   - Any hardcoded API keys, secrets, or credentials found (CRITICAL - check thoroughly)
   - Security vulnerabilities or suspicious patterns
   - Validation rule violations
   - Files examined and their purposes
4. Dump all detected instructions to the database using dump_instruction BEFORE calling submit_verdict
5. Finally submit your verdict

WORKFLOW:
1. First, list the files to understand the project structure
2. Read the main entry point and any imported modules
3. Search for potentially dangerous patterns (subprocess, os.system, socket, requests, etc.)
4. Search for hardcoded secrets, API keys, tokens, passwords (check all string literals, variable assignments)
5. Track all prompt/template variables you encounter and dump each one using dump_instruction
6. Once you have analyzed all relevant code and dumped all instructions, submit your verdict

IMPORTANT:
- You MUST call dump_instruction for EACH detected prompt variable BEFORE calling submit_verdict
- You MUST check for hardcoded secrets/API keys thoroughly - this is CRITICAL
- You MUST call submit_verdict when you have finished your analysis
- If ANY rule is violated, set approved=false
- Be thorough - check all Python files in the project
- The violations array should list specific rule violations found"#;

/// Default system prompt for similarity/plagiarism review
const DEFAULT_SYSTEM_PROMPT_SIMILARITY: &str = r#"You are a code similarity reviewer for a terminal-based AI agent challenge.

Your task is to analyze agent code and compare it against reference agents to detect plagiarism and code similarity.

You have access to a workspace containing:
- The pending agent's source code at the root
- Reference agents in reference/<label>/ subdirectories for comparison

Use the provided tools to explore and analyze the code:

- list_files(path): List files in a directory (use "." for root, "reference/<label>" for reference agents)
- read_file(path): Read the contents of a file
- grep(pattern, path): Search for a regex pattern in files (path can be "." for all files)
- dump_instruction(json): Store a similarity finding as JSON in the database for analysis
- submit_verdict(approved, reason, violations): Submit your final verdict

REQUIRED ACTIONS:
1. Read both the pending agent code AND reference agent codes
2. As you detect similar patterns, structures, or copied code, track the findings
3. For EACH similarity finding, call dump_instruction with JSON format: {"variable": "similarity_type", "prompt": "description of similarity found", "files": "affected files"}
4. Your analysis MUST include:
   - Comparison summary between pending agent and each reference
   - Specific code sections that are similar or identical
   - Similarity percentage estimate for each file/section
   - Conclusion on whether plagiarism is likely
5. Dump all similarity findings to the database using dump_instruction BEFORE calling submit_verdict
6. Finally submit your verdict

WORKFLOW:
1. First, list the files to understand the project structure
2. Read the pending agent's main files
3. Read each reference agent's corresponding files
4. Compare code structure, variable names, logic patterns, comments
5. Document all similarities found using dump_instruction
6. Once comparison is complete, submit your verdict

IMPORTANT:
- You MUST call dump_instruction for EACH similarity finding BEFORE calling submit_verdict
- You MUST be thorough - compare all relevant files
- You MUST call submit_verdict when you have finished your analysis
- Set approved=false if significant plagiarism is detected
- The violations array should list specific similarities found"#;

/// Redact API keys and secrets from code before LLM review
/// This prevents the LLM from seeing actual API keys in agent code
fn redact_api_keys(code: &str) -> String {
    use regex::Regex;

    let patterns = [
        // Any variable containing API_KEY, SECRET, TOKEN, PASSWORD with assignment
        (
            r#"(?i)([A-Z_]*(?:API_KEY|SECRET|TOKEN|PASSWORD|CREDENTIAL|AUTH)[A-Z_]*)\s*[=:]\s*['"](.[^'"]{8,}?)['"]"#,
            "$1=\"[REDACTED]\"",
        ),
        // Any variable containing api_key, secret, token (lowercase)
        (
            r#"(?i)([a-z_]*(?:api_key|secret|token|password|credential|auth)[a-z_]*)\s*[=:]\s*['"](.[^'"]{8,}?)['"]"#,
            "$1=\"[REDACTED]\"",
        ),
        // Chutes API tokens (cpk_ prefix with any chars)
        (r#"cpk_[a-zA-Z0-9._\-]{10,}"#, "[REDACTED_CHUTES_KEY]"),
        // sk- prefix (OpenAI, etc) - extended pattern
        (r#"sk-[a-zA-Z0-9\-_]{20,}"#, "[REDACTED_SK_KEY]"),
        // sk-proj- prefix (OpenAI project keys)
        (r#"sk-proj-[a-zA-Z0-9\-_]{20,}"#, "[REDACTED_SK_PROJ_KEY]"),
        // Bearer tokens
        (
            r#"Bearer\s+[a-zA-Z0-9\-_.]{20,}"#,
            "Bearer [REDACTED_TOKEN]",
        ),
        // AWS keys
        (r#"AKIA[0-9A-Z]{16}"#, "[REDACTED_AWS_KEY]"),
        // AWS secret keys (40 char base64)
        (
            r#"(?i)(aws_secret_access_key|aws_secret)\s*[=:]\s*['"](.[^'"]{30,}?)['"]"#,
            "$1=\"[REDACTED_AWS_SECRET]\"",
        ),
        // Generic long hex strings (64 chars - likely hashes/keys)
        (r#"['"]([a-fA-F0-9]{64})['"]"#, "\"[REDACTED_HASH]\""),
        // Generic long alphanumeric in quotes (32+ chars, likely API keys)
        (r#"['"]([a-zA-Z0-9\-_]{32,})['"]"#, "\"[REDACTED_KEY]\""),
        // Anthropic keys
        (r#"sk-ant-[a-zA-Z0-9\-_]{20,}"#, "[REDACTED_ANTHROPIC_KEY]"),
        // Google API keys
        (r#"AIza[a-zA-Z0-9\-_]{35}"#, "[REDACTED_GOOGLE_KEY]"),
        // GitHub tokens
        (r#"ghp_[a-zA-Z0-9]{36}"#, "[REDACTED_GITHUB_TOKEN]"),
        (r#"gho_[a-zA-Z0-9]{36}"#, "[REDACTED_GITHUB_TOKEN]"),
        (r#"ghu_[a-zA-Z0-9]{36}"#, "[REDACTED_GITHUB_TOKEN]"),
        // Hugging Face tokens
        (r#"hf_[a-zA-Z0-9]{34}"#, "[REDACTED_HF_TOKEN]"),
        // OpenRouter keys
        (r#"sk-or-[a-zA-Z0-9\-_]{20,}"#, "[REDACTED_OPENROUTER_KEY]"),
        // Groq keys
        (r#"gsk_[a-zA-Z0-9]{20,}"#, "[REDACTED_GROQ_KEY]"),
    ];

    let mut result = code.to_string();
    for (pattern, replacement) in patterns {
        if let Ok(re) = Regex::new(pattern) {
            result = re.replace_all(&result, replacement).to_string();
        }
    }
    result
}

/// Tool definitions for the LLM
fn get_tools() -> serde_json::Value {
    json!([
        {
            "type": "function",
            "function": {
                "name": "list_files",
                "description": "List files and directories in the specified path",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Path relative to workspace root (use '.' for root)"
                        }
                    },
                    "required": ["path"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "read_file",
                "description": "Read the contents of a file",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Path to the file relative to workspace root"
                        }
                    },
                    "required": ["path"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "grep",
                "description": "Search for a pattern in files using regex",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "pattern": {
                            "type": "string",
                            "description": "Regex pattern to search for"
                        },
                        "path": {
                            "type": "string",
                            "description": "Path to search in (use '.' for all files)"
                        }
                    },
                    "required": ["pattern"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "write_file",
                "description": "Write content to a file in the workspace. Use for recording analysis results.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Path to the file relative to workspace root"
                        },
                        "content": {
                            "type": "string",
                            "description": "Content to write to the file"
                        }
                    },
                    "required": ["path", "content"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "dump_instruction",
                "description": "Store an extracted instruction or prompt variable as JSON in the database for analysis. You MUST call this for EACH instruction/prompt you detect. This is MANDATORY before submit_verdict.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "variable": {
                            "type": "string",
                            "description": "Name of the detected variable, prompt type, or similarity type (e.g., 'system_prompt', 'user_prompt_template', 'similarity_pattern', etc.)"
                        },
                        "prompt": {
                            "type": "string",
                            "description": "The actual content of the prompt, template, or description of what was found"
                        },
                        "context": {
                            "type": "string",
                            "description": "Optional context about where this was found (file path, function name, line number, etc.)"
                        },
                        "has_hardcoded_secrets": {
                            "type": "boolean",
                            "description": "Whether this prompt/instruction contains hardcoded API keys, secrets, or credentials"
                        }
                    },
                    "required": ["variable", "prompt"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "submit_verdict",
                "description": "Submit your final code review verdict. Call this when you have finished analyzing the code. Will fail if no instructions were reported via dump_instruction.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "approved": {
                            "type": "boolean",
                            "description": "true if the code passes ALL rules, false if ANY rule is violated"
                        },
                        "reason": {
                            "type": "string",
                            "description": "Brief explanation of the review decision"
                        },
                        "violations": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of specific rule violations found (empty array if approved)"
                        }
                    },
                    "required": ["approved", "reason", "violations"]
                }
            }
        }
    ])
}

/// Workspace for exploring agent code
struct ReviewWorkspace {
    root: PathBuf,
}

impl ReviewWorkspace {
    #[allow(deprecated)]
    fn new(source_code: &str, is_package: bool) -> Result<Self> {
        let tmp_dir = tempfile::tempdir().context("Failed to create temp dir")?;
        let root = tmp_dir.into_path(); // Take ownership of path, dir won't be auto-deleted

        if is_package {
            // Source code is already formatted as "### FILE: path ###\ncontent"
            for section in source_code.split("### FILE: ") {
                if section.trim().is_empty() {
                    continue;
                }
                if let Some(header_end) = section.find(" ###\n") {
                    let path = section[..header_end].trim();
                    let content = &section[header_end + 5..];

                    let file_path = root.join(path);
                    if let Some(parent) = file_path.parent() {
                        std::fs::create_dir_all(parent).ok();
                    }
                    std::fs::write(&file_path, content).ok();
                }
            }
        } else {
            // Single file submission
            let file_path = root.join("agent.py");
            std::fs::write(&file_path, source_code).ok();
        }

        Ok(Self { root })
    }

    fn list_files(&self, path: &str) -> String {
        let target = if path == "." || path.is_empty() {
            self.root.clone()
        } else {
            self.root.join(path)
        };

        if !target.exists() {
            return format!("Error: Path '{}' not found", path);
        }

        let mut entries = Vec::new();
        if let Ok(read_dir) = std::fs::read_dir(&target) {
            for entry in read_dir.flatten() {
                let name = entry.file_name().to_string_lossy().to_string();
                let is_dir = entry.file_type().map(|t| t.is_dir()).unwrap_or(false);
                if is_dir {
                    entries.push(format!("{}/", name));
                } else {
                    entries.push(name);
                }
            }
        }

        if entries.is_empty() {
            "Directory is empty".to_string()
        } else {
            entries.sort();
            entries.join("\n")
        }
    }

    fn read_file(&self, path: &str) -> String {
        // Security: reject path traversal and absolute paths at component level
        for component in Path::new(path).components() {
            match component {
                std::path::Component::ParentDir => {
                    return "Error: Access denied - path traversal detected".to_string();
                }
                std::path::Component::RootDir | std::path::Component::Prefix(_) => {
                    return "Error: Access denied - absolute paths not allowed".to_string();
                }
                _ => {}
            }
        }

        let file_path = self.root.join(path);
        if !file_path.starts_with(&self.root) {
            return "Error: Access denied - path traversal detected".to_string();
        }

        match std::fs::read_to_string(&file_path) {
            Ok(content) => {
                if content.len() > 50000 {
                    format!(
                        "{}...\n\n[Truncated - file too large ({} bytes)]",
                        &content[..50000],
                        content.len()
                    )
                } else {
                    content
                }
            }
            Err(e) => format!("Error reading file '{}': {}", path, e),
        }
    }

    fn write_file(&self, path: &str, content: &str) -> String {
        for component in Path::new(path).components() {
            match component {
                std::path::Component::ParentDir => {
                    return "Error: Access denied - path traversal detected".to_string();
                }
                std::path::Component::RootDir | std::path::Component::Prefix(_) => {
                    return "Error: Access denied - absolute paths not allowed".to_string();
                }
                _ => {}
            }
        }

        let file_path = self.root.join(path);
        if !file_path.starts_with(&self.root) {
            return "Error: Access denied - path traversal detected".to_string();
        }

        if let Some(parent) = file_path.parent() {
            if let Err(e) = std::fs::create_dir_all(parent) {
                return format!("Error creating directories for '{}': {}", path, e);
            }
        }

        match std::fs::write(&file_path, content) {
            Ok(_) => format!("Successfully wrote {} bytes to '{}'", content.len(), path),
            Err(e) => format!("Error writing file '{}': {}", path, e),
        }
    }

    fn grep(&self, pattern: &str, path: &str) -> String {
        let regex = match regex::Regex::new(pattern) {
            Ok(r) => r,
            Err(e) => return format!("Error: Invalid regex pattern: {}", e),
        };

        let mut results = Vec::new();
        let search_path = if path == "." || path.is_empty() {
            self.root.clone()
        } else {
            self.root.join(path)
        };

        self.grep_recursive(&search_path, &regex, &mut results);

        if results.is_empty() {
            format!("No matches found for pattern '{}'", pattern)
        } else {
            results.join("\n")
        }
    }

    fn grep_recursive(&self, path: &Path, regex: &regex::Regex, results: &mut Vec<String>) {
        if path.is_file() {
            if let Ok(content) = std::fs::read_to_string(path) {
                let rel_path = path.strip_prefix(&self.root).unwrap_or(path);
                for (line_num, line) in content.lines().enumerate() {
                    if regex.is_match(line) {
                        results.push(format!(
                            "{}:{}: {}",
                            rel_path.display(),
                            line_num + 1,
                            line.trim()
                        ));
                    }
                }
            }
        } else if path.is_dir() {
            if let Ok(entries) = std::fs::read_dir(path) {
                for entry in entries.flatten() {
                    self.grep_recursive(&entry.path(), regex, results);
                }
            }
        }
    }

    /// Add a reference agent's code into a `reference/<label>/` subfolder.
    /// Used for plagiarism comparison -- the LLM can read both the pending
    /// agent (at the workspace root) and each reference agent.
    #[allow(deprecated)]
    fn add_reference_agent(&self, label: &str, source_code: &str, is_package: bool) -> Result<()> {
        let ref_dir = self.root.join("reference").join(label);
        std::fs::create_dir_all(&ref_dir).context("Failed to create reference dir")?;

        if is_package {
            for section in source_code.split("### FILE: ") {
                if section.trim().is_empty() {
                    continue;
                }
                if let Some(header_end) = section.find(" ###\n") {
                    let path = section[..header_end].trim();
                    let content = &section[header_end + 5..];
                    let file_path = ref_dir.join(path);
                    if let Some(parent) = file_path.parent() {
                        std::fs::create_dir_all(parent).ok();
                    }
                    std::fs::write(&file_path, content).ok();
                }
            }
        } else {
            std::fs::write(ref_dir.join("agent.py"), source_code).ok();
        }

        Ok(())
    }

    fn cleanup(self) {
        let _ = std::fs::remove_dir_all(&self.root);
    }
}

pub struct LlmReviewWorkerConfig {
    pub poll_interval_secs: u64,
    pub batch_size: i64,
    pub chutes_api_token: String,
}

impl Default for LlmReviewWorkerConfig {
    fn default() -> Self {
        Self {
            poll_interval_secs: POLL_INTERVAL_SECS,
            batch_size: BATCH_SIZE,
            chutes_api_token: String::new(),
        }
    }
}

pub struct LlmReviewWorker {
    storage: Arc<PgStorage>,
    config: LlmReviewWorkerConfig,
    http_client: Client,
}

impl LlmReviewWorker {
    pub fn new(storage: Arc<PgStorage>, config: LlmReviewWorkerConfig) -> Self {
        let http_client = Client::builder()
            .timeout(Duration::from_secs(REVIEW_TIMEOUT_SECS))
            .build()
            .expect("Failed to create HTTP client");

        Self {
            storage,
            config,
            http_client,
        }
    }

    pub fn from_env(storage: Arc<PgStorage>) -> Option<Self> {
        let token = std::env::var("EXTRA_CHUTES_API_TOKEN")
            .or_else(|_| std::env::var("CHUTES_API_KEY"))
            .ok()?;

        if token.is_empty() {
            return None;
        }

        Some(Self::new(
            storage,
            LlmReviewWorkerConfig {
                chutes_api_token: token,
                ..Default::default()
            },
        ))
    }

    pub async fn run(&self) {
        info!(
            "LLM Review worker started (pool={}, poll={}s, model={}, max_turns={})",
            CONCURRENT_REVIEWS, self.config.poll_interval_secs, LLM_MODEL, MAX_CONVERSATION_TURNS
        );

        let semaphore = Arc::new(tokio::sync::Semaphore::new(CONCURRENT_REVIEWS));
        let mut join_set: tokio::task::JoinSet<(String, String, Result<ReviewResult>, String)> =
            tokio::task::JoinSet::new();
        let mut ticker = interval(Duration::from_secs(self.config.poll_interval_secs));

        loop {
            // Reap completed tasks (non-blocking)
            while let Some(res) = join_set.try_join_next() {
                self.handle_review_result(res).await;
            }

            ticker.tick().await;

            if let Err(e) = self.fill_pool(&semaphore, &mut join_set).await {
                error!("Error filling review pool: {}", e);
            }
        }
    }

    async fn fill_pool(
        &self,
        semaphore: &Arc<tokio::sync::Semaphore>,
        join_set: &mut tokio::task::JoinSet<(String, String, Result<ReviewResult>, String)>,
    ) -> Result<()> {
        let available = semaphore.available_permits();
        if available == 0 {
            debug!("All {} review slots occupied", CONCURRENT_REVIEWS);
            return Ok(());
        }

        let rules = self.storage.get_active_validation_rules().await?;
        if rules.is_empty() {
            debug!("No active validation rules found - skipping LLM review cycle");
            return Ok(());
        }

        let pending = self
            .storage
            .claim_pending_llm_reviews(available as i64, MAX_LLM_REVIEW_RETRIES)
            .await?;

        if pending.is_empty() {
            debug!("No pending LLM reviews");
            return Ok(());
        }

        info!(
            "Claimed {} agents for LLM review ({}/{} slots free)",
            pending.len(),
            available,
            CONCURRENT_REVIEWS
        );

        let rules_prompt_template = self
            .storage
            .get_llm_review_config("system_prompt_rules")
            .await
            .ok()
            .flatten()
            .unwrap_or_else(|| DEFAULT_SYSTEM_PROMPT_RULES.to_string());

        let similarity_prompt_template = self
            .storage
            .get_llm_review_config("system_prompt_similarity")
            .await
            .ok()
            .flatten()
            .unwrap_or_else(|| DEFAULT_SYSTEM_PROMPT_SIMILARITY.to_string());

        let formatted_rules = rules
            .iter()
            .enumerate()
            .map(|(i, r)| format!("{}. {}", i + 1, r))
            .collect::<Vec<_>>()
            .join("\n");

        for submission in pending {
            let agent_hash = submission.agent_hash.clone();
            let short_hash = agent_hash[..16.min(agent_hash.len())].to_string();

            let review_code = if submission.is_package {
                match Self::extract_package_code(&submission) {
                    Ok(code) => code,
                    Err(e) => {
                        error!("Failed to extract package for {}: {}", short_hash, e);
                        if let Err(e2) = self.storage.reset_llm_review_for_retry(&agent_hash).await
                        {
                            error!("Failed to reset review status for {}: {}", short_hash, e2);
                        }
                        continue;
                    }
                }
            } else {
                submission.source_code.clone()
            };

            if review_code.trim().is_empty() {
                warn!("Empty review code for agent {}, skipping", short_hash);
                if let Err(e) = self.storage.reset_llm_review_for_retry(&agent_hash).await {
                    error!("Failed to reset review status for {}: {}", short_hash, e);
                }
                continue;
            }

            let redacted_code = redact_api_keys(&review_code);

            // Build plagiarism context if flagged
            let plagiarism_info = self
                .build_plagiarism_context(&agent_hash, &short_hash, &similarity_prompt_template)
                .await;

            let rules_prompt = rules_prompt_template.replace("{rules}", &formatted_rules);

            let storage = Arc::clone(&self.storage);
            let http_client = self.http_client.clone();
            let chutes_token = self.config.chutes_api_token.clone();
            let permit = semaphore.clone().acquire_owned().await?;

            join_set.spawn(async move {
                let _permit = permit;

                let worker = LlmReviewWorker {
                    storage,
                    config: LlmReviewWorkerConfig {
                        poll_interval_secs: POLL_INTERVAL_SECS,
                        batch_size: BATCH_SIZE,
                        chutes_api_token: chutes_token,
                    },
                    http_client,
                };

                let review_type;

                // If flagged for plagiarism: run similarity review first
                if let Some((sim_prompt, ref_agents)) = plagiarism_info {
                    review_type = "similarity+rules".to_string();
                    info!(
                        "Agent {} starting similarity review ({} bytes, 1 reference agent)",
                        short_hash, redacted_code.len()
                    );

                    let sim_result = worker
                        .review_code(
                            &agent_hash,
                            &redacted_code,
                            submission.is_package,
                            &sim_prompt,
                            &ref_agents,
                            "similarity",
                        )
                        .await;

                    match sim_result {
                        Ok(result) => {
                            let approved = result.verdict["approved"].as_bool().unwrap_or(false);
                            if !approved {
                                // Similarity rejected -> stop, don't do rules review
                                let reason = result.verdict["reason"]
                                    .as_str()
                                    .unwrap_or("Plagiarism confirmed by similarity review")
                                    .to_string();
                                warn!(
                                    "Agent {} REJECTED by similarity review: {}",
                                    short_hash, reason
                                );
                                return (agent_hash, short_hash, Ok(result), "similarity_rejected".to_string());
                            }
                            info!(
                                "Agent {} APPROVED by similarity review, proceeding to rules review",
                                short_hash
                            );
                            // Store similarity approval
                            if let Err(e) = worker
                                .storage
                                .update_llm_similarity_review_result(
                                    &agent_hash,
                                    "approved",
                                    LLM_MODEL,
                                    &result.verdict,
                                )
                                .await
                            {
                                error!(
                                    "Failed to store similarity approval for {}: {}",
                                    short_hash, e
                                );
                            }
                        }
                        Err(e) => {
                            error!("Similarity review failed for {}: {}", short_hash, e);
                            return (agent_hash, short_hash, Err(e), "similarity_error".to_string());
                        }
                    }

                    // Similarity approved -> continue with rules review
                    info!(
                        "Agent {} starting rules review ({} bytes)",
                        short_hash, redacted_code.len()
                    );
                    let rules_result = worker
                        .review_code(
                            &agent_hash,
                            &redacted_code,
                            submission.is_package,
                            &rules_prompt,
                            &[],
                            "rules",
                        )
                        .await;

                    (agent_hash, short_hash, rules_result, review_type)
                } else {
                    // Not flagged: rules review only
                    review_type = "rules".to_string();
                    info!(
                        "Agent {} starting rules review ({} bytes)",
                        short_hash, redacted_code.len()
                    );
                    let result = worker
                        .review_code(
                            &agent_hash,
                            &redacted_code,
                            submission.is_package,
                            &rules_prompt,
                            &[],
                            "rules",
                        )
                        .await;

                    (agent_hash, short_hash, result, review_type)
                }
            });
        }

        Ok(())
    }

    /// Build plagiarism context (similarity prompt + reference agent) if agent is flagged.
    /// Returns None if not flagged. Limits to 1 reference agent (most similar).
    async fn build_plagiarism_context(
        &self,
        agent_hash: &str,
        short_hash: &str,
        similarity_prompt_template: &str,
    ) -> Option<(String, Vec<(String, String, bool)>)> {
        let report = match self.storage.get_plagiarism_report(agent_hash).await {
            Ok(Some(r)) => r,
            _ => return None,
        };

        if report["status"].as_str() != Some("flagged") {
            return None;
        }

        let score = report["score"].as_f64().unwrap_or(0.0);
        info!(
            "Agent {} flagged for plagiarism ({:.1}%), loading most similar reference agent",
            short_hash, score
        );

        // Get the single most similar matched agent hash
        let matched_hashes: Vec<String> = report["matches"]
            .as_array()
            .map(|arr| {
                let mut seen = std::collections::HashSet::new();
                arr.iter()
                    .filter_map(|m| m["matched_agent_hash"].as_str().map(String::from))
                    .filter(|h| seen.insert(h.clone()))
                    .take(1)
                    .collect()
            })
            .unwrap_or_default();

        let mut reference_agents: Vec<(String, String, bool)> = Vec::new();
        if let Ok(refs) = self
            .storage
            .get_reference_agents_by_hashes(&matched_hashes, 1)
            .await
        {
            for ref_submission in refs {
                let ref_hash = &ref_submission.agent_hash;
                let label = ref_hash[..16.min(ref_hash.len())].to_string();
                let ref_code = if ref_submission.is_package {
                    match Self::extract_package_code(&ref_submission) {
                        Ok(code) => redact_api_keys(&code),
                        Err(e) => {
                            warn!("Failed to extract reference agent {}: {}", label, e);
                            continue;
                        }
                    }
                } else {
                    redact_api_keys(&ref_submission.source_code)
                };
                reference_agents.push((label, ref_code, ref_submission.is_package));
            }
        }

        if reference_agents.is_empty() {
            warn!(
                "Agent {} flagged but no reference agents could be loaded, skipping similarity review",
                short_hash
            );
            return None;
        }

        // Build effective similarity prompt with plagiarism context
        let mut effective_prompt = similarity_prompt_template.to_string();
        if let Ok(config) = self.storage.get_plagiarism_config().await {
            if !config.prompt_template.is_empty() {
                let ref_labels: Vec<String> =
                    reference_agents.iter().map(|(l, _, _)| l.clone()).collect();

                let matches_summary = report["matches"]
                    .as_array()
                    .map(|arr| {
                        arr.iter()
                            .take(10)
                            .map(|m| {
                                format!(
                                    "- {} in {}:{}-{} matches {}:{}-{} ({} nodes)",
                                    m["node_type"].as_str().unwrap_or("?"),
                                    m["pending_file"].as_str().unwrap_or("?"),
                                    m["pending_lines"]
                                        .as_array()
                                        .and_then(|a| a.first())
                                        .and_then(|v| v.as_u64())
                                        .unwrap_or(0),
                                    m["pending_lines"]
                                        .as_array()
                                        .and_then(|a| a.get(1))
                                        .and_then(|v| v.as_u64())
                                        .unwrap_or(0),
                                    m["matched_file"].as_str().unwrap_or("?"),
                                    m["matched_lines"]
                                        .as_array()
                                        .and_then(|a| a.first())
                                        .and_then(|v| v.as_u64())
                                        .unwrap_or(0),
                                    m["matched_lines"]
                                        .as_array()
                                        .and_then(|a| a.get(1))
                                        .and_then(|v| v.as_u64())
                                        .unwrap_or(0),
                                    m["subtree_size"].as_u64().unwrap_or(0),
                                )
                            })
                            .collect::<Vec<_>>()
                            .join("\n")
                    })
                    .unwrap_or_default();

                let plagiarism_context = config
                    .prompt_template
                    .replace("{match_percent}", &format!("{:.1}", score))
                    .replace("{matches_summary}", &matches_summary)
                    .replace("{reference_labels}", &ref_labels.join(", "));

                effective_prompt = format!(
                    "{}\n\n⚠️ PLAGIARISM CONTEXT:\n{}",
                    effective_prompt, plagiarism_context
                );
            }
        }

        Some((effective_prompt, reference_agents))
    }

    /// Handle the result of a completed review task
    async fn handle_review_result(
        &self,
        res: Result<(String, String, Result<ReviewResult>, String), tokio::task::JoinError>,
    ) {
        match res {
            Ok((agent_hash, short_hash, review_result, review_type)) => {
                match review_result {
                    Ok(result) => {
                        let verdict = &result.verdict;
                        let approved = verdict["approved"].as_bool().unwrap_or(false);
                        let reason = verdict["reason"]
                            .as_str()
                            .unwrap_or("No reason provided")
                            .to_string();
                        let violations: Vec<String> = verdict["violations"]
                            .as_array()
                            .map(|arr| {
                                arr.iter()
                                    .filter_map(|v| v.as_str().map(String::from))
                                    .collect()
                            })
                            .unwrap_or_default();

                        if review_type == "similarity_rejected" {
                            // Similarity review rejected -> reject agent entirely
                            warn!(
                                "Agent {} REJECTED by similarity review: {} ({} turns, {} tool calls)",
                                short_hash, reason, result.turns_count, result.tool_calls_count
                            );
                            if let Err(e) = self
                                .storage
                                .update_llm_similarity_review_rejected(
                                    &agent_hash,
                                    LLM_MODEL,
                                    verdict,
                                    &format!("Plagiarism confirmed: {}", reason),
                                )
                                .await
                            {
                                error!(
                                    "Failed to update similarity rejected status for {}: {}",
                                    short_hash, e
                                );
                            }
                        } else if approved {
                            info!(
                                "Agent {} APPROVED by LLM {} review ({} turns, {} tool calls)",
                                short_hash,
                                review_type,
                                result.turns_count,
                                result.tool_calls_count
                            );
                            if let Err(e) = self
                                .storage
                                .update_llm_review_result(
                                    &agent_hash,
                                    "approved",
                                    LLM_MODEL,
                                    verdict,
                                )
                                .await
                            {
                                error!(
                                    "Failed to update approved status for {}: {}",
                                    short_hash, e
                                );
                            }
                        } else {
                            warn!(
                                "Agent {} REJECTED by LLM {} review: {} (violations: {:?}, {} turns, {} tool calls)",
                                short_hash, review_type, reason, violations, result.turns_count, result.tool_calls_count
                            );
                            if let Err(e) = self
                                .storage
                                .update_llm_review_rejected(
                                    &agent_hash,
                                    LLM_MODEL,
                                    verdict,
                                    &reason,
                                )
                                .await
                            {
                                error!(
                                    "Failed to update rejected status for {}: {}",
                                    short_hash, e
                                );
                            }
                        }
                    }
                    Err(e) => {
                        error!("LLM review failed for agent {}: {}", short_hash, e);
                        if let Err(e2) = self.storage.reset_llm_review_for_retry(&agent_hash).await
                        {
                            error!("Failed to reset review status for {}: {}", short_hash, e2);
                        }
                    }
                }
            }
            Err(e) => {
                error!("Review task panicked: {}", e);
            }
        }
    }

    fn extract_package_code(submission: &crate::storage::pg::PendingLlmReview) -> Result<String> {
        let pkg_data = submission
            .package_data
            .as_deref()
            .context("Package data is missing for package submission")?;
        let format = submission.package_format.as_deref().unwrap_or("zip");
        let entry = submission.entry_point.as_deref().unwrap_or("agent.py");

        let validator = PackageValidator::new();
        let (_validation, files) = validator
            .validate_and_extract(pkg_data, format, entry)
            .context("Failed to validate and extract package")?;

        let code = files
            .iter()
            .filter(|f| f.is_python)
            .map(|f| {
                format!(
                    "### FILE: {} ###\n{}",
                    f.path,
                    String::from_utf8_lossy(&f.content)
                )
            })
            .collect::<Vec<_>>()
            .join("\n\n");

        if code.is_empty() {
            anyhow::bail!("No Python files found in package");
        }

        Ok(code)
    }

    async fn review_code(
        &self,
        agent_hash: &str,
        source_code: &str,
        is_package: bool,
        system_prompt: &str,
        reference_agents: &[(String, String, bool)], // (label, code, is_package)
        review_type: &str,                           // "rules" or "similarity"
    ) -> Result<ReviewResult> {
        let workspace = ReviewWorkspace::new(source_code, is_package)
            .context("Failed to create review workspace")?;

        // Add reference agents to workspace for plagiarism comparison
        for (label, ref_code, ref_is_package) in reference_agents {
            if let Err(e) = workspace.add_reference_agent(label, ref_code, *ref_is_package) {
                warn!("Failed to add reference agent {}: {}", label, e);
            }
        }

        // Clear any existing instructions for this agent before starting new review
        if review_type == "similarity" {
            let _ = self
                .storage
                .clear_llm_similarity_instructions(agent_hash)
                .await;
        } else {
            let _ = self.storage.clear_llm_review_instructions(agent_hash).await;
        }

        let tools = get_tools();

        let user_message = if reference_agents.is_empty() {
            "Please review the agent code in the workspace. Start by listing the files, then read and analyze them. Track all prompt variables you detect and dump each one using dump_instruction with JSON format including 'variable' and 'prompt' fields. When done, call submit_verdict with your decision.".to_string()
        } else {
            let ref_labels: Vec<&str> = reference_agents
                .iter()
                .map(|(l, _, _)| l.as_str())
                .collect();
            format!(
                "Please review the agent code in the workspace for similarity comparison. The agent's code is at the root. \
                 Reference agents are in reference/ subdirectories: [{}]. \
                 First list the files, read the agent code AND the reference code, compare them, \
                 and dump each similarity finding using dump_instruction with JSON format including 'variable' and 'prompt' fields. When done, call submit_verdict with your decision.",
                ref_labels.join(", ")
            )
        };

        let mut messages = vec![
            json!({"role": "system", "content": system_prompt}),
            json!({"role": "user", "content": user_message}),
        ];

        let mut verdict: Option<serde_json::Value> = None;
        let mut dumped_instructions_count: i32 = 0;
        let mut tool_calls_count: i32 = 0;
        let mut turns_count: i32 = 0;
        let started_at = Utc::now();
        let start_time = Instant::now();

        for turn in 0..MAX_CONVERSATION_TURNS {
            turns_count = turn as i32 + 1;
            debug!("LLM review turn {}/{}", turn + 1, MAX_CONVERSATION_TURNS);

            let payload = json!({
                "model": LLM_MODEL,
                "messages": messages,
                "tools": tools,
                "max_tokens": 4096,
                "temperature": 0.1
            });

            // Retry loop for Chutes API errors with exponential backoff
            let mut response = None;
            let mut last_error = None;

            for attempt in 0..=MAX_CHUTES_RETRIES {
                let is_retry = attempt > 0;
                match self
                    .http_client
                    .post(CHUTES_API_URL)
                    .bearer_auth(&self.config.chutes_api_token)
                    .json(&payload)
                    .send()
                    .await
                {
                    Ok(resp) => {
                        let status = resp.status();
                        let is_retryable = status == reqwest::StatusCode::TOO_MANY_REQUESTS
                            || status.is_server_error(); // 429, 500, 502, 503, etc.

                        if is_retryable && attempt < MAX_CHUTES_RETRIES {
                            last_error =
                                Some(format!("Chutes API {} on attempt {}", status, attempt + 1));
                            let delay = (CHUTES_INITIAL_RETRY_DELAY_MS
                                * 2u64.saturating_pow(attempt))
                            .min(CHUTES_MAX_RETRY_DELAY_MS);
                            if is_retry {
                                warn!(
                                    "Chutes API {} for agent {}, retry {}/{} in {}ms",
                                    status,
                                    &agent_hash[..16.min(agent_hash.len())],
                                    attempt + 1,
                                    MAX_CHUTES_RETRIES,
                                    delay
                                );
                            }
                            tokio::time::sleep(Duration::from_millis(delay)).await;
                            continue;
                        }
                        response = Some(resp);
                        break;
                    }
                    Err(e) => {
                        last_error =
                            Some(format!("Network error on attempt {}: {}", attempt + 1, e));
                        if attempt < MAX_CHUTES_RETRIES {
                            let delay = (CHUTES_INITIAL_RETRY_DELAY_MS
                                * 2u64.saturating_pow(attempt))
                            .min(CHUTES_MAX_RETRY_DELAY_MS);
                            warn!(
                                "Chutes API network error for agent {}, retry {}/{} in {}ms: {}",
                                &agent_hash[..16.min(agent_hash.len())],
                                attempt + 1,
                                MAX_CHUTES_RETRIES,
                                delay,
                                e
                            );
                            tokio::time::sleep(Duration::from_millis(delay)).await;
                            continue;
                        }
                        break;
                    }
                }
            }

            let response = response.ok_or_else(|| {
                anyhow::anyhow!(
                    "Chutes API request failed after {} retries: {}",
                    MAX_CHUTES_RETRIES,
                    last_error.unwrap_or_else(|| "Unknown error".to_string())
                )
            })?;

            let status = response.status();
            if !status.is_success() {
                let error_body = response
                    .text()
                    .await
                    .unwrap_or_else(|_| "Unknown error".to_string());
                let error_msg = format!(
                    "Chutes API error HTTP {} after retries: {}",
                    status,
                    &error_body[..500.min(error_body.len())]
                );

                let duration_ms = start_time.elapsed().as_millis() as i32;
                let _ = self
                    .storage
                    .save_llm_review_log(
                        agent_hash,
                        None,
                        &json!(messages),
                        tool_calls_count,
                        turns_count,
                        None,
                        LLM_MODEL,
                        started_at,
                        Some(duration_ms),
                        Some(&error_msg),
                    )
                    .await;

                anyhow::bail!("{}", error_msg);
            }

            let response_json: serde_json::Value = response
                .json()
                .await
                .context("Failed to parse Chutes API response")?;

            let message = &response_json["choices"][0]["message"];
            let tool_calls = message["tool_calls"].as_array();

            // Add assistant message to history
            messages.push(message.clone());

            if let Some(calls) = tool_calls {
                if calls.is_empty() {
                    // No tool calls - prompt the LLM to continue
                    debug!("No tool calls in response, prompting for verdict");
                    messages.push(json!({
                        "role": "user",
                        "content": "Please continue your analysis or call submit_verdict if you have finished reviewing the code."
                    }));
                    continue;
                }

                // Process each tool call
                for call in calls {
                    tool_calls_count += 1;
                    let tool_id = call["id"].as_str().unwrap_or("");
                    let func_name = call["function"]["name"].as_str().unwrap_or("");
                    let args_str = call["function"]["arguments"].as_str().unwrap_or("{}");

                    let args: serde_json::Value =
                        serde_json::from_str(args_str).unwrap_or(json!({}));

                    debug!("Tool call: {}({})", func_name, args_str);

                    let result = match func_name {
                        "list_files" => {
                            let path = args["path"].as_str().unwrap_or(".");
                            workspace.list_files(path)
                        }
                        "read_file" => {
                            let path = args["path"].as_str().unwrap_or("");
                            workspace.read_file(path)
                        }
                        "grep" => {
                            let pattern = args["pattern"].as_str().unwrap_or("");
                            let path = args["path"].as_str().unwrap_or(".");
                            workspace.grep(pattern, path)
                        }
                        "write_file" => {
                            let path = args["path"].as_str().unwrap_or("");
                            let content = args["content"].as_str().unwrap_or("");
                            workspace.write_file(path, content)
                        }
                        "dump_instruction" => {
                            let store_result = if review_type == "similarity" {
                                self.storage
                                    .store_llm_similarity_instruction(agent_hash, &args)
                                    .await
                            } else {
                                self.storage
                                    .store_llm_review_instruction(agent_hash, &args)
                                    .await
                            };
                            if let Err(e) = store_result {
                                warn!(
                                    "Failed to store {} instruction for {}: {}",
                                    review_type, agent_hash, e
                                );
                                format!("Error storing instruction: {}", e)
                            } else {
                                dumped_instructions_count += 1;
                                let variable = args["variable"].as_str().unwrap_or("unknown");
                                debug!(
                                    "Stored {} instruction '{}' for agent {} (total: {})",
                                    review_type, variable, agent_hash, dumped_instructions_count
                                );
                                format!(
                                    "Instruction '{}' stored (total: {})",
                                    variable, dumped_instructions_count
                                )
                            }
                        }
                        "submit_verdict" => {
                            if dumped_instructions_count == 0 {
                                "Error: You MUST call dump_instruction at least once to report the prompt variables found in the code before submitting your verdict. Please analyze the code and report all instructions/prompts/templates you found.".to_string()
                            } else {
                                info!(
                                    "LLM submitted verdict: approved={}, {} instructions dumped",
                                    args["approved"], dumped_instructions_count
                                );
                                verdict = Some(args.clone());
                                format!(
                                    "Verdict received with {} dumped instructions.",
                                    dumped_instructions_count
                                )
                            }
                        }
                        _ => format!("Unknown function: {}", func_name),
                    };

                    // Add tool response to messages
                    messages.push(json!({
                        "role": "tool",
                        "tool_call_id": tool_id,
                        "content": result
                    }));

                    if verdict.is_some() {
                        break;
                    }
                }

                if verdict.is_some() {
                    break;
                }
            } else {
                // No tool_calls field at all - prompt for verdict
                debug!("No tool_calls in response, prompting for verdict");
                messages.push(json!({
                    "role": "user",
                    "content": "You must use the available tools to analyze the code. Please call list_files to see the project structure, or if you have finished your analysis, call submit_verdict with your decision."
                }));
            }
        }

        workspace.cleanup();
        let duration_ms = start_time.elapsed().as_millis() as i32;

        // Save conversation log to database
        let error_msg = if verdict.is_none() {
            Some(format!(
                "LLM did not submit verdict after {} turns",
                MAX_CONVERSATION_TURNS
            ))
        } else {
            None
        };

        if let Err(e) = self
            .storage
            .save_llm_review_log(
                agent_hash,
                None,
                &json!(messages),
                tool_calls_count,
                turns_count,
                verdict.as_ref(),
                LLM_MODEL,
                started_at,
                Some(duration_ms),
                error_msg.as_deref(),
            )
            .await
        {
            warn!("Failed to save LLM review log: {}", e);
        }

        match verdict {
            Some(v) => Ok(ReviewResult {
                verdict: v,
                conversation: json!(messages),
                tool_calls_count,
                turns_count,
                duration_ms,
            }),
            None => anyhow::bail!(
                "LLM did not submit verdict after {} turns",
                MAX_CONVERSATION_TURNS
            ),
        }
    }
}

/// Result of an LLM review including conversation log
struct ReviewResult {
    verdict: serde_json::Value,
    #[allow(dead_code)]
    conversation: serde_json::Value,
    #[allow(dead_code)]
    tool_calls_count: i32,
    #[allow(dead_code)]
    turns_count: i32,
    #[allow(dead_code)]
    duration_ms: i32,
}
