//! LLM Review Worker
//!
//! Background service that reviews pending agent submissions using an LLM
//! agent (docker/llm-reviewer/) inside an isolated Docker container.
//!
//! Flow:
//! 1. Polls DB for agents with llm_review_status='pending'
//! 2. Loads validation rules from the validation_rules table
//! 3. Writes agent source code to a temp directory
//! 4. Launches a Docker container with the agent code mounted read-only
//! 5. Container runs the reviewer agent which calls Chutes API (Kimi-K2.5-TEE)
//! 6. Container outputs JSON verdict on stdout
//! 7. Updates DB: approved -> llm_review_status='approved', rejected -> flagged=true

use crate::container::llm_reviewer::{build_llm_reviewer_image, LLM_REVIEWER_IMAGE};
use crate::storage::pg::PgStorage;
use anyhow::{Context, Result};
use bollard::container::{
    Config, CreateContainerOptions, LogsOptions, RemoveContainerOptions, WaitContainerOptions,
};
use bollard::Docker;
use futures::StreamExt;
use std::sync::Arc;
use std::time::Duration;
use tokio::time::interval;
use tracing::{debug, error, info, warn};

const REVIEW_TIMEOUT_SECS: u64 = 180;
const POLL_INTERVAL_SECS: u64 = 10;
const BATCH_SIZE: i64 = 5;
const LLM_MODEL: &str = "moonshotai/Kimi-K2.5-TEE";
/// Maximum number of retries for failed LLM reviews before giving up
const MAX_LLM_REVIEW_RETRIES: i32 = 3;

pub struct LlmReviewWorkerConfig {
    pub poll_interval_secs: u64,
    pub batch_size: i64,
    pub chutes_api_token: String,
}

impl Default for LlmReviewWorkerConfig {
    fn default() -> Self {
        Self {
            poll_interval_secs: POLL_INTERVAL_SECS,
            batch_size: BATCH_SIZE,
            chutes_api_token: String::new(),
        }
    }
}

pub struct LlmReviewWorker {
    storage: Arc<PgStorage>,
    config: LlmReviewWorkerConfig,
}

impl LlmReviewWorker {
    pub fn new(storage: Arc<PgStorage>, config: LlmReviewWorkerConfig) -> Self {
        Self { storage, config }
    }

    /// Create from environment (reads CHUTES_API_TOKEN)
    pub fn from_env(storage: Arc<PgStorage>) -> Option<Self> {
        let token = std::env::var("CHUTES_API_TOKEN")
            .or_else(|_| std::env::var("CHUTES_API_KEY"))
            .ok()?;

        if token.is_empty() {
            return None;
        }

        Some(Self::new(
            storage,
            LlmReviewWorkerConfig {
                chutes_api_token: token,
                ..Default::default()
            },
        ))
    }

    /// Start the worker (runs forever)
    pub async fn run(&self) {
        info!(
            "LLM Review worker started (poll={}s, batch={}, model={})",
            self.config.poll_interval_secs, self.config.batch_size, LLM_MODEL
        );

        let mut ticker = interval(Duration::from_secs(self.config.poll_interval_secs));

        loop {
            ticker.tick().await;

            if let Err(e) = self.process_pending().await {
                error!("Error processing pending LLM reviews: {}", e);
            }
        }
    }

    async fn process_pending(&self) -> Result<()> {
        // 1. FIRST check validation rules exist before claiming any agents
        // This prevents agents from being stuck if rules are empty
        let rules = self.storage.get_active_validation_rules().await?;
        if rules.is_empty() {
            debug!("No active validation rules found - skipping LLM review cycle");
            return Ok(());
        }

        // 2. Only NOW claim submissions (after we know we can process them)
        let pending = self
            .storage
            .claim_pending_llm_reviews(self.config.batch_size, MAX_LLM_REVIEW_RETRIES)
            .await?;

        if pending.is_empty() {
            debug!("No pending LLM reviews");
            return Ok(());
        }

        info!("Claimed {} agents for LLM review", pending.len());

        let formatted_rules = rules
            .iter()
            .enumerate()
            .map(|(i, r)| format!("{}. {}", i + 1, r))
            .collect::<Vec<_>>()
            .join("\n");

        for submission in pending {
            let agent_hash = &submission.agent_hash;
            let short_hash = &agent_hash[..16.min(agent_hash.len())];

            info!("Reviewing agent {} with {}", short_hash, LLM_MODEL);

            match self
                .review_in_container(&submission.source_code, &formatted_rules)
                .await
            {
                Ok(result) => {
                    let approved = result["approved"].as_bool().unwrap_or(false);
                    let reason = result["reason"]
                        .as_str()
                        .unwrap_or("No reason provided")
                        .to_string();
                    let violations: Vec<String> = result["violations"]
                        .as_array()
                        .map(|arr| {
                            arr.iter()
                                .filter_map(|v| v.as_str().map(String::from))
                                .collect()
                        })
                        .unwrap_or_default();

                    if approved {
                        info!("Agent {} APPROVED by LLM review", short_hash);
                        if let Err(e) = self
                            .storage
                            .update_llm_review_result(agent_hash, "approved", LLM_MODEL, &result)
                            .await
                        {
                            error!("Failed to update approved status for {}: {}", short_hash, e);
                        }
                    } else {
                        warn!(
                            "Agent {} REJECTED by LLM review: {} (violations: {:?})",
                            short_hash, reason, violations
                        );
                        if let Err(e) = self
                            .storage
                            .update_llm_review_rejected(agent_hash, LLM_MODEL, &result, &reason)
                            .await
                        {
                            error!("Failed to update rejected status for {}: {}", short_hash, e);
                        }
                    }
                }
                Err(e) => {
                    error!("LLM review failed for agent {}: {}", short_hash, e);
                    // On error, reset for retry (sets llm_review_called = FALSE)
                    if let Err(e2) = self.storage.reset_llm_review_for_retry(agent_hash).await {
                        error!("Failed to reset review status for {}: {}", short_hash, e2);
                    }
                }
            }
        }

        Ok(())
    }

    /// Write agent source code to a temp directory and launch a Docker container to review it.
    /// The container mounts the agent directory read-only at /review/agent.
    async fn review_in_container(
        &self,
        source_code: &str,
        rules: &str,
    ) -> Result<serde_json::Value> {
        // Ensure LLM reviewer image is built before running reviews
        // This is idempotent and will skip building if image already exists and unchanged
        let backend = crate::container::backend::create_backend()
            .await
            .context("Failed to create container backend for LLM reviewer image build")?;

        build_llm_reviewer_image(&backend)
            .await
            .context("Failed to ensure LLM reviewer image is built")?;

        let docker =
            Docker::connect_with_local_defaults().context("Failed to connect to Docker daemon")?;

        // Write source code to a temp directory (simulating an agent workspace)
        let tmp_dir = tempfile::tempdir().context("Failed to create temp dir")?;
        let agent_dir = tmp_dir.path().join("agent");
        std::fs::create_dir_all(&agent_dir).context("Failed to create agent dir")?;
        let code_path = agent_dir.join("agent.py");
        std::fs::write(&code_path, source_code).context("Failed to write agent code")?;

        let container_name = format!("llm-review-{}", uuid::Uuid::new_v4());

        let host_agent_dir = agent_dir.to_str().context("Invalid temp path")?.to_string();

        let env_vars = vec![
            format!("CHUTES_API_TOKEN={}", self.config.chutes_api_token),
            format!("RULES={}", rules),
            "AGENT_CODE_DIR=/review/agent".to_string(),
        ];

        let container_config = Config {
            image: Some(LLM_REVIEWER_IMAGE.to_string()),
            env: Some(env_vars),
            host_config: Some(bollard::models::HostConfig {
                binds: Some(vec![format!("{}:/review/agent:ro", host_agent_dir)]),
                memory: Some(256 * 1024 * 1024),          // 256MB
                nano_cpus: Some(1_000_000_000),           // 1 CPU
                network_mode: Some("bridge".to_string()), // Needs network for API call
                // SECURITY: Hardening like other containers in the codebase
                privileged: Some(false),
                cap_drop: Some(vec!["ALL".to_string()]),
                cap_add: Some(vec![
                    "CHOWN".to_string(),
                    "SETUID".to_string(),
                    "SETGID".to_string(),
                ]),
                security_opt: Some(vec!["no-new-privileges:true".to_string()]),
                pids_limit: Some(256),
                ..Default::default()
            }),
            ..Default::default()
        };

        let create_opts = CreateContainerOptions {
            name: container_name.as_str(),
            platform: None,
        };

        // Create and start container
        docker
            .create_container(Some(create_opts), container_config)
            .await
            .context("Failed to create review container")?;

        docker
            .start_container(
                &container_name,
                None::<bollard::container::StartContainerOptions<String>>,
            )
            .await
            .context("Failed to start review container")?;

        // Wait for container to finish with timeout
        let wait_result = tokio::time::timeout(
            Duration::from_secs(REVIEW_TIMEOUT_SECS),
            Self::wait_for_container(&docker, &container_name),
        )
        .await;

        // Collect stdout logs regardless of exit status
        let stdout = Self::collect_logs(&docker, &container_name).await;

        // Cleanup container
        let _ = docker
            .remove_container(
                &container_name,
                Some(RemoveContainerOptions {
                    force: true,
                    ..Default::default()
                }),
            )
            .await;

        // Handle timeout
        let exit_code = match wait_result {
            Ok(Ok(code)) => code,
            Ok(Err(e)) => {
                return Err(anyhow::anyhow!("Container wait error: {}", e));
            }
            Err(_) => {
                return Err(anyhow::anyhow!(
                    "Review container timed out after {}s",
                    REVIEW_TIMEOUT_SECS
                ));
            }
        };

        debug!(
            "Review container exited with code {}, stdout len={}",
            exit_code,
            stdout.len()
        );

        // Parse JSON from stdout
        if stdout.trim().is_empty() {
            return Err(anyhow::anyhow!(
                "Review container produced no output (exit code {})",
                exit_code
            ));
        }

        let result: serde_json::Value = serde_json::from_str(stdout.trim()).context(format!(
            "Failed to parse review output as JSON: {}",
            &stdout[..200.min(stdout.len())]
        ))?;

        Ok(result)
    }

    async fn wait_for_container(docker: &Docker, container_name: &str) -> Result<i64> {
        let options = WaitContainerOptions {
            condition: "not-running",
        };

        let mut stream = docker.wait_container(container_name, Some(options));

        if let Some(result) = stream.next().await {
            match result {
                Ok(response) => Ok(response.status_code),
                Err(e) => Err(anyhow::anyhow!("Wait error: {}", e)),
            }
        } else {
            Err(anyhow::anyhow!("Container wait stream ended unexpectedly"))
        }
    }

    async fn collect_logs(docker: &Docker, container_name: &str) -> String {
        let options = LogsOptions::<String> {
            stdout: true,
            stderr: false,
            follow: false,
            ..Default::default()
        };

        let mut output = String::new();
        let mut stream = docker.logs(container_name, Some(options));

        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(log) => output.push_str(&log.to_string()),
                Err(e) => {
                    warn!("Error reading container logs: {}", e);
                    break;
                }
            }
        }

        output
    }
}
