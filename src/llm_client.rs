//! LLM Client for Agent Execution
//!
//! SECURITY NOTE: This module NO LONGER executes agent code on the host.
//! All agent execution happens inside Docker containers via the evaluator.
//! This module only provides LLM API client functionality.

use anyhow::{Context, Result};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tracing::{debug, info};

use crate::terminal_harness::{AgentRequest, AgentResponse};

/// LLM configuration
#[derive(Debug, Clone)]
pub struct LlmConfig {
    pub api_base: String,
    pub api_key: String,
    pub model: String,
    pub max_tokens: u32,
    pub temperature: f32,
    pub timeout_secs: u64,
}

impl Default for LlmConfig {
    fn default() -> Self {
        Self {
            api_base: std::env::var("LLM_API_BASE")
                .unwrap_or_else(|_| "https://openrouter.ai/api/v1".to_string()),
            api_key: std::env::var("OPENROUTER_API_KEY")
                .or_else(|_| std::env::var("LLM_API_KEY"))
                .or_else(|_| std::env::var("OPENAI_API_KEY"))
                .unwrap_or_default(),
            model: std::env::var("LLM_MODEL")
                .unwrap_or_else(|_| "anthropic/claude-3-haiku".to_string()),
            max_tokens: 2048,
            temperature: 0.3,
            timeout_secs: 120,
        }
    }
}

#[derive(Debug, Serialize)]
struct ChatRequest {
    model: String,
    messages: Vec<Message>,
    max_tokens: u32,
    temperature: f32,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Message {
    pub role: String,
    pub content: String,
}

impl Message {
    pub fn system(content: &str) -> Self {
        Self {
            role: "system".to_string(),
            content: content.to_string(),
        }
    }

    pub fn user(content: &str) -> Self {
        Self {
            role: "user".to_string(),
            content: content.to_string(),
        }
    }

    pub fn assistant(content: &str) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.to_string(),
        }
    }
}

#[derive(Debug, Deserialize)]
struct ChatResponse {
    choices: Vec<Choice>,
}

#[derive(Debug, Deserialize)]
struct Choice {
    message: Message,
}

/// LLM client for API calls
pub struct LlmClient {
    client: Client,
    config: LlmConfig,
}

impl LlmClient {
    pub fn new(config: LlmConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_secs))
            .build()?;

        info!(
            "LLM client initialized: model={}, api_base={}",
            config.model, config.api_base
        );
        Ok(Self { client, config })
    }

    pub fn from_env() -> Result<Self> {
        Self::new(LlmConfig::default())
    }

    fn system_prompt(&self) -> String {
        r#"You are a terminal agent. Execute shell commands to complete tasks.

RESPONSE FORMAT (JSON only):
{"command": "your shell command here", "task_complete": false}

When done:
{"command": null, "task_complete": true}

RULES:
- One command at a time
- You receive the output of each command
- Set task_complete=true only when finished
- Respond with valid JSON only, no other text"#
            .to_string()
    }

    fn build_user_message(&self, req: &AgentRequest) -> String {
        let mut msg = format!(
            "TASK: {}\n\nSTEP: {}\nCWD: {}",
            req.instruction, req.step, req.cwd
        );

        if let Some(cmd) = &req.last_command {
            msg.push_str(&format!("\n\nLAST COMMAND: {}", cmd));
        }
        if let Some(code) = req.exit_code {
            msg.push_str(&format!("\nEXIT CODE: {}", code));
        }
        if let Some(out) = &req.output {
            let truncated = if out.len() > 16000 {
                format!("{}...[truncated]", &out[..16000])
            } else {
                out.clone()
            };
            msg.push_str(&format!("\n\nOUTPUT:\n{}", truncated));
        }

        msg
    }

    /// Execute a single LLM call and get agent response
    pub async fn execute(&self, request: AgentRequest) -> Result<AgentResponse> {
        let messages = vec![
            Message::system(&self.system_prompt()),
            Message::user(&self.build_user_message(&request)),
        ];

        debug!("Calling LLM: step={}", request.step);

        let resp = self
            .client
            .post(format!("{}/chat/completions", self.config.api_base))
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .header("HTTP-Referer", "https://platform.network")
            .json(&ChatRequest {
                model: self.config.model.clone(),
                messages,
                max_tokens: self.config.max_tokens,
                temperature: self.config.temperature,
            })
            .send()
            .await
            .context("LLM request failed")?;

        if !resp.status().is_success() {
            let status = resp.status();
            let err = resp.text().await.unwrap_or_default();
            anyhow::bail!("LLM error ({}): {}", status, err);
        }

        let chat: ChatResponse = resp.json().await?;
        let content = chat
            .choices
            .first()
            .map(|c| c.message.content.clone())
            .unwrap_or_default();

        debug!("LLM response: {}", content);
        crate::terminal_harness::parse_agent_response(&content)
    }

    /// Chat with conversation history
    pub async fn chat(&self, messages: Vec<Message>) -> Result<String> {
        let resp = self
            .client
            .post(format!("{}/chat/completions", self.config.api_base))
            .header("Authorization", format!("Bearer {}", self.config.api_key))
            .header("Content-Type", "application/json")
            .header("HTTP-Referer", "https://platform.network")
            .json(&ChatRequest {
                model: self.config.model.clone(),
                messages,
                max_tokens: self.config.max_tokens,
                temperature: self.config.temperature,
            })
            .send()
            .await
            .context("LLM chat request failed")?;

        if !resp.status().is_success() {
            let status = resp.status();
            let err = resp.text().await.unwrap_or_default();
            anyhow::bail!("LLM chat error ({}): {}", status, err);
        }

        let chat: ChatResponse = resp.json().await?;
        Ok(chat
            .choices
            .first()
            .map(|c| c.message.content.clone())
            .unwrap_or_default())
    }
}

// ============================================================================
// REMOVED: SourceCodeAgent
// ============================================================================
// The SourceCodeAgent struct that executed Python on the host has been REMOVED
// for security reasons. All agent code now executes inside Docker containers
// via the evaluator module.
//
// If you need to run agent code, use:
// - TaskEvaluator::evaluate_task() for full task evaluation
// - ContainerRun::inject_agent_code() + start_agent() for direct container execution
// ============================================================================
