//! LLM-based Agent Code Review System
//!
//! Provides validation rules, LLM provider configuration, and types
//! used by the LLM review worker (worker/llm_review.rs).
//!
//! The actual review is performed by an agent running in an isolated
//! Docker container (docker/llm-reviewer/) that calls the Chutes API.

use reqwest::Client;
use serde::{Deserialize, Serialize};
use sha2::{Digest, Sha256};
use std::time::Duration;
use thiserror::Error;
use tracing::{debug, info, warn};

/// LLM Provider configuration
#[derive(Debug, Clone, Serialize, Deserialize, Default, PartialEq, Eq)]
pub enum LlmProvider {
    #[default]
    OpenRouter,
    Chutes,
    OpenAI,
    Anthropic,
    Grok,
}

impl LlmProvider {
    /// Get the API endpoint for this provider
    pub fn endpoint(&self) -> &str {
        match self {
            LlmProvider::OpenRouter => "https://openrouter.ai/api/v1/chat/completions",
            LlmProvider::Chutes => "https://llm.chutes.ai/v1/chat/completions",
            LlmProvider::OpenAI => "https://api.openai.com/v1/chat/completions",
            LlmProvider::Anthropic => "https://api.anthropic.com/v1/messages",
            LlmProvider::Grok => "https://api.x.ai/v1/chat/completions",
        }
    }

    /// Get the default model for this provider
    pub fn default_model(&self) -> &str {
        match self {
            LlmProvider::OpenRouter => "anthropic/claude-3.5-sonnet",
            LlmProvider::Chutes => "moonshotai/Kimi-K2.5-TEE",
            LlmProvider::OpenAI => "gpt-4o-mini",
            LlmProvider::Anthropic => "claude-3-5-sonnet-20241022",
            LlmProvider::Grok => "grok-2-latest",
        }
    }

    /// Parse provider from string
    pub fn parse(s: &str) -> Self {
        match s.to_lowercase().as_str() {
            "chutes" | "ch" => LlmProvider::Chutes,
            "openai" | "oa" => LlmProvider::OpenAI,
            "anthropic" | "claude" => LlmProvider::Anthropic,
            "grok" | "xai" => LlmProvider::Grok,
            _ => LlmProvider::OpenRouter,
        }
    }

    /// Check if this provider uses Anthropic's API format
    pub fn is_anthropic(&self) -> bool {
        matches!(self, LlmProvider::Anthropic)
    }
}

/// LLM configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LlmConfig {
    pub provider: LlmProvider,
    pub api_key: String,
    pub model_id: String,
    pub timeout_secs: u64,
    pub max_tokens: u32,
}

impl Default for LlmConfig {
    fn default() -> Self {
        Self {
            provider: LlmProvider::OpenRouter,
            api_key: String::new(),
            model_id: LlmProvider::OpenRouter.default_model().to_string(),
            timeout_secs: 60,
            max_tokens: 1024,
        }
    }
}

impl LlmConfig {
    /// Create config for a specific provider with default model
    pub fn for_provider(provider: LlmProvider, api_key: String) -> Self {
        let model_id = provider.default_model().to_string();
        Self {
            provider,
            api_key,
            model_id,
            timeout_secs: 60,
            max_tokens: 1024,
        }
    }

    pub fn openrouter(api_key: String) -> Self {
        Self::for_provider(LlmProvider::OpenRouter, api_key)
    }

    pub fn chutes(api_key: String) -> Self {
        Self::for_provider(LlmProvider::Chutes, api_key)
    }

    pub fn openai(api_key: String) -> Self {
        Self::for_provider(LlmProvider::OpenAI, api_key)
    }

    pub fn anthropic(api_key: String) -> Self {
        Self::for_provider(LlmProvider::Anthropic, api_key)
    }

    pub fn grok(api_key: String) -> Self {
        Self::for_provider(LlmProvider::Grok, api_key)
    }

    pub fn endpoint(&self) -> &str {
        self.provider.endpoint()
    }

    /// Create LlmConfig from environment variables (validator's own key)
    pub fn from_env() -> Option<Self> {
        let provider_str =
            std::env::var("LLM_PROVIDER").unwrap_or_else(|_| "openrouter".to_string());

        let provider = LlmProvider::parse(&provider_str);

        let api_key = match provider {
            LlmProvider::Chutes => std::env::var("CHUTES_API_KEY").ok()?,
            LlmProvider::OpenAI => std::env::var("OPENAI_API_KEY").ok()?,
            LlmProvider::Anthropic => std::env::var("ANTHROPIC_API_KEY").ok()?,
            LlmProvider::Grok => std::env::var("GROK_API_KEY").ok()?,
            LlmProvider::OpenRouter => std::env::var("OPENROUTER_API_KEY").ok()?,
        };

        let model_id =
            std::env::var("LLM_MODEL").unwrap_or_else(|_| provider.default_model().to_string());

        info!(
            "LLM Review configured: provider={:?}, model={}",
            provider, model_id
        );

        Some(Self {
            provider,
            api_key,
            model_id,
            timeout_secs: 60,
            max_tokens: 2048,
        })
    }
}

/// Challenge validation rules (synced from blockchain)
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct ValidationRules {
    /// List of rules for the challenge
    pub rules: Vec<String>,
    /// Version/epoch when rules were updated
    pub version: u64,
    /// Hash of the rules for verification
    pub rules_hash: String,
    /// Last update timestamp
    pub updated_at: u64,
}

/// API response for a single rule from GET /api/v1/rules
#[derive(Debug, Clone, Deserialize)]
struct ApiRule {
    #[allow(dead_code)]
    id: i64,
    rule_text: String,
    #[allow(dead_code)]
    #[serde(default)]
    category: Option<String>,
    #[allow(dead_code)]
    #[serde(default)]
    priority: Option<i32>,
}

/// API response from GET /api/v1/rules
#[derive(Debug, Clone, Deserialize)]
struct ApiRulesResponse {
    rules: Vec<ApiRule>,
    version: u64,
}

/// Timeout for fetching rules from API (in seconds)
const RULES_API_TIMEOUT_SECS: u64 = 5;

/// Number of retry attempts for transient failures
const RULES_API_MAX_RETRIES: u32 = 3;

/// Delay between retries in milliseconds
const RULES_API_RETRY_DELAY_MS: u64 = 500;

/// Fetch LLM validation rules from the rules API endpoint with retry logic.
///
/// Attempts to fetch rules from `{base_url}/api/v1/rules`.
/// Retries up to 3 times on transient network failures.
/// Falls back to `default_term_challenge_rules()` if all attempts fail.
///
/// # Arguments
/// * `base_url` - The base URL of the API server (e.g., "https://api.example.com")
///
/// # Returns
/// `ValidationRules` from the API, or default rules on failure
pub async fn fetch_rules_from_api(base_url: &str) -> ValidationRules {
    let url = format!("{}/api/v1/rules", base_url.trim_end_matches('/'));

    info!("Fetching LLM rules from API: {}", url);

    let client = match Client::builder()
        .timeout(Duration::from_secs(RULES_API_TIMEOUT_SECS))
        .build()
    {
        Ok(c) => c,
        Err(_) => {
            warn!("Failed to create HTTP client for rules API. Using default rules.");
            return ValidationRules::default_term_challenge_rules();
        }
    };

    // Retry loop for transient failures
    let mut last_error = String::new();
    for attempt in 1..=RULES_API_MAX_RETRIES {
        match fetch_rules_single_attempt(&client, &url).await {
            Ok(rules) => return rules,
            Err(e) => {
                last_error = e;
                if attempt < RULES_API_MAX_RETRIES {
                    debug!(
                        "Rules API attempt {}/{} failed, retrying in {}ms...",
                        attempt, RULES_API_MAX_RETRIES, RULES_API_RETRY_DELAY_MS
                    );
                    tokio::time::sleep(Duration::from_millis(RULES_API_RETRY_DELAY_MS)).await;
                }
            }
        }
    }

    warn!(
        "Failed to fetch rules from API after {} attempts. Using default rules.",
        RULES_API_MAX_RETRIES
    );
    debug!("Last error: {}", last_error);
    ValidationRules::default_term_challenge_rules()
}

/// Single attempt to fetch rules from the API
async fn fetch_rules_single_attempt(client: &Client, url: &str) -> Result<ValidationRules, String> {
    let response = client
        .get(url)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("Non-success status: {}", response.status()));
    }

    let api_response: ApiRulesResponse = response
        .json()
        .await
        .map_err(|e| format!("Failed to parse response: {}", e))?;

    if api_response.rules.is_empty() {
        return Err("Empty rules list".to_string());
    }

    // Convert API rules to ValidationRules
    let rules: Vec<String> = api_response
        .rules
        .into_iter()
        .map(|r| r.rule_text)
        .collect();

    let rules_hash = ValidationRules::compute_hash(&rules);

    info!(
        "Successfully fetched {} rules from API (version {})",
        rules.len(),
        api_response.version
    );

    Ok(ValidationRules {
        rules,
        version: api_response.version,
        rules_hash,
        updated_at: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs(),
    })
}

impl ValidationRules {
    pub fn new(rules: Vec<String>) -> Self {
        let rules_hash = Self::compute_hash(&rules);
        Self {
            rules,
            version: 1,
            rules_hash,
            updated_at: std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
        }
    }

    pub fn compute_hash(rules: &[String]) -> String {
        let mut hasher = Sha256::new();
        for rule in rules {
            hasher.update(rule.as_bytes());
            hasher.update(b"\n");
        }
        hex::encode(hasher.finalize())
    }

    pub fn formatted_rules(&self) -> String {
        self.rules
            .iter()
            .enumerate()
            .map(|(i, rule)| format!("{}. {}", i + 1, rule))
            .collect::<Vec<_>>()
            .join("\n")
    }

    /// Returns the default Term Challenge validation rules.
    ///
    /// This is a fallback when the rules API is unavailable.
    /// In production, rules should be fetched dynamically via `fetch_rules_from_api()`.
    pub fn default_term_challenge_rules() -> Self {
        Self::new(vec![
            "The agent must use only term_sdk (Agent, Request, Response, run) for terminal interaction. Response.cmd() is the CORRECT way to execute shell commands.".to_string(),
            "The agent must not attempt to access the network or make HTTP requests directly (urllib, requests, socket).".to_string(),
            "The agent must not use subprocess, os.system(), os.popen(), or exec() to run commands. Use Response.cmd() instead.".to_string(),
            "The agent must not attempt to import forbidden modules (socket, requests, urllib, subprocess, os, sys for system calls).".to_string(),
            "The agent must implement a valid solve(self, req: Request) method that returns Response objects.".to_string(),
            "The agent must inherit from Agent class and use run(MyAgent()) in main.".to_string(),
            "The agent must not contain obfuscated or encoded malicious code.".to_string(),
            "The agent must not attempt to escape the sandbox environment.".to_string(),
            "The agent must not contain infinite loops without termination conditions.".to_string(),
            "Response.cmd('shell command') is ALLOWED and is the proper way to execute terminal commands.".to_string(),
        ])
    }
}

/// LLM Review result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReviewResult {
    pub approved: bool,
    pub reason: String,
    pub violations: Vec<String>,
    pub reviewer_id: String,
    pub reviewed_at: u64,
    pub rules_version: u64,
}

#[derive(Debug, Error)]
pub enum ReviewError {
    #[error("LLM API error: {0}")]
    ApiError(String),
    #[error("Invalid response: {0}")]
    InvalidResponse(String),
    #[error("Timeout")]
    Timeout,
    #[error("Rate limited")]
    RateLimited,
    #[error("Configuration error: {0}")]
    ConfigError(String),
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_validation_rules() {
        let rules = ValidationRules::default_term_challenge_rules();
        assert!(!rules.rules.is_empty());
        assert!(!rules.rules_hash.is_empty());

        let formatted = rules.formatted_rules();
        assert!(formatted.contains("1."));
        assert!(formatted.contains("term_sdk"));
    }

    #[test]
    fn test_validation_rules_new() {
        let rules = ValidationRules::new(vec!["Rule 1".to_string(), "Rule 2".to_string()]);
        assert_eq!(rules.rules.len(), 2);
        assert!(!rules.rules_hash.is_empty());
    }

    #[test]
    fn test_validation_rules_hash_changes() {
        let rules1 = ValidationRules::new(vec!["Rule A".to_string()]);
        let rules2 = ValidationRules::new(vec!["Rule B".to_string()]);
        assert_ne!(rules1.rules_hash, rules2.rules_hash);
    }

    #[test]
    fn test_llm_provider_parse() {
        assert_eq!(LlmProvider::parse("chutes"), LlmProvider::Chutes);
        assert_eq!(LlmProvider::parse("openai"), LlmProvider::OpenAI);
        assert_eq!(LlmProvider::parse("anthropic"), LlmProvider::Anthropic);
        assert_eq!(LlmProvider::parse("grok"), LlmProvider::Grok);
        assert_eq!(LlmProvider::parse("unknown"), LlmProvider::OpenRouter);
    }

    #[test]
    fn test_llm_config_default() {
        let config = LlmConfig::default();
        assert!(config.max_tokens > 0);
        assert!(config.timeout_secs > 0);
    }

    #[test]
    fn test_llm_config_for_provider() {
        let config = LlmConfig::for_provider(LlmProvider::Chutes, "test_key".to_string());
        assert_eq!(config.provider, LlmProvider::Chutes);
        assert_eq!(config.api_key, "test_key");
        assert_eq!(config.model_id, "moonshotai/Kimi-K2.5-TEE");
    }
}
