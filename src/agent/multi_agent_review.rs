//! Multi-Agent LLM Code Review System
//!
//! Implements a Discord-like multi-agent conversation system where multiple LLM agents
//! with different personas debate and reach consensus on code quality.
//!
//! ## Architecture
//!
//! ```text
//! ┌─────────────────────────────────────────────────────────────────────┐
//! │                    Multi-Agent Review Session                       │
//! ├─────────────────────────────────────────────────────────────────────┤
//! │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐              │
//! │  │   Security   │  │  Readability │  │  Compliance  │              │
//! │  │    Auditor   │  │    Expert    │  │   Checker    │              │
//! │  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘              │
//! │         │                 │                 │                       │
//! │         └────────────┬────┴────────────────┘                       │
//! │                      ▼                                              │
//! │              ┌──────────────┐                                       │
//! │              │ Conversation │ ◄─── Discord-like debate             │
//! │              │   History    │                                       │
//! │              └──────┬───────┘                                       │
//! │                     ▼                                               │
//! │              ┌──────────────┐                                       │
//! │              │  Moderator   │ ◄─── Synthesizes final verdict       │
//! │              └──────┬───────┘                                       │
//! │                     ▼                                               │
//! │              ┌──────────────┐                                       │
//! │              │  Consensus   │                                       │
//! │              │   Result     │                                       │
//! │              └──────────────┘                                       │
//! └─────────────────────────────────────────────────────────────────────┘
//! ```
//!
//! ## Agent Personas
//!
//! 1. **Security Auditor**: Focuses on security vulnerabilities, sandbox escapes, malicious code
//! 2. **Readability Expert**: Evaluates code clarity, naming, structure, documentation
//! 3. **Compliance Checker**: Verifies adherence to term_sdk rules and best practices
//! 4. **Moderator**: Synthesizes discussion and determines final consensus

use parking_lot::RwLock;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use thiserror::Error;
use tracing::{debug, error, info, warn};

use super::review::{LlmConfig, LlmProvider, ReviewError, ValidationRules};

/// Maximum number of debate rounds before forcing consensus
const MAX_DEBATE_ROUNDS: usize = 3;

/// Minimum agreement threshold for consensus (0.0 to 1.0)
const CONSENSUS_THRESHOLD: f64 = 0.66;

/// Agent persona definitions
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum AgentPersona {
    /// Security expert focused on vulnerabilities and malicious code
    SecurityAuditor,
    /// Code quality expert focused on readability and maintainability
    ReadabilityExpert,
    /// Rules compliance checker for term_sdk and challenge rules
    ComplianceChecker,
    /// Moderator that synthesizes debate and determines consensus
    Moderator,
}

impl AgentPersona {
    /// Get the display name for this persona
    pub fn name(&self) -> &'static str {
        match self {
            AgentPersona::SecurityAuditor => "Security Auditor",
            AgentPersona::ReadabilityExpert => "Readability Expert",
            AgentPersona::ComplianceChecker => "Compliance Checker",
            AgentPersona::Moderator => "Moderator",
        }
    }

    /// Get the emoji icon for Discord-like display
    pub fn icon(&self) -> &'static str {
        match self {
            AgentPersona::SecurityAuditor => "[SECURITY]",
            AgentPersona::ReadabilityExpert => "[READABILITY]",
            AgentPersona::ComplianceChecker => "[COMPLIANCE]",
            AgentPersona::Moderator => "[MODERATOR]",
        }
    }

    /// Get the system prompt for this persona
    pub fn system_prompt(&self) -> String {
        match self {
            AgentPersona::SecurityAuditor => r#"You are a senior security auditor reviewing Python agent code for a terminal coding challenge.

Your role is to use your security expertise to analyze code and identify potential security concerns. You should reason about the code's behavior, intent, and potential risks.

You are participating in a code review discussion with other experts. Be direct and specific about any security concerns you find through your analysis. If you disagree with another reviewer, explain your reasoning clearly.

When responding:
1. First state your overall security assessment (APPROVE or REJECT)
2. Explain your reasoning based on your analysis of the code
3. Respond to points raised by other reviewers if relevant
4. Be concise but thorough"#.to_string(),

            AgentPersona::ReadabilityExpert => r#"You are a code readability and maintainability expert reviewing Python agent code for a terminal coding challenge.

Your expertise areas:
- Evaluating code clarity and structure
- Assessing naming conventions (variables, functions, classes)
- Checking for proper documentation and comments
- Identifying overly complex or convoluted logic
- Detecting code smells and anti-patterns
- Verifying code is not obfuscated or intentionally hard to understand

You are participating in a code review discussion with other experts. Focus on whether the code is readable, maintainable, and follows Python best practices.

When responding:
1. First state your overall readability assessment (APPROVE or REJECT)
2. List specific readability issues or positive aspects
3. Respond to points raised by other reviewers if relevant
4. Be constructive and specific"#.to_string(),

            AgentPersona::ComplianceChecker => r#"You are a compliance specialist reviewing Python agent code for the Term Challenge.

Your role is to evaluate whether the code follows the Term Challenge guidelines and best practices. Use your understanding of the platform's requirements to assess compliance.

The Term Challenge expects agents to:
- Use the official SDK appropriately
- Follow the platform's execution model
- Be well-structured and maintainable

You are participating in a code review discussion. Assess the code based on your understanding of compliance requirements.

When responding:
1. First state your overall compliance assessment (APPROVE or REJECT)
2. Explain what aspects of the code inform your assessment
3. Respond to points raised by other reviewers if relevant
4. Be constructive and specific"#.to_string(),

            AgentPersona::Moderator => r#"You are the moderator of a multi-agent code review discussion.

Your role is to:
1. Synthesize the opinions of all reviewers (Security Auditor, Readability Expert, Compliance Checker)
2. Identify areas of agreement and disagreement
3. Make a final determination on whether the code should be APPROVED or REJECTED
4. Provide a clear summary of the consensus

Base your decision on the reviewers' reasoning and analysis, not on predetermined rules.

When making your final decision:
1. Summarize each reviewer's position
2. Note any unresolved disagreements
3. State your FINAL VERDICT: APPROVED or REJECTED
4. Provide a brief rationale for the decision"#.to_string(),
        }
    }

    /// Get all review personas (excluding moderator)
    pub fn reviewers() -> Vec<AgentPersona> {
        vec![
            AgentPersona::SecurityAuditor,
            AgentPersona::ReadabilityExpert,
            AgentPersona::ComplianceChecker,
        ]
    }

    /// Get all personas including moderator
    pub fn all() -> Vec<AgentPersona> {
        vec![
            AgentPersona::SecurityAuditor,
            AgentPersona::ReadabilityExpert,
            AgentPersona::ComplianceChecker,
            AgentPersona::Moderator,
        ]
    }
}

/// A single message in the multi-agent conversation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConversationMessage {
    /// The persona who sent this message
    pub persona: AgentPersona,
    /// The message content
    pub content: String,
    /// Timestamp when the message was sent
    pub timestamp: u64,
    /// Round number in the debate
    pub round: usize,
    /// Individual verdict from this message (if applicable)
    pub verdict: Option<AgentVerdict>,
}

impl ConversationMessage {
    pub fn new(persona: AgentPersona, content: String, round: usize) -> Self {
        let verdict = Self::extract_verdict(&content);
        Self {
            persona,
            content,
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            round,
            verdict,
        }
    }

    /// Extract verdict from message content
    fn extract_verdict(content: &str) -> Option<AgentVerdict> {
        let content_upper = content.to_uppercase();

        // Look for explicit verdict statements
        if content_upper.contains("REJECT")
            || content_upper.contains("NOT APPROVED")
            || content_upper.contains("CANNOT APPROVE")
        {
            Some(AgentVerdict::Reject)
        } else if content_upper.contains("APPROVE") && !content_upper.contains("NOT APPROVE") {
            Some(AgentVerdict::Approve)
        } else {
            None
        }
    }

    /// Format message for display (Discord-like)
    pub fn format_display(&self) -> String {
        format!(
            "{} **{}**: {}",
            self.persona.icon(),
            self.persona.name(),
            self.content
        )
    }
}

/// Individual agent verdict
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum AgentVerdict {
    Approve,
    Reject,
}

/// Summary of an agent's position
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AgentPosition {
    pub persona: AgentPersona,
    pub verdict: AgentVerdict,
    pub key_points: Vec<String>,
    pub concerns: Vec<String>,
}

/// Final consensus result from multi-agent review
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConsensusResult {
    /// Final verdict
    pub approved: bool,
    /// Consensus confidence (0.0 to 1.0)
    pub confidence: f64,
    /// Whether full consensus was reached
    pub unanimous: bool,
    /// Summary of the decision
    pub summary: String,
    /// Individual agent positions
    pub positions: Vec<AgentPosition>,
    /// Full conversation transcript
    pub conversation: Vec<ConversationMessage>,
    /// Number of debate rounds
    pub rounds: usize,
    /// Issues found during review
    pub issues: Vec<ReviewIssue>,
    /// Timestamp of the review
    pub reviewed_at: u64,
}

/// A specific issue found during review
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReviewIssue {
    pub category: IssueCategory,
    pub severity: IssueSeverity,
    pub description: String,
    pub found_by: AgentPersona,
    pub code_snippet: Option<String>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum IssueCategory {
    Security,
    Readability,
    Compliance,
    Obfuscation,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum IssueSeverity {
    Critical,
    Major,
    Minor,
    Info,
}

/// Multi-agent review session
pub struct MultiAgentReviewSession {
    /// Agent hash being reviewed
    pub agent_hash: String,
    /// Code being reviewed
    pub code: String,
    /// Conversation history
    pub conversation: Vec<ConversationMessage>,
    /// Current round
    pub round: usize,
    /// Validation rules
    pub rules: ValidationRules,
    /// Issues found
    pub issues: Vec<ReviewIssue>,
}

impl MultiAgentReviewSession {
    pub fn new(agent_hash: String, code: String, rules: ValidationRules) -> Self {
        Self {
            agent_hash,
            code,
            conversation: Vec::new(),
            round: 0,
            rules,
            issues: Vec::new(),
        }
    }

    /// Add a message to the conversation
    pub fn add_message(&mut self, persona: AgentPersona, content: String) {
        let message = ConversationMessage::new(persona, content, self.round);
        self.conversation.push(message);
    }

    /// Get the conversation history formatted for LLM context
    pub fn format_conversation_history(&self) -> String {
        if self.conversation.is_empty() {
            return String::new();
        }

        let mut history = String::from("\n--- PREVIOUS DISCUSSION ---\n");
        for msg in &self.conversation {
            history.push_str(&format!(
                "\n{} {}:\n{}\n",
                msg.persona.icon(),
                msg.persona.name(),
                msg.content
            ));
        }
        history.push_str("\n--- END DISCUSSION ---\n");
        history
    }

    /// Calculate current consensus state
    pub fn calculate_consensus(&self) -> (f64, bool) {
        let verdicts: Vec<AgentVerdict> = self
            .conversation
            .iter()
            .filter(|m| m.persona != AgentPersona::Moderator)
            .filter_map(|m| m.verdict)
            .collect();

        if verdicts.is_empty() {
            return (0.0, false);
        }

        let approvals = verdicts
            .iter()
            .filter(|v| **v == AgentVerdict::Approve)
            .count();
        let approval_rate = approvals as f64 / verdicts.len() as f64;
        let unanimous = approvals == verdicts.len() || approvals == 0;

        (approval_rate, unanimous)
    }

    /// Extract positions from the conversation
    pub fn extract_positions(&self) -> Vec<AgentPosition> {
        let mut positions = Vec::new();

        for persona in AgentPersona::reviewers() {
            let messages: Vec<&ConversationMessage> = self
                .conversation
                .iter()
                .filter(|m| m.persona == persona)
                .collect();

            if let Some(last_msg) = messages.last() {
                let verdict = last_msg.verdict.unwrap_or(AgentVerdict::Reject);
                positions.push(AgentPosition {
                    persona,
                    verdict,
                    key_points: Vec::new(), // Could be extracted with more parsing
                    concerns: Vec::new(),
                });
            }
        }

        positions
    }
}

/// Error types for multi-agent review
#[derive(Debug, Error)]
pub enum MultiAgentError {
    #[error("LLM API error: {0}")]
    LlmError(String),
    #[error("Consensus failed after max rounds")]
    ConsensusFailed,
    #[error("Configuration error: {0}")]
    ConfigError(String),
    #[error("Session error: {0}")]
    SessionError(String),
}

impl From<ReviewError> for MultiAgentError {
    fn from(err: ReviewError) -> Self {
        MultiAgentError::LlmError(err.to_string())
    }
}

/// Multi-Agent Review Manager
///
/// Orchestrates the multi-agent code review process
pub struct MultiAgentReviewManager {
    config: Arc<RwLock<LlmConfig>>,
    rules: Arc<RwLock<ValidationRules>>,
    client: Client,
    /// Active review sessions
    sessions: Arc<RwLock<HashMap<String, MultiAgentReviewSession>>>,
}

impl MultiAgentReviewManager {
    pub fn new(config: LlmConfig) -> Self {
        Self {
            config: Arc::new(RwLock::new(config)),
            rules: Arc::new(RwLock::new(ValidationRules::default_term_challenge_rules())),
            client: Client::new(),
            sessions: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// Update validation rules
    pub fn update_rules(&self, rules: ValidationRules) {
        *self.rules.write() = rules;
    }

    /// Get current rules
    pub fn get_rules(&self) -> ValidationRules {
        self.rules.read().clone()
    }

    /// Run a complete multi-agent review session
    pub async fn review_code(
        &self,
        agent_hash: &str,
        code: &str,
    ) -> Result<ConsensusResult, MultiAgentError> {
        let config = self.config.read().clone();
        self.review_code_with_config(agent_hash, code, &config)
            .await
    }

    /// Run multi-agent review with custom config (e.g., miner's API key)
    pub async fn review_code_with_config(
        &self,
        agent_hash: &str,
        code: &str,
        config: &LlmConfig,
    ) -> Result<ConsensusResult, MultiAgentError> {
        info!(
            "Starting multi-agent review for agent {}",
            &agent_hash[..16.min(agent_hash.len())]
        );

        let rules = self.rules.read().clone();
        let mut session =
            MultiAgentReviewSession::new(agent_hash.to_string(), code.to_string(), rules);

        // Round 1: Initial reviews from all agents
        session.round = 1;
        info!("Round 1: Initial reviews");

        for persona in AgentPersona::reviewers() {
            let response = self
                .get_agent_response(&session, persona, config, true)
                .await?;
            session.add_message(persona, response);
            debug!("{} completed initial review", persona.name());
        }

        // Check if we have immediate consensus
        let (approval_rate, unanimous) = session.calculate_consensus();
        if unanimous {
            info!(
                "Unanimous consensus reached in round 1: {}",
                if approval_rate > 0.5 {
                    "APPROVED"
                } else {
                    "REJECTED"
                }
            );
        } else {
            // Additional debate rounds if needed
            for round in 2..=MAX_DEBATE_ROUNDS {
                session.round = round;
                info!("Round {}: Debate continuation", round);

                for persona in AgentPersona::reviewers() {
                    let response = self
                        .get_agent_response(&session, persona, config, false)
                        .await?;
                    session.add_message(persona, response);
                }

                let (new_approval_rate, new_unanimous) = session.calculate_consensus();
                if new_unanimous || (new_approval_rate - 0.5).abs() > 0.4 {
                    info!(
                        "Strong consensus reached in round {}: {:.0}%",
                        round,
                        new_approval_rate * 100.0
                    );
                    break;
                }
            }
        }

        // Final round: Moderator synthesizes and makes final decision
        session.round += 1;
        info!("Final round: Moderator synthesis");
        let moderator_response = self
            .get_agent_response(&session, AgentPersona::Moderator, config, false)
            .await?;
        session.add_message(AgentPersona::Moderator, moderator_response.clone());

        // Build final consensus result
        let (final_approval_rate, unanimous) = session.calculate_consensus();
        let approved = self.determine_final_verdict(&session, &moderator_response);
        let positions = session.extract_positions();

        let result = ConsensusResult {
            approved,
            confidence: if unanimous {
                1.0
            } else {
                final_approval_rate.max(1.0 - final_approval_rate)
            },
            unanimous,
            summary: self.extract_summary(&moderator_response),
            positions,
            conversation: session.conversation,
            rounds: session.round,
            issues: session.issues,
            reviewed_at: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
        };

        info!(
            "Multi-agent review complete: approved={}, confidence={:.2}, rounds={}",
            result.approved, result.confidence, result.rounds
        );

        Ok(result)
    }

    /// Get response from a specific agent persona
    async fn get_agent_response(
        &self,
        session: &MultiAgentReviewSession,
        persona: AgentPersona,
        config: &LlmConfig,
        is_initial: bool,
    ) -> Result<String, MultiAgentError> {
        let system_prompt = persona.system_prompt();
        let user_prompt = self.build_user_prompt(session, persona, is_initial);

        let response = self.call_llm(config, &system_prompt, &user_prompt).await?;

        Ok(response)
    }

    /// Build the user prompt for an agent
    fn build_user_prompt(
        &self,
        session: &MultiAgentReviewSession,
        persona: AgentPersona,
        is_initial: bool,
    ) -> String {
        let rules_text = session.rules.formatted_rules();
        let history = if is_initial {
            String::new()
        } else {
            session.format_conversation_history()
        };

        let task_description = if is_initial {
            "Please review the following Python agent code and provide your initial assessment."
        } else {
            "Please review the discussion so far and provide your updated assessment. You may change your position if other reviewers raised valid points."
        };

        if persona == AgentPersona::Moderator {
            format!(
                r#"{history}

You are moderating a code review discussion. Please synthesize the reviewers' opinions and make a final determination.

TERM CHALLENGE RULES:
{rules}

CODE BEING REVIEWED:
```python
{code}
```

Please provide your final verdict (APPROVED or REJECTED) with a clear summary of the consensus."#,
                history = history,
                rules = rules_text,
                code = session.code
            )
        } else {
            format!(
                r#"{task}
{history}
TERM CHALLENGE RULES:
{rules}

CODE TO REVIEW:
```python
{code}
```

Provide your assessment, clearly stating whether you APPROVE or REJECT this code."#,
                task = task_description,
                history = history,
                rules = rules_text,
                code = session.code
            )
        }
    }

    /// Call the LLM API
    async fn call_llm(
        &self,
        config: &LlmConfig,
        system_prompt: &str,
        user_prompt: &str,
    ) -> Result<String, MultiAgentError> {
        if config.api_key.is_empty() {
            return Err(MultiAgentError::ConfigError(
                "API key not configured".to_string(),
            ));
        }

        let response_json = if config.provider.is_anthropic() {
            self.call_anthropic_api(config, system_prompt, user_prompt)
                .await?
        } else {
            self.call_openai_compatible_api(config, system_prompt, user_prompt)
                .await?
        };

        self.extract_response_text(&response_json, config.provider.is_anthropic())
    }

    /// Call OpenAI-compatible API
    async fn call_openai_compatible_api(
        &self,
        config: &LlmConfig,
        system_prompt: &str,
        user_prompt: &str,
    ) -> Result<serde_json::Value, MultiAgentError> {
        let request_body = serde_json::json!({
            "model": config.model_id,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": config.max_tokens,
            "temperature": 0.3
        });

        let response = self
            .client
            .post(config.endpoint())
            .header("Authorization", format!("Bearer {}", config.api_key))
            .header("Content-Type", "application/json")
            .timeout(Duration::from_secs(config.timeout_secs))
            .json(&request_body)
            .send()
            .await
            .map_err(|e| MultiAgentError::LlmError(e.to_string()))?;

        self.handle_response(response).await
    }

    /// Call Anthropic API
    async fn call_anthropic_api(
        &self,
        config: &LlmConfig,
        system_prompt: &str,
        user_prompt: &str,
    ) -> Result<serde_json::Value, MultiAgentError> {
        let request_body = serde_json::json!({
            "model": config.model_id,
            "system": system_prompt,
            "messages": [
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": config.max_tokens,
            "temperature": 0.3
        });

        let response = self
            .client
            .post(config.endpoint())
            .header("x-api-key", &config.api_key)
            .header("anthropic-version", "2023-06-01")
            .header("Content-Type", "application/json")
            .timeout(Duration::from_secs(config.timeout_secs))
            .json(&request_body)
            .send()
            .await
            .map_err(|e| MultiAgentError::LlmError(e.to_string()))?;

        self.handle_response(response).await
    }

    /// Handle HTTP response
    async fn handle_response(
        &self,
        response: reqwest::Response,
    ) -> Result<serde_json::Value, MultiAgentError> {
        if response.status() == reqwest::StatusCode::TOO_MANY_REQUESTS {
            return Err(MultiAgentError::LlmError("Rate limited".to_string()));
        }

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_default();
            return Err(MultiAgentError::LlmError(format!(
                "HTTP {}: {}",
                status, error_text
            )));
        }

        response
            .json()
            .await
            .map_err(|e| MultiAgentError::LlmError(format!("Invalid JSON response: {}", e)))
    }

    /// Extract text from LLM response
    fn extract_response_text(
        &self,
        response_json: &serde_json::Value,
        is_anthropic: bool,
    ) -> Result<String, MultiAgentError> {
        if is_anthropic {
            // Anthropic format: content[0].text
            let content = response_json["content"]
                .as_array()
                .ok_or_else(|| MultiAgentError::LlmError("No content in response".to_string()))?;

            for block in content {
                if block["type"].as_str() == Some("text") {
                    if let Some(text) = block["text"].as_str() {
                        return Ok(text.to_string());
                    }
                }
            }
            Err(MultiAgentError::LlmError(
                "No text content in Anthropic response".to_string(),
            ))
        } else {
            // OpenAI format: choices[0].message.content
            response_json["choices"][0]["message"]["content"]
                .as_str()
                .map(String::from)
                .ok_or_else(|| MultiAgentError::LlmError("No content in response".to_string()))
        }
    }

    /// Determine final verdict from moderator response
    fn determine_final_verdict(
        &self,
        session: &MultiAgentReviewSession,
        moderator_response: &str,
    ) -> bool {
        let response_upper = moderator_response.to_uppercase();

        // Check moderator's explicit verdict
        if response_upper.contains("FINAL VERDICT: APPROVED")
            || response_upper.contains("VERDICT: APPROVED")
        {
            return true;
        }

        if response_upper.contains("FINAL VERDICT: REJECTED")
            || response_upper.contains("VERDICT: REJECTED")
            || response_upper.contains("FINAL VERDICT: REJECT")
        {
            return false;
        }

        // Fall back to consensus calculation
        let (approval_rate, _) = session.calculate_consensus();
        approval_rate >= CONSENSUS_THRESHOLD
    }

    /// Extract summary from moderator response
    fn extract_summary(&self, moderator_response: &str) -> String {
        // Try to find a summary section
        let lines: Vec<&str> = moderator_response.lines().collect();

        // Look for verdict line and following context
        for (i, line) in lines.iter().enumerate() {
            let upper = line.to_uppercase();
            if upper.contains("VERDICT") || upper.contains("SUMMARY") {
                let summary_lines: Vec<&str> = lines[i..].iter().take(5).copied().collect();
                return summary_lines.join("\n");
            }
        }

        // Return last paragraph as summary
        let paragraphs: Vec<&str> = moderator_response.split("\n\n").collect();
        paragraphs.last().unwrap_or(&moderator_response).to_string()
    }

    /// Get a formatted transcript of the conversation (Discord-like)
    pub fn format_transcript(&self, result: &ConsensusResult) -> String {
        let mut transcript = String::new();

        transcript.push_str("=".repeat(60).as_str());
        transcript.push_str("\n     MULTI-AGENT CODE REVIEW TRANSCRIPT\n");
        transcript.push_str("=".repeat(60).as_str());
        transcript.push('\n');

        let mut current_round = 0;
        for msg in &result.conversation {
            if msg.round != current_round {
                current_round = msg.round;
                transcript.push_str(&format!("\n--- Round {} ---\n\n", current_round));
            }

            transcript.push_str(&format!(
                "{} **{}**\n{}\n\n",
                msg.persona.icon(),
                msg.persona.name(),
                msg.content
            ));
        }

        transcript.push_str("=".repeat(60).as_str());
        transcript.push_str(&format!(
            "\n FINAL VERDICT: {} (Confidence: {:.0}%)\n",
            if result.approved {
                "APPROVED"
            } else {
                "REJECTED"
            },
            result.confidence * 100.0
        ));
        transcript.push_str("=".repeat(60).as_str());
        transcript.push('\n');

        transcript
    }
}

/// Review session configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReviewSessionConfig {
    /// Maximum debate rounds
    pub max_rounds: usize,
    /// Consensus threshold (0.0 to 1.0)
    pub consensus_threshold: f64,
    /// Whether to require unanimous approval
    pub require_unanimous: bool,
    /// Custom validation rules
    pub custom_rules: Option<Vec<String>>,
}

impl Default for ReviewSessionConfig {
    fn default() -> Self {
        Self {
            max_rounds: MAX_DEBATE_ROUNDS,
            consensus_threshold: CONSENSUS_THRESHOLD,
            require_unanimous: false,
            custom_rules: None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_agent_persona_names() {
        assert_eq!(AgentPersona::SecurityAuditor.name(), "Security Auditor");
        assert_eq!(AgentPersona::ReadabilityExpert.name(), "Readability Expert");
        assert_eq!(AgentPersona::ComplianceChecker.name(), "Compliance Checker");
        assert_eq!(AgentPersona::Moderator.name(), "Moderator");
    }

    #[test]
    fn test_agent_persona_icons() {
        assert_eq!(AgentPersona::SecurityAuditor.icon(), "[SECURITY]");
        assert_eq!(AgentPersona::ReadabilityExpert.icon(), "[READABILITY]");
        assert_eq!(AgentPersona::ComplianceChecker.icon(), "[COMPLIANCE]");
        assert_eq!(AgentPersona::Moderator.icon(), "[MODERATOR]");
    }

    #[test]
    fn test_agent_persona_reviewers() {
        let reviewers = AgentPersona::reviewers();
        assert_eq!(reviewers.len(), 3);
        assert!(!reviewers.contains(&AgentPersona::Moderator));
    }

    #[test]
    fn test_agent_persona_all() {
        let all = AgentPersona::all();
        assert_eq!(all.len(), 4);
        assert!(all.contains(&AgentPersona::Moderator));
    }

    #[test]
    fn test_conversation_message_extract_verdict_approve() {
        let verdict = ConversationMessage::extract_verdict("I APPROVE this code.");
        assert_eq!(verdict, Some(AgentVerdict::Approve));

        let verdict = ConversationMessage::extract_verdict("This is APPROVED.");
        assert_eq!(verdict, Some(AgentVerdict::Approve));
    }

    #[test]
    fn test_conversation_message_extract_verdict_reject() {
        let verdict = ConversationMessage::extract_verdict("I REJECT this code.");
        assert_eq!(verdict, Some(AgentVerdict::Reject));

        let verdict = ConversationMessage::extract_verdict("This is NOT APPROVED.");
        assert_eq!(verdict, Some(AgentVerdict::Reject));

        let verdict = ConversationMessage::extract_verdict("CANNOT APPROVE this code.");
        assert_eq!(verdict, Some(AgentVerdict::Reject));
    }

    #[test]
    fn test_conversation_message_extract_verdict_none() {
        let verdict = ConversationMessage::extract_verdict("The code looks okay.");
        assert_eq!(verdict, None);
    }

    #[test]
    fn test_conversation_message_new() {
        let msg = ConversationMessage::new(
            AgentPersona::SecurityAuditor,
            "I APPROVE this code.".to_string(),
            1,
        );
        assert_eq!(msg.persona, AgentPersona::SecurityAuditor);
        assert_eq!(msg.round, 1);
        assert_eq!(msg.verdict, Some(AgentVerdict::Approve));
    }

    #[test]
    fn test_review_session_new() {
        let rules = ValidationRules::default_term_challenge_rules();
        let session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules.clone(),
        );
        assert_eq!(session.agent_hash, "hash123");
        assert_eq!(session.code, "print('hello')");
        assert!(session.conversation.is_empty());
        assert_eq!(session.round, 0);
    }

    #[test]
    fn test_review_session_add_message() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        session.add_message(
            AgentPersona::SecurityAuditor,
            "I APPROVE this code.".to_string(),
        );
        assert_eq!(session.conversation.len(), 1);
    }

    #[test]
    fn test_review_session_calculate_consensus_empty() {
        let rules = ValidationRules::default_term_challenge_rules();
        let session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );
        let (rate, unanimous) = session.calculate_consensus();
        assert_eq!(rate, 0.0);
        assert!(!unanimous);
    }

    #[test]
    fn test_review_session_calculate_consensus_unanimous_approve() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        session.add_message(AgentPersona::SecurityAuditor, "I APPROVE".to_string());
        session.add_message(AgentPersona::ReadabilityExpert, "APPROVED".to_string());
        session.add_message(AgentPersona::ComplianceChecker, "I APPROVE".to_string());

        let (rate, unanimous) = session.calculate_consensus();
        assert_eq!(rate, 1.0);
        assert!(unanimous);
    }

    #[test]
    fn test_review_session_calculate_consensus_unanimous_reject() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        session.add_message(AgentPersona::SecurityAuditor, "I REJECT".to_string());
        session.add_message(AgentPersona::ReadabilityExpert, "REJECTED".to_string());
        session.add_message(AgentPersona::ComplianceChecker, "NOT APPROVED".to_string());

        let (rate, unanimous) = session.calculate_consensus();
        assert_eq!(rate, 0.0);
        assert!(unanimous);
    }

    #[test]
    fn test_review_session_calculate_consensus_mixed() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        session.add_message(AgentPersona::SecurityAuditor, "I APPROVE".to_string());
        session.add_message(AgentPersona::ReadabilityExpert, "APPROVED".to_string());
        session.add_message(AgentPersona::ComplianceChecker, "I REJECT".to_string());

        let (rate, unanimous) = session.calculate_consensus();
        assert!((rate - 0.666).abs() < 0.01);
        assert!(!unanimous);
    }

    #[test]
    fn test_review_session_format_conversation_history() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        session.add_message(
            AgentPersona::SecurityAuditor,
            "Code looks secure.".to_string(),
        );

        let history = session.format_conversation_history();
        assert!(history.contains("Security Auditor"));
        assert!(history.contains("Code looks secure"));
        assert!(history.contains("PREVIOUS DISCUSSION"));
    }

    #[test]
    fn test_review_session_format_conversation_history_empty() {
        let rules = ValidationRules::default_term_challenge_rules();
        let session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        let history = session.format_conversation_history();
        assert!(history.is_empty());
    }

    #[test]
    fn test_review_session_extract_positions() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        session.add_message(AgentPersona::SecurityAuditor, "I APPROVE".to_string());
        session.add_message(AgentPersona::ReadabilityExpert, "I REJECT".to_string());

        let positions = session.extract_positions();
        assert_eq!(positions.len(), 2);
    }

    #[test]
    fn test_consensus_result_fields() {
        let result = ConsensusResult {
            approved: true,
            confidence: 0.9,
            unanimous: false,
            summary: "Code approved".to_string(),
            positions: vec![],
            conversation: vec![],
            rounds: 2,
            issues: vec![],
            reviewed_at: 123456,
        };

        assert!(result.approved);
        assert_eq!(result.confidence, 0.9);
        assert!(!result.unanimous);
        assert_eq!(result.rounds, 2);
    }

    #[test]
    fn test_review_issue_fields() {
        let issue = ReviewIssue {
            category: IssueCategory::Security,
            severity: IssueSeverity::Critical,
            description: "Uses subprocess".to_string(),
            found_by: AgentPersona::SecurityAuditor,
            code_snippet: Some("import subprocess".to_string()),
        };

        assert_eq!(issue.category, IssueCategory::Security);
        assert_eq!(issue.severity, IssueSeverity::Critical);
        assert_eq!(issue.found_by, AgentPersona::SecurityAuditor);
    }

    #[test]
    fn test_review_session_config_default() {
        let config = ReviewSessionConfig::default();
        assert_eq!(config.max_rounds, MAX_DEBATE_ROUNDS);
        assert_eq!(config.consensus_threshold, CONSENSUS_THRESHOLD);
        assert!(!config.require_unanimous);
        assert!(config.custom_rules.is_none());
    }

    #[test]
    fn test_multi_agent_error_from_review_error() {
        let review_err = ReviewError::Timeout;
        let multi_err: MultiAgentError = review_err.into();
        assert!(matches!(multi_err, MultiAgentError::LlmError(_)));
    }

    #[test]
    fn test_agent_persona_system_prompts_not_empty() {
        for persona in AgentPersona::all() {
            let prompt = persona.system_prompt();
            assert!(!prompt.is_empty());
            assert!(prompt.len() > 100); // Should be substantial prompts
        }
    }

    #[test]
    fn test_conversation_message_format_display() {
        let msg =
            ConversationMessage::new(AgentPersona::SecurityAuditor, "Test message".to_string(), 1);
        let display = msg.format_display();
        assert!(display.contains("[SECURITY]"));
        assert!(display.contains("Security Auditor"));
        assert!(display.contains("Test message"));
    }

    #[test]
    fn test_issue_categories() {
        assert_ne!(IssueCategory::Security, IssueCategory::Readability);
        assert_ne!(IssueCategory::Compliance, IssueCategory::Obfuscation);
    }

    #[test]
    fn test_issue_severities() {
        assert_ne!(IssueSeverity::Critical, IssueSeverity::Major);
        assert_ne!(IssueSeverity::Minor, IssueSeverity::Info);
    }

    #[test]
    fn test_manager_new() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);
        let rules = manager.get_rules();
        assert!(!rules.rules.is_empty());
    }

    #[test]
    fn test_manager_update_rules() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);

        let new_rules = ValidationRules::new(vec!["New rule".to_string()]);
        manager.update_rules(new_rules.clone());

        let current = manager.get_rules();
        assert_eq!(current.rules, new_rules.rules);
    }

    #[test]
    fn test_manager_determine_final_verdict_explicit_approved() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);
        let rules = ValidationRules::default_term_challenge_rules();
        let session = MultiAgentReviewSession::new("hash".to_string(), "code".to_string(), rules);

        let result =
            manager.determine_final_verdict(&session, "FINAL VERDICT: APPROVED. The code is safe.");
        assert!(result);
    }

    #[test]
    fn test_manager_determine_final_verdict_explicit_rejected() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);
        let rules = ValidationRules::default_term_challenge_rules();
        let session = MultiAgentReviewSession::new("hash".to_string(), "code".to_string(), rules);

        let result = manager
            .determine_final_verdict(&session, "FINAL VERDICT: REJECTED. Security issues found.");
        assert!(!result);
    }

    #[test]
    fn test_manager_extract_summary() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);

        let response = "Some analysis...\n\nVERDICT: APPROVED\nThe code is safe.";
        let summary = manager.extract_summary(response);
        assert!(summary.contains("VERDICT"));
    }

    #[test]
    fn test_manager_extract_summary_no_verdict() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);

        let response = "First paragraph.\n\nSecond paragraph.\n\nLast paragraph.";
        let summary = manager.extract_summary(response);
        assert_eq!(summary, "Last paragraph.");
    }

    #[test]
    fn test_manager_format_transcript() {
        let config = LlmConfig::default();
        let manager = MultiAgentReviewManager::new(config);

        let result = ConsensusResult {
            approved: true,
            confidence: 0.95,
            unanimous: true,
            summary: "All approved".to_string(),
            positions: vec![],
            conversation: vec![ConversationMessage::new(
                AgentPersona::SecurityAuditor,
                "I APPROVE".to_string(),
                1,
            )],
            rounds: 1,
            issues: vec![],
            reviewed_at: 123456,
        };

        let transcript = manager.format_transcript(&result);
        assert!(transcript.contains("MULTI-AGENT CODE REVIEW TRANSCRIPT"));
        assert!(transcript.contains("[SECURITY]"));
        assert!(transcript.contains("APPROVED"));
        assert!(transcript.contains("95%"));
    }

    #[test]
    fn test_moderator_excludes_from_reviewers() {
        let reviewers = AgentPersona::reviewers();
        for persona in &reviewers {
            assert_ne!(*persona, AgentPersona::Moderator);
        }
    }

    #[test]
    fn test_calculate_consensus_ignores_moderator() {
        let rules = ValidationRules::default_term_challenge_rules();
        let mut session = MultiAgentReviewSession::new(
            "hash123".to_string(),
            "print('hello')".to_string(),
            rules,
        );

        // Add 3 approvals from reviewers
        session.add_message(AgentPersona::SecurityAuditor, "I APPROVE".to_string());
        session.add_message(AgentPersona::ReadabilityExpert, "APPROVED".to_string());
        session.add_message(AgentPersona::ComplianceChecker, "I APPROVE".to_string());
        // Moderator rejection should not affect consensus calculation
        session.add_message(AgentPersona::Moderator, "I REJECT".to_string());

        let (rate, unanimous) = session.calculate_consensus();
        // Should still be 100% from reviewers only
        assert_eq!(rate, 1.0);
        assert!(unanimous);
    }

    #[test]
    fn test_agent_position_fields() {
        let position = AgentPosition {
            persona: AgentPersona::SecurityAuditor,
            verdict: AgentVerdict::Approve,
            key_points: vec!["No security issues".to_string()],
            concerns: vec![],
        };

        assert_eq!(position.persona, AgentPersona::SecurityAuditor);
        assert_eq!(position.verdict, AgentVerdict::Approve);
        assert_eq!(position.key_points.len(), 1);
        assert!(position.concerns.is_empty());
    }
}
