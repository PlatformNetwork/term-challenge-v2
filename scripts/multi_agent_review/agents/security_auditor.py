"""Security Auditor Agent - Checks for malicious/obfuscated code."""

import re
from typing import List

from .base import CodeAnalysis, ReviewAgent, ReviewMessage, ReviewVerdict


class SecurityAuditor(ReviewAgent):
    """Agent specialized in detecting security issues and obfuscated code.
    
    This agent uses LLM-driven analysis rather than hardcoded pattern matching.
    The LLM is responsible for identifying security issues, obfuscation attempts,
    and dangerous operations based on its understanding of the code.
    """

    def __init__(self, llm_client=None):
        """Initialize the Security Auditor agent."""
        super().__init__(
            name="SecurityAuditor",
            role="Security & Obfuscation Detection Specialist",
            llm_client=llm_client,
        )

    def analyze_code(self, code: str, filename: str = "agent.py") -> CodeAnalysis:
        """
        Analyze code for security issues using LLM reasoning.

        Instead of hardcoded patterns, the LLM is responsible for identifying:
        - Obfuscation attempts
        - Security vulnerabilities
        - Dangerous operations

        Args:
            code: The source code to analyze.
            filename: Name of the file being analyzed.

        Returns:
            CodeAnalysis with security findings.
        """
        # If LLM client available, use LLM-driven analysis
        if self.llm_client:
            return self._llm_analyze_code(code, filename)

        # Fallback: return neutral analysis that defers to discussion
        # Collect informational positive signals only (not decision-making)
        positives = self._collect_positive_signals(code)

        return CodeAnalysis(
            issues=[],
            warnings=[],
            positives=positives,
            verdict=ReviewVerdict.NEEDS_DISCUSSION,
            confidence=0.5,
        )

    def _collect_positive_signals(self, code: str) -> List[str]:
        """
        Collect informational positive signals from code.
        
        These are informational only and do not drive decision-making.
        The LLM makes all security decisions.

        Args:
            code: The source code to analyze.

        Returns:
            List of positive signal descriptions.
        """
        positives: List[str] = []

        if "term_sdk" in code or "from term_sdk import" in code:
            positives.append("Uses official term_sdk")

        if re.search(r"def\s+(setup|solve|run|cleanup)\s*\(", code):
            positives.append("Has standard agent methods")

        if '"""' in code or "'''" in code:
            positives.append("Has docstrings")

        if re.search(r"#.*[A-Za-z]", code):
            positives.append("Has inline comments")

        return positives

    def _llm_analyze_code(self, code: str, filename: str) -> CodeAnalysis:
        """
        Perform LLM-driven security analysis of code.

        Args:
            code: The source code to analyze.
            filename: Name of the file being analyzed.

        Returns:
            CodeAnalysis with LLM-determined findings.
        """
        positives = self._collect_positive_signals(code)

        prompt = f"""Analyze the following Python code for security issues.

File: {filename}

```python
{code[:8000]}
```

You are a security auditor checking for:
1. Obfuscated or encoded malicious code (exec/eval with strings, base64 encoded code, etc.)
2. Dangerous operations (shell commands, network access, file system manipulation)
3. Sandbox escape attempts
4. Code injection vulnerabilities

Respond in this exact format:
ISSUES: [comma-separated list of critical security issues, or "none"]
WARNINGS: [comma-separated list of concerning patterns that need review, or "none"]
VERDICT: [APPROVE or REJECT or NEEDS_DISCUSSION]
CONFIDENCE: [0.0 to 1.0]

Be thorough but fair - flag real security issues, not coding style preferences."""

        try:
            response = self.llm_client.chat(
                [
                    {"role": "system", "content": self.get_system_prompt()},
                    {"role": "user", "content": prompt},
                ]
            )

            response_text = response if isinstance(response, str) else str(response)
            return self._parse_llm_analysis(response_text, positives)
        except Exception:
            # On LLM failure, defer to discussion
            return CodeAnalysis(
                issues=[],
                warnings=[],
                positives=positives,
                verdict=ReviewVerdict.NEEDS_DISCUSSION,
                confidence=0.5,
            )

    def _parse_llm_analysis(
        self, response: str, positives: List[str]
    ) -> CodeAnalysis:
        """
        Parse LLM response into CodeAnalysis.

        Args:
            response: The LLM response text.
            positives: Pre-collected positive signals.

        Returns:
            Parsed CodeAnalysis object.
        """
        issues: List[str] = []
        warnings: List[str] = []
        verdict = ReviewVerdict.NEEDS_DISCUSSION
        confidence = 0.5

        lines = response.strip().split("\n")
        for line in lines:
            line_upper = line.upper()
            if line_upper.startswith("ISSUES:"):
                content = line.split(":", 1)[1].strip()
                if content.lower() != "none" and content:
                    issues = [i.strip() for i in content.split(",") if i.strip()]
            elif line_upper.startswith("WARNINGS:"):
                content = line.split(":", 1)[1].strip()
                if content.lower() != "none" and content:
                    warnings = [w.strip() for w in content.split(",") if w.strip()]
            elif line_upper.startswith("VERDICT:"):
                content = line.split(":", 1)[1].strip().upper()
                if "APPROVE" in content:
                    verdict = ReviewVerdict.APPROVE
                elif "REJECT" in content:
                    verdict = ReviewVerdict.REJECT
                else:
                    verdict = ReviewVerdict.NEEDS_DISCUSSION
            elif line_upper.startswith("CONFIDENCE:"):
                try:
                    confidence = float(line.split(":", 1)[1].strip())
                    confidence = max(0.0, min(1.0, confidence))
                except ValueError:
                    confidence = 0.5

        return CodeAnalysis(
            issues=issues,
            warnings=warnings,
            positives=positives,
            verdict=verdict,
            confidence=confidence,
        )

    def respond_to_discussion(
        self,
        code: str,
        conversation: List[ReviewMessage],
        my_analysis: CodeAnalysis,
    ) -> ReviewMessage:
        """
        Respond to discussion based on other agents' input.

        Args:
            code: The source code being reviewed.
            conversation: List of messages in the discussion so far.
            my_analysis: This agent's initial analysis.

        Returns:
            A ReviewMessage with the agent's response.
        """
        # Find last message not from us
        other_messages = [m for m in conversation if m.agent_name != self.name]

        if not other_messages:
            # First response - state our analysis
            content = self._format_initial_response(my_analysis)
            return self._create_message(content, my_analysis.verdict)

        last_msg = other_messages[-1]

        # If using LLM, generate contextual response
        if self.llm_client:
            return self._llm_respond(code, conversation, my_analysis, last_msg)

        # Rule-based response
        return self._rule_based_respond(my_analysis, last_msg)

    def _format_initial_response(self, analysis: CodeAnalysis) -> str:
        """
        Format initial analysis into a readable message.

        Args:
            analysis: The code analysis results.

        Returns:
            Formatted message string.
        """
        parts = ["## Security Analysis\n"]

        if analysis.issues:
            parts.append("**ðŸš¨ Critical Issues Found:**")
            for issue in analysis.issues:
                parts.append(f"- {issue}")
            parts.append("")

        if analysis.warnings:
            parts.append("**âš ï¸ Warnings:**")
            for warning in analysis.warnings:
                parts.append(f"- {warning}")
            parts.append("")

        if analysis.positives:
            parts.append("**âœ… Positive Signals:**")
            for positive in analysis.positives:
                parts.append(f"- {positive}")
            parts.append("")

        verdict_emoji = {
            ReviewVerdict.APPROVE: "âœ…",
            ReviewVerdict.REJECT: "âŒ",
            ReviewVerdict.NEEDS_DISCUSSION: "ðŸ¤”",
        }

        parts.append(
            f"**Verdict:** {verdict_emoji[analysis.verdict]} {analysis.verdict.value}"
        )
        parts.append(f"**Confidence:** {analysis.confidence:.0%}")

        return "\n".join(parts)

    def _rule_based_respond(
        self, analysis: CodeAnalysis, last_msg: ReviewMessage
    ) -> ReviewMessage:
        """
        Generate rule-based response without LLM.

        Args:
            analysis: This agent's analysis.
            last_msg: The last message from another agent.

        Returns:
            A ReviewMessage with the response.
        """
        verdict = analysis.verdict

        # If other agent approved but we found issues, maintain rejection
        if last_msg.verdict == ReviewVerdict.APPROVE and analysis.issues:
            content = (
                f"I understand @{last_msg.agent_name}'s perspective, but I maintain my "
                f"concerns about security. The following issues cannot be ignored:\n\n"
                + "\n".join(f"- {i}" for i in analysis.issues[:3])
            )
            verdict = ReviewVerdict.REJECT

        # If other agent rejected but we found it safe, express support with caution
        elif last_msg.verdict == ReviewVerdict.REJECT and not analysis.issues:
            content = (
                f"From a pure security standpoint, I found no critical issues. "
                f"However, I defer to @{last_msg.agent_name}'s concerns in their domain."
            )
            verdict = ReviewVerdict.NEEDS_DISCUSSION

        # General acknowledgment
        else:
            content = (
                f"Acknowledged @{last_msg.agent_name}'s points. "
                f"My security assessment stands: {len(analysis.issues)} critical issues, "
                f"{len(analysis.warnings)} warnings."
            )

        return self._create_message(content, verdict, last_msg.agent_name)

    def _llm_respond(
        self,
        code: str,
        conversation: List[ReviewMessage],
        analysis: CodeAnalysis,
        last_msg: ReviewMessage,
    ) -> ReviewMessage:
        """
        Generate LLM-powered response.

        Args:
            code: The source code being reviewed.
            conversation: List of messages in the discussion.
            analysis: This agent's analysis.
            last_msg: The last message from another agent.

        Returns:
            A ReviewMessage with the LLM-generated response.
        """
        # Format conversation for LLM
        conv_text = "\n\n".join(
            [
                f"**{m.agent_name}** ({m.verdict.value if m.verdict else 'N/A'}):\n{m.content}"
                for m in conversation[-5:]  # Last 5 messages
            ]
        )

        prompt = f"""You are the Security Auditor in a code review discussion.
Your analysis found:
- Issues: {analysis.issues}
- Warnings: {analysis.warnings}
- Positives: {analysis.positives}

Recent conversation:
{conv_text}

Last message was from {last_msg.agent_name}. Respond thoughtfully:
1. Acknowledge their points
2. Maintain/adjust your security assessment
3. Provide specific evidence from code
4. End with your current verdict: APPROVE, REJECT, or NEEDS_DISCUSSION

Keep response under 200 words. Be professional but direct."""

        try:
            response = self.llm_client.chat(
                [
                    {"role": "system", "content": self.get_system_prompt()},
                    {"role": "user", "content": prompt},
                ]
            )

            # Extract verdict from response
            verdict = analysis.verdict
            response_text = response if isinstance(response, str) else str(response)
            if "APPROVE" in response_text.upper():
                verdict = ReviewVerdict.APPROVE
            elif "REJECT" in response_text.upper():
                verdict = ReviewVerdict.REJECT

            return self._create_message(response_text, verdict, last_msg.agent_name)
        except Exception:
            return self._rule_based_respond(analysis, last_msg)

    def get_system_prompt(self) -> str:
        """Get the system prompt for this agent's LLM."""
        return """You are a Security Auditor AI specialized in detecting:
- Obfuscated or encoded malicious code
- Dangerous operations (file system, network, process execution)
- Code injection vulnerabilities
- Sandbox escape attempts

You are reviewing Python code submissions for an AI agent competition.
Be thorough but fair - flag real security issues, not coding style preferences.
Your job is to protect the platform from malicious submissions."""
