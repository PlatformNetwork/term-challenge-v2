"""Rule Compliance Verifier Agent - Checks against AGENTS.md rules."""
from typing import List
from .base import ReviewAgent, CodeAnalysis, ReviewVerdict, ReviewMessage


class RuleComplianceVerifier(ReviewAgent):
    """Agent specialized in verifying compliance with term-challenge rules.
    
    This agent uses LLM-driven analysis rather than hardcoded pattern matching.
    The LLM is responsible for understanding and applying term-challenge rules
    based on its knowledge of the AGENTS.md specification.
    """
    
    def __init__(self, llm_client=None):
        super().__init__(
            name="RuleComplianceVerifier",
            role="Term-Challenge Rules Compliance Specialist",
            llm_client=llm_client
        )
        # No hardcoded rules - LLM decides compliance
    
    def analyze_code(self, code: str, filename: str = "agent.py") -> CodeAnalysis:
        """
        Verify code compliance using LLM reasoning.

        The LLM is responsible for understanding and applying rules,
        not hardcoded pattern matching.

        Args:
            code: The source code to analyze.
            filename: Name of the file being analyzed.

        Returns:
            CodeAnalysis with compliance findings.
        """
        if self.llm_client:
            return self._llm_analyze_code(code, filename)

        # Fallback: return neutral analysis that defers to discussion
        return CodeAnalysis(
            issues=[],
            warnings=[],
            positives=[],
            verdict=ReviewVerdict.NEEDS_DISCUSSION,
            confidence=0.5
        )

    def _llm_analyze_code(self, code: str, filename: str) -> CodeAnalysis:
        """
        Perform LLM-driven compliance analysis of code.

        Args:
            code: The source code to analyze.
            filename: Name of the file being analyzed.

        Returns:
            CodeAnalysis with LLM-determined findings.
        """
        prompt = f"""Analyze the following Python code for term-challenge rules compliance.

File: {filename}

```python
{code[:8000]}
```

You are a rules compliance verifier checking for:
1. Proper SDK usage (term_sdk SDK 2.0 pattern OR argparse+subprocess SDK 3.0 pattern)
2. Required methods and structure (run/solve methods, ctx.done() calls, main guard)
3. Forbidden network modules (direct socket, urllib access instead of LLM proxy)
4. Sandbox escape attempts (accessing /proc, /sys, /dev, privilege manipulation)
5. Proper agent structure and completion logic

Respond in this exact format:
ISSUES: [comma-separated list of rule violations, or "none"]
WARNINGS: [comma-separated list of compliance concerns, or "none"]  
POSITIVES: [comma-separated list of compliant aspects, or "none"]
VERDICT: [APPROVE or REJECT or NEEDS_DISCUSSION]
CONFIDENCE: [0.0 to 1.0]

Be strict but fair - rule violations are non-negotiable per AGENTS.md requirements."""

        try:
            response = self.llm_client.chat([
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": prompt}
            ])

            response_text = response if isinstance(response, str) else str(response)
            return self._parse_llm_analysis(response_text)
        except Exception:
            # On LLM failure, defer to discussion
            return CodeAnalysis(
                issues=[],
                warnings=[],
                positives=[],
                verdict=ReviewVerdict.NEEDS_DISCUSSION,
                confidence=0.5
            )

    def _parse_llm_analysis(self, response: str) -> CodeAnalysis:
        """
        Parse LLM response into CodeAnalysis.

        Args:
            response: The LLM response text.

        Returns:
            Parsed CodeAnalysis object.
        """
        issues: List[str] = []
        warnings: List[str] = []
        positives: List[str] = []
        verdict = ReviewVerdict.NEEDS_DISCUSSION
        confidence = 0.5

        lines = response.strip().split("\n")
        for line in lines:
            line_upper = line.upper()
            if line_upper.startswith("ISSUES:"):
                content = line.split(":", 1)[1].strip()
                if content.lower() != "none" and content:
                    issues = [i.strip() for i in content.split(",") if i.strip()]
            elif line_upper.startswith("WARNINGS:"):
                content = line.split(":", 1)[1].strip()
                if content.lower() != "none" and content:
                    warnings = [w.strip() for w in content.split(",") if w.strip()]
            elif line_upper.startswith("POSITIVES:"):
                content = line.split(":", 1)[1].strip()
                if content.lower() != "none" and content:
                    positives = [p.strip() for p in content.split(",") if p.strip()]
            elif line_upper.startswith("VERDICT:"):
                content = line.split(":", 1)[1].strip().upper()
                if "APPROVE" in content:
                    verdict = ReviewVerdict.APPROVE
                elif "REJECT" in content:
                    verdict = ReviewVerdict.REJECT
                else:
                    verdict = ReviewVerdict.NEEDS_DISCUSSION
            elif line_upper.startswith("CONFIDENCE:"):
                try:
                    confidence = float(line.split(":", 1)[1].strip())
                    confidence = max(0.0, min(1.0, confidence))
                except ValueError:
                    confidence = 0.5

        return CodeAnalysis(
            issues=issues,
            warnings=warnings,
            positives=positives,
            verdict=verdict,
            confidence=confidence
        )
    
    def respond_to_discussion(
        self,
        code: str,
        conversation: List[ReviewMessage],
        my_analysis: CodeAnalysis
    ) -> ReviewMessage:
        """Respond to discussion."""
        other_messages = [m for m in conversation if m.agent_name != self.name]
        
        if not other_messages:
            content = self._format_initial_response(my_analysis)
            return self._create_message(content, my_analysis.verdict)
        
        last_msg = other_messages[-1]
        
        if self.llm_client:
            return self._llm_respond(code, conversation, my_analysis, last_msg)
        
        return self._rule_based_respond(my_analysis, last_msg)
    
    def _format_initial_response(self, analysis: CodeAnalysis) -> str:
        """Format initial analysis."""
        parts = ["## Rules Compliance Analysis\n"]
        
        if analysis.issues:
            parts.append("**ðŸš¨ Rule Violations:**")
            for issue in analysis.issues:
                parts.append(f"- {issue}")
            parts.append("")
        
        if analysis.warnings:
            parts.append("**âš ï¸ Compliance Concerns:**")
            for warning in analysis.warnings:
                parts.append(f"- {warning}")
            parts.append("")
        
        if analysis.positives:
            parts.append("**âœ… Compliance Verified:**")
            for positive in analysis.positives:
                parts.append(f"- {positive}")
            parts.append("")
        
        verdict_emoji = {
            ReviewVerdict.APPROVE: "âœ…",
            ReviewVerdict.REJECT: "âŒ",
            ReviewVerdict.NEEDS_DISCUSSION: "ðŸ¤”"
        }
        
        parts.append(f"**Verdict:** {verdict_emoji[analysis.verdict]} {analysis.verdict.value}")
        parts.append(f"**Confidence:** {analysis.confidence:.0%}")
        
        return "\n".join(parts)
    
    def _rule_based_respond(
        self,
        analysis: CodeAnalysis,
        last_msg: ReviewMessage
    ) -> ReviewMessage:
        """Generate rule-based response."""
        # Rule compliance is strict - don't easily change verdict
        if analysis.issues:
            content = (
                f"Responding to @{last_msg.agent_name}: While I appreciate the input, "
                f"the code has {len(analysis.issues)} rule violation(s) that must be fixed. "
                f"These are non-negotiable per AGENTS.md requirements."
            )
            return self._create_message(content, ReviewVerdict.REJECT, last_msg.agent_name)
        
        content = (
            f"@{last_msg.agent_name}: From a rules perspective, the code is compliant. "
            f"SDK patterns are correct and no forbidden modules detected."
        )
        
        return self._create_message(content, analysis.verdict, last_msg.agent_name)
    
    def _llm_respond(
        self,
        code: str,
        conversation: List[ReviewMessage],
        analysis: CodeAnalysis,
        last_msg: ReviewMessage
    ) -> ReviewMessage:
        """Generate LLM response."""
        conv_text = "\n\n".join([
            f"**{m.agent_name}** ({m.verdict.value if m.verdict else 'N/A'}):\n{m.content}"
            for m in conversation[-5:]
        ])
        
        prompt = f"""You are the Rules Compliance Verifier in a code review.
Your analysis found:
- Rule Violations: {analysis.issues}
- Warnings: {analysis.warnings}
- Compliant: {analysis.positives}

Recent conversation:
{conv_text}

Last message was from {last_msg.agent_name}. Respond:
1. Rule compliance is non-negotiable
2. Cite specific rules from AGENTS.md if relevant
3. Be firm but professional
4. End with verdict: APPROVE, REJECT, or NEEDS_DISCUSSION

Keep under 200 words."""

        try:
            response = self.llm_client.chat([
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": prompt}
            ])
            
            # Rule violations = always reject
            verdict = ReviewVerdict.REJECT if analysis.issues else analysis.verdict
            if not analysis.issues:
                if "APPROVE" in response.upper():
                    verdict = ReviewVerdict.APPROVE
                elif "NEEDS_DISCUSSION" in response.upper():
                    verdict = ReviewVerdict.NEEDS_DISCUSSION
            
            return self._create_message(response, verdict, last_msg.agent_name)
        except Exception:
            return self._rule_based_respond(analysis, last_msg)
    
    def get_system_prompt(self) -> str:
        return """You are a Rules Compliance Verifier for term-challenge.
You verify code against these rules from AGENTS.md:

1. Must use term_sdk (SDK 2.0) OR argparse+subprocess (SDK 3.0)
2. No forbidden network modules (socket, urllib direct access)
3. No sandbox escape attempts
4. Proper agent structure with required methods
5. Must call ctx.done() or have clear completion

You are strict but fair. Rule violations are non-negotiable.
Security and compliance override other considerations."""
