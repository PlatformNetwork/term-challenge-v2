# How to Mine on Term Challenge

> Simple guide to build and submit your agent.

## What You Need

- Docker installed
- Python 3.10+
- LLM API key (OpenRouter recommended)
- `term` CLI built from repo

## Setup

```bash
# Build CLI
git clone https://github.com/PlatformNetwork/term-challenge.git
cd term-challenge
cargo build --release
export PATH="$PWD/target/release:$PATH"

# Download benchmark
term bench download terminal-bench@2.0
```

## Checkpoints

Checkpoints are curated task sets used for evaluation. Production uses `checkpoint4` (15 tasks).

```bash
# List available checkpoints
term bench list-checkpoints

# Run on a specific checkpoint
term bench agent -a ./my-agent --checkpoint checkpoint4

# Run on specific checkpoint file directly
term bench agent -a ./my-agent -d ./checkpoints/checkpoint4.json
```

| Checkpoint | Tasks | Description |
|------------|-------|-------------|
| `checkpoint1` | 30 | First 30 tasks (alphabetically) |
| `checkpoint2` | 30 | 20 hard failed + 10 complex succeeded |
| `checkpoint3` | 15 | 10 hardest (0% success) + 5 fragile (60%) |
| `checkpoint4` | 15 | Mix of tasks where top agents succeeded but our agent failed, and vice versa |

## Your Agent (Project Structure)

```
my-agent/
├── agent.py           # Entry point (REQUIRED)
├── requirements.txt   # Dependencies (REQUIRED)
```

### Minimal agent.py with LiteLLM

```python
#!/usr/bin/env python3
import argparse
import subprocess
import json
from litellm import completion

def shell(cmd, timeout=60):
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)
    return result.stdout + result.stderr

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--instruction", required=True)  # REQUIRED
    args = parser.parse_args()
    
    messages = [
        {"role": "system", "content": "You are a terminal agent. Reply JSON: {\"thinking\": \"...\", \"command\": \"...\", \"done\": false}"},
        {"role": "user", "content": args.instruction}
    ]
    
    for _ in range(100):
        response = completion(model="openrouter/anthropic/claude-sonnet-4", messages=messages, max_tokens=4096)
        reply = response.choices[0].message.content
        messages.append({"role": "assistant", "content": reply})
        
        try:
            data = json.loads(reply)
            if data.get("done"):
                break
            if cmd := data.get("command"):
                output = shell(cmd)
                messages.append({"role": "user", "content": f"Output:\n{output}"})
        except:
            pass
    
    print("[DONE]")

if __name__ == "__main__":
    main()
```

### requirements.txt

```
litellm>=1.0.0
```

## Test Your Agent

```bash
# Single task
term bench agent -a ./my-agent \
    -t ~/.cache/term-challenge/datasets/terminal-bench@2.0/hello-world

# Full benchmark (91 tasks)
term bench agent -a ./my-agent -d terminal-bench@2.0 --concurrent 4
```

> **Note:** API key is managed inside your agent code (see API Key Security section below).

## Submit

```bash
term wizard
```

Follow the prompts: select agent folder and confirm submission.

## The 5 Rules

1. **Let LLM reason** - No hardcoded `if "task" in instruction`
2. **Never match task content** - Agent has zero knowledge of specific tasks
3. **Explore first** - Run `ls`, `cat README.md` before acting
4. **Verify outputs** - Check files exist before finishing
5. **Always finish** - Print `[DONE]` or call `ctx.done()`

## Environment Variables (Optional)

These are passed to your agent by the validator but **API key must be in your code**:

| Variable | Description |
|----------|-------------|
| `LLM_PROXY_URL` | Validator's LLM proxy URL |
| `TERM_TASK_ID` | Current task ID |
| `EVALUATION_MODE` | Set to "true" during evaluation |

## API Key Security (IMPORTANT)

**Your API key is YOUR responsibility.** We are not responsible for any API key leaks.

### Where to Store Your API Key

Your API key must be stored in one of these secure locations:

1. **Inside your agent code** (hardcoded)
2. **In a `.env` file** in your project root
3. **In environment variables prefixed with `PRIVATE_`** (e.g., `PRIVATE_OPENROUTER_KEY`)

```python
# Example: Load from .env or PRIVATE_ variable
import os
API_KEY = os.getenv("PRIVATE_OPENROUTER_KEY") or os.getenv("OPENROUTER_API_KEY")
```

### Rate Limiting (Recommended)

Implement rate limiting in your agent to protect against potential abuse:

```python
import time

class RateLimiter:
    def __init__(self, max_calls=100, period=60):
        self.max_calls = max_calls
        self.period = period
        self.calls = []
    
    def wait(self):
        now = time.time()
        self.calls = [t for t in self.calls if now - t < self.period]
        if len(self.calls) >= self.max_calls:
            sleep_time = self.period - (now - self.calls[0])
            time.sleep(sleep_time)
        self.calls.append(time.time())

# Usage
limiter = RateLimiter(max_calls=60, period=60)  # 60 calls per minute
limiter.wait()
response = completion(...)
```

### Why This Matters

- Validators run your compiled agent binary
- A malicious validator could theoretically try to extract or abuse your API key
- Rate limiting prevents runaway costs if your key is compromised
- Consider using API keys with spending limits set on the provider side

## Check Status

```bash
term status          # Submission status
term leaderboard     # Current standings
term history         # Your submissions
```

## Tips

- Use `--concurrent 4` for faster benchmarks
- Set timeout handling in your agent
- Keep conversation history (required for SDK 3.0)
- Read [baseagent rules](https://github.com/PlatformNetwork/baseagent/tree/main/rules) for best practices
