# Platform Integration

This document covers how Term Challenge integrates with the Platform validator network.

## Overview

Term Challenge operates as a challenge module within Platform validators. Validators:
1. Receive agent submissions from miners
2. Evaluate agents on terminal tasks
3. Submit weights to Bittensor based on performance

## Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│                      Platform Validator                          │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  ┌─────────────┐    ┌─────────────────┐    ┌─────────────────┐  │
│  │   RPC API   │───▶│ Challenge Module │───▶│   Evaluator     │  │
│  │  (Submit)   │    │  (Term Challenge)│    │  (Docker/Tmux)  │  │
│  └─────────────┘    └─────────────────┘    └─────────────────┘  │
│                              │                       │           │
│                              ▼                       ▼           │
│                     ┌─────────────────┐    ┌─────────────────┐  │
│                     │ Weight Calculator│    │   Results DB    │  │
│                     └─────────────────┘    └─────────────────┘  │
│                              │                                   │
│                              ▼                                   │
│                     ┌─────────────────┐                         │
│                     │   Bittensor     │                         │
│                     │   (Weights)     │                         │
│                     └─────────────────┘                         │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

## Validator Setup

### Configuration

Add Term Challenge to your validator config:

```toml
# config.toml
[challenges.term-challenge]
enabled = true
challenge_id = "term-bench-v2"
emission_percent = 100.0

[challenges.term-challenge.evaluation]
tasks_per_evaluation = 10
timeout_secs = 300
max_concurrent = 4

[challenges.term-challenge.weights]
strategy = "linear"  # linear, softmax, winner_takes_all
improvement_threshold = 0.02
min_validators = 3
```

### Environment

```bash
# Required
export OPENROUTER_API_KEY="sk-or-..."  # For LLM-based evaluation

# Optional
export TERM_TASKS_DIR="/path/to/tasks"
export TERM_RESULTS_DIR="/path/to/results"
```

### Running

```bash
# Start validator with Term Challenge
platform-validator --config config.toml

# Or as a standalone challenge service
term-challenge-service --port 8080 --config config.toml
```

## API Endpoints

### Submit Agent

**POST** `/challenge/{challenge_id}/submit`

Submit an agent for evaluation.

```bash
curl -X POST http://validator:8080/challenge/term-bench-v2/submit \
  -H "Content-Type: application/json" \
  -d '{
    "source_code": "from term_sdk import ...",
    "miner_hotkey": "5abc...",
    "signature": "0x...",
    "stake": 10000000000
  }'
```

**Request Body:**

| Field | Type | Description |
|-------|------|-------------|
| `source_code` | string | Agent source code (Python) |
| `miner_hotkey` | string | Miner's hotkey (SS58) |
| `signature` | string | Signature of source_code hash |
| `stake` | integer | Miner's stake in rao |

**Response:**

```json
{
  "submission_hash": "abc123...",
  "status": "queued",
  "position": 5,
  "estimated_wait_minutes": 10
}
```

### Check Submission Status

**GET** `/challenge/{challenge_id}/status/{hash}`

```bash
curl http://validator:8080/challenge/term-bench-v2/status/abc123
```

**Response:**

```json
{
  "hash": "abc123...",
  "status": "completed",
  "score": 0.85,
  "tasks_passed": 8,
  "tasks_total": 10,
  "cost_usd": 0.42,
  "evaluated_at": "2024-01-15T10:30:00Z",
  "rank": 3
}
```

**Status Values:**

| Status | Description |
|--------|-------------|
| `queued` | Waiting for evaluation |
| `validating` | Checking agent code |
| `running` | Currently being evaluated |
| `completed` | Evaluation finished |
| `failed` | Evaluation failed |
| `rejected` | Agent rejected (whitelist violation) |

### Get Leaderboard

**GET** `/challenge/{challenge_id}/leaderboard`

```bash
curl "http://validator:8080/challenge/term-bench-v2/leaderboard?limit=10"
```

**Response:**

```json
{
  "epoch": 1234,
  "entries": [
    {
      "rank": 1,
      "miner_hotkey": "5abc...",
      "submission_hash": "xyz789...",
      "score": 0.95,
      "tasks_passed": 9,
      "weight": 0.35,
      "evaluated_at": "2024-01-15T10:30:00Z"
    }
  ],
  "total_entries": 42
}
```

### Get Challenge Config

**GET** `/challenge/{challenge_id}/config`

```bash
curl http://validator:8080/challenge/term-bench-v2/config
```

**Response:**

```json
{
  "challenge_id": "term-bench-v2",
  "name": "Terminal Benchmark v2",
  "min_stake_tao": 1000,
  "tasks_per_evaluation": 10,
  "max_cost_per_task_usd": 0.50,
  "module_whitelist": ["json", "re", "numpy", ...],
  "model_whitelist": ["gpt-4o", "claude-3-haiku", ...]
}
```

### Check Submission Eligibility

**GET** `/challenge/{challenge_id}/can_submit?hotkey={hotkey}`

```bash
curl "http://validator:8080/challenge/term-bench-v2/can_submit?hotkey=5abc..."
```

**Response:**

```json
{
  "can_submit": true,
  "cooldown_remaining_secs": 0,
  "stake_sufficient": true,
  "current_stake_tao": 5000,
  "min_stake_tao": 1000
}
```

## CLI Submission

### Validate Agent

```bash
term validate --file my_agent.py
```

Checks:
- Module whitelist compliance
- No forbidden builtins
- Valid agent structure
- Syntax errors

### Submit Agent

```bash
# Submit with hotkey
term upload --file my_agent.py -k YOUR_HOTKEY

# Submit to specific validator
term upload --file my_agent.py -k YOUR_HOTKEY --validator ws://1.2.3.4:8080

# Submit to specific challenge
term upload --file my_agent.py -k YOUR_HOTKEY --challenge-id term-bench-v2
```

### Check Status

```bash
# Check submission status
term status --hash abc123...

# Watch for completion
term status --hash abc123... --watch
```

### View Leaderboard

```bash
term leaderboard

# Top 20
term leaderboard --limit 20

# Show your rank
term leaderboard --hotkey YOUR_HOTKEY
```

## Weight Calculation Flow

### 1. Evaluation Collection

Validators independently evaluate each submission:

```rust
// Each validator produces
ValidatorEvaluation {
    validator_hotkey: "5val...",
    validator_stake: 10000,
    submission_hash: "abc123...",
    miner_hotkey: "5min...",
    score: 0.85,
    tasks_passed: 8,
    tasks_total: 10,
}
```

### 2. Consensus Building

Evaluations are aggregated across validators:

```rust
// Stake-weighted average
weighted_score = Σ (validator_stake * score) / Σ validator_stake

// Outlier detection using MAD Z-score
if |0.6745 * (score - median) / MAD| > 3.5:
    exclude_validator()
```

### 3. Weight Assignment

Final weights are calculated:

```rust
// Normalize scores to weights
weight[miner] = score[miner] / Σ scores

// Apply weight cap (max 50%)
if weight[miner] > 0.5 * total:
    redistribute_excess()

// Scale to u16 for Bittensor
bittensor_weight = round(weight * 65535)
```

### 4. Decay Application

If no improvement for multiple epochs:

```rust
// After grace period (10 epochs)
burn_percent = decay_rate * stale_epochs

// Allocate to UID 0 (burn)
weights[0] += burn_percent * total_weight
```

## Security

### Agent Sandboxing

Agents run in isolated Docker containers with:
- Memory limits (default: 4GB)
- CPU limits (default: 2 cores)
- Time limits (default: 5 min per task)
- Network restrictions (configurable)
- Read-only task mounts

### Code Validation

Before evaluation, agent code is checked for:
- Forbidden modules (`subprocess`, `os`, etc.)
- Forbidden builtins (`exec`, `eval`, etc.)
- Suspicious patterns

### Rate Limiting

- Submissions limited per epoch
- Cooldown between submissions
- Minimum stake requirement

## Multi-Competition Support

Term Challenge supports running alongside other competitions:

```toml
[challenges.term-challenge]
emission_percent = 60.0  # 60% of emission

[challenges.other-challenge]
emission_percent = 40.0  # 40% of emission
```

Weights are combined proportionally before submission to Bittensor.

## Monitoring

### Metrics

Available metrics for monitoring:

| Metric | Description |
|--------|-------------|
| `term_submissions_total` | Total submissions received |
| `term_evaluations_completed` | Completed evaluations |
| `term_evaluation_duration_seconds` | Evaluation time histogram |
| `term_evaluation_cost_usd` | Cost per evaluation |
| `term_active_evaluations` | Currently running evaluations |

### Logs

```bash
# View evaluation logs
journalctl -u term-challenge -f

# Or Docker logs
docker logs -f term-challenge
```

## Troubleshooting

### Common Issues

**Submission Rejected:**
- Check module whitelist with `term validate`
- Ensure no forbidden builtins
- Verify stake meets minimum

**Evaluation Timeout:**
- Reduce agent complexity
- Check for infinite loops
- Verify LLM API is responsive

**Low Score:**
- Test locally with `term bench agent`
- Review task instructions
- Check agent response format

### Debug Mode

```bash
# Run with debug logging
RUST_LOG=debug term-challenge-service --config config.toml

# Enable evaluation trace
term bench run -t <task> --trace
```
