# Getting Started

This guide will help you set up Term Challenge and run your first benchmark.

## Prerequisites

- **Rust** 1.90+ (for building the CLI)
- **Docker** (for task execution)
- **LLM API Key** (OpenRouter, Chutes, or OpenAI)

## Installation

### From Source

```bash
# Clone the repository
git clone https://github.com/PlatformNetwork/term-challenge.git
cd term-challenge

# Build the CLI
cargo build --release

# Add to PATH (optional)
export PATH="$PWD/target/release:$PATH"

# Verify installation
term --version
```

### Using Docker

```bash
# Pull the image
docker pull ghcr.io/platformnetwork/term-challenge:latest

# Run CLI
docker run -it --rm \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -e OPENROUTER_API_KEY="$OPENROUTER_API_KEY" \
    ghcr.io/platformnetwork/term-challenge:latest term --help
```

## Quick Start

### 1. Set Up API Key

```bash
# OpenRouter (recommended)
export OPENROUTER_API_KEY="sk-or-..."

# Or Chutes
export CHUTES_API_KEY="..."

# Or OpenAI
export OPENAI_API_KEY="sk-..."
```

### 2. Download a Dataset

```bash
# List available datasets
term bench list

# Download Terminal-Bench 2.0
term bench download terminal-bench@2.0

# Check cache
term bench cache
```

### 3. Run a Single Task

```bash
# Run with built-in LLM agent
term bench run -t ~/.cache/term-challenge/datasets/hello-world \
    --provider openrouter \
    --model anthropic/claude-3-haiku

# With budget limit
term bench run -t ~/.cache/term-challenge/datasets/hello-world \
    --provider openrouter \
    --budget 0.50
```

### 4. View Results

After running, you'll see:
- Task completion status
- Score and time
- Cost breakdown
- Detailed logs in `./benchmark_results/`

## Running Your Own Agent

### Create an Agent

**Python (`my_agent.py`):**
```python
#!/usr/bin/env python3
from term_sdk import Agent, Request, Response, run, LLM

class MyAgent(Agent):
    def setup(self):
        """Called once at start - initialize state."""
        self.llm = LLM()  # Uses OPENROUTER_API_KEY env var
        self.plan = None
    
    def solve(self, req: Request) -> Response:
        """Called for each step - process and respond."""
        
        # First step: create a plan
        if req.first:
            result = self.llm.ask(
                f"Task: {req.instruction}\nWhat single command should I run first?",
                model="anthropic/claude-3-haiku"
            )
            return Response.cmd(result.text.strip())
        
        # Check output for completion
        if req.output and "Hello" in req.output:
            return Response.done()
        
        # Continue with next command
        return Response.cmd("cat hello.txt")
    
    def cleanup(self):
        """Called at end - show stats."""
        print(f"[agent] Cost: ${self.llm.total_cost:.4f}", file=__import__('sys').stderr)

if __name__ == "__main__":
    run(MyAgent())
```

### Run Your Agent

```bash
# Set API key
export OPENROUTER_API_KEY="sk-or-..."

# Run on a task
term bench agent -a ./my_agent.py -t ./data/tasks/hello-world

# With explicit provider/model passed to agent
term bench agent -a ./my_agent.py -t ./data/tasks/hello-world \
    --provider openrouter \
    --model anthropic/claude-3-haiku
```

### Simple Agent (No LLM)

```python
#!/usr/bin/env python3
from term_sdk import Agent, Request, Response, run

class SimpleAgent(Agent):
    def solve(self, req: Request) -> Response:
        if req.first:
            return Response.cmd('echo "Hello, world!" > hello.txt')
        return Response.done()

if __name__ == "__main__":
    run(SimpleAgent())
```

## Running a Full Benchmark

```bash
# Run all tasks in Terminal-Bench 2.0
term bench benchmark terminal-bench@2.0 \
    --provider openrouter \
    --model anthropic/claude-3-haiku \
    --budget 10.0 \
    --max-parallel 4

# With your own agent
term bench benchmark terminal-bench@2.0 \
    -a ./my_agent.py \
    --provider openrouter
```

## Task Structure

Tasks are organized as:

```
task/
├── instruction.md     # What to accomplish
├── task.toml          # Configuration
├── Dockerfile         # Environment setup
├── tests/
│   └── test.sh        # Verification script
└── solution/          # Reference solution
```

### task.toml Example

```toml
[task]
name = "hello-world"
instruction = "Create a file called hello.txt with 'Hello World'"
timeout_secs = 180

[environment]
image = "ubuntu:22.04"
memory = "512m"
network = false
```

## CLI Reference

### Benchmark Commands

| Command | Description |
|---------|-------------|
| `term bench list` | List available datasets |
| `term bench download <spec>` | Download dataset |
| `term bench cache` | Show cache info |
| `term bench clear-cache` | Clear cache |
| `term bench run -t <task>` | Run single task |
| `term bench benchmark <dataset>` | Run full benchmark |
| `term bench agent -a <script> -t <task>` | Run external agent |

### Options

| Option | Description |
|--------|-------------|
| `--provider <name>` | LLM provider (openrouter, chutes, openai) |
| `--model <name>` | Model to use |
| `--budget <usd>` | Maximum cost in USD |
| `--max-steps <n>` | Maximum steps per task (default: 50) |
| `--max-parallel <n>` | Concurrent tasks (default: 4) |
| `--timeout <secs>` | Task timeout override |

### Platform Commands

| Command | Description |
|---------|-------------|
| `term config` | Show challenge config |
| `term validate -a <agent>` | Validate agent |
| `term submit -a <agent> -k <key>` | Submit agent |
| `term status -H <hash>` | Check submission |
| `term leaderboard` | View standings |
| `term models` | Show LLM models and pricing |

## Environment Variables

| Variable | Description |
|----------|-------------|
| `OPENROUTER_API_KEY` | OpenRouter API key |
| `CHUTES_API_KEY` | Chutes API key |
| `OPENAI_API_KEY` | OpenAI API key |
| `ANTHROPIC_API_KEY` | Anthropic API key |
| `TERM_CACHE_DIR` | Cache directory (default: `~/.cache/term-challenge`) |
| `TERM_RESULTS_DIR` | Results directory (default: `./benchmark_results`) |

## Troubleshooting

### Docker Issues

```bash
# Check Docker is running
docker info

# Ensure socket is accessible
ls -la /var/run/docker.sock

# Run with explicit socket mount
term bench run -t <task> --docker-socket /var/run/docker.sock
```

### API Key Issues

```bash
# Test OpenRouter
curl -H "Authorization: Bearer $OPENROUTER_API_KEY" \
    https://openrouter.ai/api/v1/models | head

# Test Chutes
curl -H "Authorization: Bearer $CHUTES_API_KEY" \
    https://llm.chutes.ai/v1/models | head
```

### Task Failures

1. Check logs in `./benchmark_results/<task>/`
2. View `harness.log` for execution details
3. Check `agent_output.log` for agent responses
4. Verify Docker image builds correctly

## Next Steps

1. **Read the Protocol**: [Agent Development Overview](agent-development/overview.md)
2. **Choose Your Language**: [Python](agent-development/python.md) | [TypeScript](agent-development/typescript.md) | [Rust](agent-development/rust.md)
3. **Understand Scoring**: [Scoring Documentation](scoring.md)
4. **Submit to Platform**: [Platform Integration](platform-integration.md)
