# Multi-Agent Code Review System

A Discord-style debate system where multiple AI agents review submitted code for security, quality, and rule compliance, reaching consensus through structured discussion.

## Overview

The multi-agent review system provides automated code review for agent submissions to the term-challenge platform. It uses three specialized AI agents that **reason about code** independently, then engage in a structured debate to reach consensus on whether the code should be approved or rejected.

Key principle: All analysis decisions are made by LLM reasoning, not hardcoded pattern matching.

## Features

- **Multiple Specialized Agents**: Security Auditor, Code Quality Reviewer, and Rule Compliance Verifier
- **Discord-style Conversation**: Agents debate and respond to each other's findings
- **Consensus Mechanisms**: Multiple voting methods including security veto
- **LLM-Driven Analysis**: Each agent uses AI reasoning to evaluate code
- **Flexible Output**: JSON, text, or Discord-style chat format

## Installation

The multi-agent review system is included in the term-challenge repository:

```bash
cd scripts
python -m multi_agent_review --help
```

## Quick Start

### Command Line

```bash
# Review a file
python -m multi_agent_review path/to/agent.py

# Quick analysis (no discussion rounds)
python -m multi_agent_review agent.py --quick

# Output as JSON
python -m multi_agent_review agent.py --format json --output report.json

# Use different consensus method
python -m multi_agent_review agent.py --method weighted --rounds 5
```

### Python API

```python
from multi_agent_review import ConversationOrchestrator, ConsensusMethod

# Create orchestrator
orchestrator = ConversationOrchestrator(
    consensus_method=ConsensusMethod.SECURITY_VETO,
    verbose=True
)

# Review code
code = open("agent.py").read()
log = orchestrator.review_code(code, "agent.py")

# Get results
print(log.format_discord_style())
print(f"Verdict: {log.consensus_result.final_verdict.value}")
```

## Agents

### SecurityAuditor

Specializes in security analysis using LLM reasoning:

- **Security Assessment**: Analyzes code for potential security concerns
- **Risk Evaluation**: Evaluates code behavior and intent
- **Vulnerability Detection**: Uses AI reasoning to identify issues

### CodeQualityReviewer

Analyzes code readability and structure:

- **Documentation**: Module docstrings, function docstrings
- **Naming**: Variable and function naming conventions
- **Structure**: Line length, nesting depth, function length
- **Best Practices**: Type hints, constants, error handling

### RuleComplianceVerifier

Evaluates compliance using LLM understanding:

- **SDK Usage**: Assesses appropriate use of the SDK
- **Best Practices**: Evaluates code against platform guidelines
- **Agent Structure**: Reviews overall code organization

## Consensus Methods

### Security Veto (Default)

The SecurityAuditor can unilaterally reject code if:
- It finds critical security issues
- Its confidence level is >= 70%

Otherwise, uses weighted voting.

### Weighted Voting

Votes are weighted by:
- Agent role importance (Security: 1.5x, Compliance: 1.3x, Quality: 1.0x)
- Confidence level

### Majority Vote

Simple majority (>50%) wins.

### Unanimous

All agents must agree on the verdict.

## Output Formats

### Discord Style (default)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“‹ CODE REVIEW SESSION
File: agent.py
Started: 2024-01-15 14:30:00
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€ [14:30:01] **SecurityAuditor** âœ…
â”‚
â”‚  ## Security Analysis
â”‚  
â”‚  **âœ… Positive Signals:**
â”‚  - Uses official term_sdk
â”‚  - Has standard agent methods
â”‚  
â”‚  **Verdict:** âœ… APPROVE
â”‚  **Confidence:** 80%
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### JSON

```json
{
  "code_hash": "abc123...",
  "filename": "agent.py",
  "started_at": "2024-01-15T14:30:00",
  "messages": [...],
  "analyses": {...},
  "consensus_result": {
    "final_verdict": "APPROVE",
    "consensus_reached": true,
    "confidence_scores": {...}
  }
}
```

## API Reference

### ConversationOrchestrator

```python
class ConversationOrchestrator:
    def __init__(
        self,
        llm_client=None,
        consensus_method: ConsensusMethod = ConsensusMethod.SECURITY_VETO,
        max_rounds: int = 3,
        verbose: bool = True
    )
    
    def review_code(self, code: str, filename: str = "agent.py") -> ConversationLog
    def quick_review(self, code: str, filename: str = "agent.py") -> ConsensusResult
    def add_agent(self, agent: ReviewAgent)
```

### ConsensusManager

```python
class ConsensusManager:
    def __init__(self, method: ConsensusMethod = ConsensusMethod.SECURITY_VETO)
    
    def add_vote(self, agent_name: str, verdict: ReviewVerdict, confidence: float, rationale: str = "")
    def calculate_consensus(self) -> ConsensusResult
    def should_continue(self) -> bool
```

### ReviewVerdict

```python
class ReviewVerdict(Enum):
    APPROVE = "APPROVE"
    REJECT = "REJECT"
    NEEDS_DISCUSSION = "NEEDS_DISCUSSION"
```

## Adding Custom Agents

```python
from multi_agent_review import ReviewAgent, CodeAnalysis, ReviewVerdict

class MyCustomAgent(ReviewAgent):
    def __init__(self, llm_client=None):
        super().__init__(
            name="MyCustomAgent",
            role="Custom Analysis Specialist",
            llm_client=llm_client
        )
    
    def analyze_code(self, code: str, filename: str = "agent.py") -> CodeAnalysis:
        # Your analysis logic
        return CodeAnalysis(
            issues=[],
            warnings=[],
            positives=["Custom check passed"],
            verdict=ReviewVerdict.APPROVE,
            confidence=0.8
        )
    
    def respond_to_discussion(self, code, conversation, my_analysis) -> ReviewMessage:
        # Respond to other agents
        return self._create_message("My response", my_analysis.verdict)
    
    def get_system_prompt(self) -> str:
        return "You are a custom code reviewer..."

# Use it
orchestrator = ConversationOrchestrator()
orchestrator.add_agent(MyCustomAgent())
```

## Exit Codes

| Code | Meaning |
|------|---------|
| 0 | Code APPROVED |
| 1 | Code REJECTED or NEEDS_DISCUSSION |

## Integration with CI/CD

```yaml
# GitHub Actions example
- name: Review Agent Code
  run: |
    python -m multi_agent_review submission/agent.py --format json --output review.json
    if [ $? -ne 0 ]; then
      echo "Code review failed"
      cat review.json
      exit 1
    fi
```

## License

MIT License - See LICENSE file for details.
